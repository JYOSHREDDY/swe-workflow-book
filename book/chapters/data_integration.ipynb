{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7387d7e6",
   "metadata": {},
   "source": [
    "# 4.1 Bringing Data Together\n",
    "\n",
    "## 4.1.1 Introduction to the Data\n",
    "\n",
    "The three key datasets:\n",
    "\n",
    "- **Climatology Data:** Offers a broad view of weather patterns over time.\n",
    "- **SNOTEL Data:** Provides specific insights into snowpack conditions.\n",
    "- **Terrain Data:** Brings in the geographical and physical characteristics of the landscape.\n",
    "\n",
    "Each dataset comes packed with essential features like latitude, longitude, and date, ready to enrich our SWE prediction model.\n",
    "\n",
    "## 4.1.2 Integrating the Datasets\n",
    "\n",
    "We are combining these large datasets into one DataFrame using Dask. Dask allows us to work with big data efficiently, so we can merge the datasets quickly and easily, no matter how large they are.\n",
    "\n",
    "And also if the size of the data is larger then reading large CSV files in chunks helps manage big data more efficiently by reducing memory use, speeding up processing, and improving error handling. This approach makes it easier to work on large datasets with limited resources, ensuring flexibility and scalability in data analysis.\n",
    "\n",
    "### Read and Convert\n",
    "- Each CSV file is read into a Dask DataFrame, with latitude and longitude data types converted to floats for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25039fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "file_path1 = '../data/training_ready_climatology_data.csv'\n",
    "file_path2 = '../data/training_ready_snotel_data.csv'\n",
    "file_path3 = '../data/training_ready_terrain_data.csv'\n",
    "# Read each CSV file into a Dask DataFrame\n",
    "df1 = dd.read_csv(file_path1)\n",
    "df2 = dd.read_csv(file_path2)\n",
    "df3 = dd.read_csv(file_path3)\n",
    "# Perform data type conversion for latitude and longitude columns\n",
    "df1['lat'] = df1['lat'].astype(float)\n",
    "df1['lon'] = df1['lon'].astype(float)\n",
    "df2['lat'] = df2['lat'].astype(float)\n",
    "df2['lon'] = df2['lon'].astype(float)\n",
    "df3['lat'] = df3['lat'].astype(float)\n",
    "df3['lon'] = df3['lon'].astype(float)\n",
    "#rename the columns to match the other dataframes\n",
    "df2 = df2.rename(columns={\"Date\": \"date\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96f259",
   "metadata": {},
   "source": [
    "#### Merge on Common Ground\n",
    "- The dataframes are then merged based on shared columns (latitude, longitude, and date), ensuring that each row represents a coherent set of data from all three sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43de425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the first two DataFrames based on 'lat', 'lon', and 'date'\n",
    "merged_df1 = dd.merge(df1, df2, left_on=['lat', 'lon', 'date'], right_on=['lat', 'lon', 'date'])\n",
    "\n",
    "# Merge the third DataFrame based on 'lat' and 'lon'\n",
    "merged_df2 = dd.merge(merged_df1, df3, on=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c17226",
   "metadata": {},
   "source": [
    "#### Output\n",
    "- The merged DataFrame is saved as a new CSV file, ready for further processing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c200ffde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/vangavetisaivivek/research/swe-workflow-book/book/data/model_training_data.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df2.to_csv('../data/model_training_data.csv', index=False, single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414a3ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax',\n",
       "       'rmin', 'tmmn', 'tmmx', 'vpd', 'vs',\n",
       "       'Snow Water Equivalent (in) Start of Day Values',\n",
       "       'Change In Snow Water Equivalent (in)',\n",
       "       'Snow Depth (in) Start of Day Values', 'Change In Snow Depth (in)',\n",
       "       'Air Temperature Observed (degF) Start of Day Values', 'station_name',\n",
       "       'station_triplet', 'station_elevation', 'station_lat', 'station_long',\n",
       "       'mapping_station_id', 'mapping_cell_id', 'elevation', 'slope',\n",
       "       'curvature', 'aspect', 'eastness', 'northness'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_csv('../data/model_training_data.csv')\n",
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
