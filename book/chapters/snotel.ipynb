{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f3ae743c9b5caf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3.3 SNOTEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d542203b",
   "metadata": {},
   "source": [
    "Having explored the details of SNOTEL and its significance, let's dive into the exciting journey of collecting and processing the data.\n",
    "\n",
    "This script automates the collection of Snow Water Equivalent (SWE) data from SNOTEL stations, filters it based on geographic criteria, and saves it into CSV files. By the end of this process, we'll have a valuable dataset, ready to provide insights into SWE, snow depth, and temperature trends in the Western United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c538c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf48b29",
   "metadata": {},
   "source": [
    "Common Python libraries for mathematical operations, data handling, HTTP requests, CSV operations, file handling, and parallel computing with Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc8bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# homedir = os.path.expanduser('~')\n",
    "working_dir = f\"../data/snotel_training_data\"\n",
    "southwest_lon = -125.0\n",
    "southwest_lat = 25.0\n",
    "northeast_lon = -100.0\n",
    "northeast_lat = 49.0\n",
    "#for demonstration purposes, we will use a time period of 2022-2023\n",
    "train_start_date = \"2022-01-03\"\n",
    "train_end_date = \"2022-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988206e",
   "metadata": {},
   "source": [
    "We've defined our geographic criteria using latitude and longitude bounds.\n",
    "For the demonstration purpose we set the start and end dates, and specified the directory for saving all our work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23891664",
   "metadata": {},
   "source": [
    "## 3.3.1 Download Snotel Station Data\n",
    "\n",
    "The below code downloads station data from a web API if it doesn't already exist locally. It saves the data as a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8aa032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded and saved to ../data/snotel_training_data/all_snotel_cdec_stations.json\n"
     ]
    }
   ],
   "source": [
    "output_json_file = f'{working_dir}/all_snotel_cdec_stations.json'\n",
    "if not os.path.exists(output_json_file):\n",
    "    # Fetch data from the URL\n",
    "    response = requests.get(\"https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/stations?activeOnly=true&returnForecastPointMetadata=false&returnReservoirMetadata=false&returnStationElements=false\")\n",
    "    \n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Decode the JSON content\n",
    "        json_content = response.json()\n",
    "\n",
    "        # Save the JSON content to a file\n",
    "        with open(output_json_file, 'w') as json_file:\n",
    "            json.dump(json_content, json_file, indent=2)\n",
    "\n",
    "        print(f\"Data downloaded and saved to {output_json_file}\")\n",
    "    else:\n",
    "        print(f\"Failed to download data. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"The file {output_json_file} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542e2df",
   "metadata": {},
   "source": [
    "## 3.3.2 Convert JSON Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893b663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data converted and saved to ../data/snotel_training_data/all_snotel_cdec_stations.csv\n"
     ]
    }
   ],
   "source": [
    "# read the json file and convert it to csv\n",
    "csv_file_path = f'{working_dir}/all_snotel_cdec_stations.csv'\n",
    "if not os.path.exists(csv_file_path):\n",
    "    # Read the JSON file\n",
    "    with open(output_json_file, 'r') as json_file:\n",
    "        json_content = json.load(json_file)\n",
    "\n",
    "    # Check the content (print or analyze as needed)\n",
    "    #print(\"JSON Content:\")\n",
    "    #print(json.dumps(json_content, indent=2))\n",
    "\n",
    "    # Convert JSON data to a list of dictionaries (assuming JSON is a list of objects)\n",
    "    data_list = json_content if isinstance(json_content, list) else [json_content]\n",
    "\n",
    "    # Get the header from the keys of the first dictionary (assuming consistent structure)\n",
    "    header = data_list[0].keys()\n",
    "    # Write to CSV file\n",
    "    with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=header)\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(data_list)\n",
    "\n",
    "    print(f\"Data converted and saved to {csv_file_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"The csv all snotel/cdec stations exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44190a7b",
   "metadata": {},
   "source": [
    "The above code converts the downloaded snotel station json data to csv and saves the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2396ab",
   "metadata": {},
   "source": [
    "## 3.3.3 Filter Active Snotel Stations in the Western U.S. and Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b0d178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  stationTriplet stationId stateCode networkCode                    name  \\\n",
      "0   2057:AL:SCAN      2057        AL        SCAN                AAMU-JTG   \n",
      "1    ABY:CA:SNOW       ABY        CA        SNOW                   Abbey   \n",
      "2   0010:ID:COOP      0010        ID        COOP  Aberdeen Experimnt Stn   \n",
      "3  1F01A:BC:SNOW     1F01A        BC        SNOW           Aberdeen Lake   \n",
      "4   0041:NM:COOP      0041        NM        COOP             Abiquiu Dam   \n",
      "\n",
      "  dcoCode  countyName           huc  elevation  latitude  longitude  \\\n",
      "0      GC     Madison  6.030002e+10      860.0  34.78333  -86.55000   \n",
      "1      UN      Plumas  1.802012e+11     5650.0  39.95500 -120.53800   \n",
      "2      ID     Bingham  1.704021e+11     4410.0  42.95000 -112.83333   \n",
      "3      OR     UNKNOWN           NaN     4298.0  50.14733 -119.05340   \n",
      "4      UN  Rio Arriba  1.302010e+11     6380.0  36.23333 -106.43333   \n",
      "\n",
      "   dataTimeZone pedonCode shefId         beginDate           endDate  \n",
      "0          -6.0     27979  AAMA1  2002-02-23 00:00  2100-01-01 00:00  \n",
      "1           NaN       NaN    NaN  1963-02-01 00:00  2100-01-01 00:00  \n",
      "2           NaN       NaN  ABDI1  1914-01-01 00:00  2100-01-01 00:00  \n",
      "3           NaN       NaN  ABLQ2  1939-04-01 00:00  2100-01-01 00:00  \n",
      "4           NaN       NaN  ABIN5  1957-01-01 00:00  2100-01-01 00:00  \n",
      "(4414, 16)\n",
      "Filtered DataFrame:\n",
      "(3674, 16)\n"
     ]
    }
   ],
   "source": [
    "active_csv_file_path = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv'\n",
    "if not os.path.exists(active_csv_file_path):\n",
    "    all_df = pd.read_csv(csv_file_path)\n",
    "    print(all_df.head())\n",
    "    all_df['endDate'] = pd.to_datetime(all_df['endDate'])\n",
    "    print(all_df.shape)\n",
    "    end_date = pd.to_datetime('2050-01-01')\n",
    "    filtered_df = all_df[all_df['endDate'] > end_date]\n",
    "    \n",
    "    # Filter rows within the latitude and longitude ranges\n",
    "    filtered_df = filtered_df[\n",
    "        (filtered_df['latitude'] >= southwest_lat) & (filtered_df['latitude'] <= northeast_lat) &\n",
    "        (filtered_df['longitude'] >= southwest_lon) & (filtered_df['longitude'] <= northeast_lon)\n",
    "    ]\n",
    "\n",
    "    # Print the original and filtered DataFrames\n",
    "    print(\"Filtered DataFrame:\")\n",
    "    print(filtered_df.shape)\n",
    "    filtered_df.to_csv(active_csv_file_path, index=False)\n",
    "else:\n",
    "    print(f\"The active csv already exists: {active_csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c7107",
   "metadata": {},
   "source": [
    "The above code filters out the snotel stations in the western us based on the bounderies specified. And it also filters out the active stations based on the end date field and saved in the file `all_snotel_cdec_stations_active_in_westus.csv` which will be used for later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a11bd",
   "metadata": {},
   "source": [
    "## 3.3.4 Retreive Snow Water Equivalent (SWE) from Snotel Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a046c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_commented_lines(text):\n",
    "    lines = text.split(os.linesep)\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            cleaned_lines.append(line)\n",
    "    cleaned_text = os.linesep.join(cleaned_lines)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dbaa79",
   "metadata": {},
   "source": [
    "Removes lines starting with # from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b045e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    stationTriplet stationId stateCode networkCode                    name  \\\n",
      "0      ABY:CA:SNOW       ABY        CA        SNOW                   Abbey   \n",
      "1     0010:ID:COOP      0010        ID        COOP  Aberdeen Experimnt Stn   \n",
      "2     0041:NM:COOP      0041        NM        COOP             Abiquiu Dam   \n",
      "3  08108010:NM:BOR  08108010        NM         BOR       Abiquiu Reservoir   \n",
      "4    13E19:ID:SNOW     13E19        ID        SNOW           Above Gilmore   \n",
      "\n",
      "  dcoCode  countyName           huc  elevation  latitude  longitude  \\\n",
      "0      UN      Plumas  1.802012e+11     5650.0  39.95500 -120.53800   \n",
      "1      ID     Bingham  1.704021e+11     4410.0  42.95000 -112.83333   \n",
      "2      UN  Rio Arriba  1.302010e+11     6380.0  36.23333 -106.43333   \n",
      "3      CO  Rio Arriba  1.302010e+11     6180.0  36.23700 -106.42912   \n",
      "4      ID       Lemhi  1.706020e+11     8289.0  44.45615 -113.30097   \n",
      "\n",
      "   dataTimeZone  pedonCode shefId         beginDate     endDate  \n",
      "0           NaN        NaN    NaN  1963-02-01 00:00  2100-01-01  \n",
      "1           NaN        NaN  ABDI1  1914-01-01 00:00  2100-01-01  \n",
      "2           NaN        NaN  ABIN5  1957-01-01 00:00  2100-01-01  \n",
      "3           NaN        NaN    NaN  1964-09-01 00:00  2100-01-01  \n",
      "4           NaN        NaN  ABGI1  1961-01-01 00:00  2100-01-01  \n"
     ]
    }
   ],
   "source": [
    "new_base_station_list_file = f\"{working_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\n",
    "new_base_df = pd.read_csv(new_base_station_list_file)\n",
    "print(new_base_df.head())\n",
    "\n",
    "csv_file = f'{new_base_station_list_file}_swe_restored_dask_all_vars.csv'\n",
    "start_date = train_start_date\n",
    "end_date = train_end_date\n",
    "\n",
    " # Create an empty Pandas DataFrame with the desired columns\n",
    "result_df = pd.DataFrame(columns=[\n",
    "    'station_name', \n",
    "    'date', \n",
    "    'lat', \n",
    "    'lon', \n",
    "    'swe_value', \n",
    "    'change_in_swe_inch', \n",
    "    'snow_depth', \n",
    "    'change_in_swe_inch', \n",
    "    'air_temperature_observed_f'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de336bf",
   "metadata": {},
   "source": [
    "Reads a CSV file containing active station data for the Western U.S. into a DataFrame and initializes an empty DataFrame with specified columns to store results related to snow water equivalent (SWE) and other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281f126",
   "metadata": {},
   "source": [
    "## 3.3.5 Process Data using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985dde0",
   "metadata": {},
   "source": [
    "Dask is a flexible parallel computing library for analytic computing. It is designed to scale from a single machine to a cluster of machines. `@dask.delayed` decorator is used to mark the process_station function for lazy execution. This means that the function will not execute immediately but will instead create a task graph that can be executed in parallel.\n",
    "\n",
    "By parallelizing the processing of each station, Dask can utilize multiple CPU cores or even multiple machines, leading to faster execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fa905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each station\n",
    "@dask.delayed\n",
    "def process_station(station):\n",
    "    location_name = station['name']\n",
    "    location_triplet = station['stationTriplet']\n",
    "    location_elevation = station['elevation']\n",
    "    location_station_lat = station['latitude']\n",
    "    location_station_long = station['longitude']\n",
    "\n",
    "    url = f\"https://wcc.sc.egov.usda.gov/reportGenerator/view_csv/customSingleStationReport/daily/{location_triplet}%7Cid%3D%22%22%7Cname/{start_date},{end_date}%2C0/WTEQ%3A%3Avalue%2CWTEQ%3A%3Adelta%2CSNWD%3A%3Avalue%2CSNWD%3A%3Adelta%2CTOBS%3A%3Avalue\"\n",
    "\n",
    "    r = requests.get(url)\n",
    "    text = remove_commented_lines(r.text)\n",
    "    reader = csv.DictReader(io.StringIO(text))\n",
    "    json_data = json.loads(json.dumps(list(reader)))\n",
    "\n",
    "    entries = []\n",
    "    \n",
    "    for entry in json_data:\n",
    "        try:\n",
    "            # {'Date': '2021-06-18', 'Snow Water Equivalent (in) Start of Day Values': '', 'Change In Snow Water Equivalent (in)': '', 'Snow Depth (in) Start of Day Values': '', 'Change In Snow Depth (in)': '', 'Air Temperature Observed (degF) Start of Day Values': '70.5'}\n",
    "            required_data = {\n",
    "            'station_name': location_name,\n",
    "            'date': entry.get('Date', None),\n",
    "            'lat': location_station_lat, \n",
    "            'lon': location_station_long,\n",
    "            'swe_value': entry.get('Snow Water Equivalent (in) Start of Day Values',None),\n",
    "            'change_in_swe_inch': entry.get('Change In Snow Water Equivalent (in)', None),\n",
    "            'snow_depth': entry.get('Snow Depth (in) Start of Day Values',None),\n",
    "            'change_in_swe_inch': entry.get('Change In Snow Depth (in)', None),\n",
    "            'air_temperature_observed_f': entry.get('Air Temperature Observed (degF) Start of Day Values',None)\n",
    "            }\n",
    "            entries.append(required_data)\n",
    "        except Exception as e:\n",
    "            print(\"entry = \", entry)\n",
    "            raise e\n",
    "    return pd.DataFrame(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff3676",
   "metadata": {},
   "source": [
    "```python\n",
    "location_name = station['name']\n",
    "location_triplet = station['stationTriplet']\n",
    "location_elevation = station['elevation']\n",
    "location_station_lat = station['latitude']\n",
    "location_station_long = station['longitude']\n",
    "```\n",
    "Extracts various pieces of information about the station from the `station` dictionary.\n",
    "\n",
    "```python\n",
    "url = f\"https://wcc.sc.egov.usda.gov/reportGenerator/view_csv/customSingleStationReport/daily/{location_triplet}%7Cid%3D%22%22%7Cname/{start_date},{end_date}%2C0/WTEQ%3A%3Avalue%2CWTEQ%3A%3Adelta%2CSNWD%3A%3Avalue%2CSNWD%3A%3Adelta%2CTOBS%3A%3Avalue\"\n",
    "```\n",
    "Constructs a `URL` to request data for the station using its `stationTriplet` and the specified date range (`start_date` and `end_date`)\n",
    "\n",
    "- Sends a GET request to the constructed URL.\n",
    "- Removes commented lines from the response text.\n",
    "- Parses the CSV data into a list of dictionaries (json_data)\n",
    "- Initializes an empty list to store the processed data entries.\n",
    "\n",
    "```python\n",
    "for entry in json_data:\n",
    "    try:\n",
    "        required_data = {\n",
    "            'station_name': location_name,\n",
    "            'date': entry.get('Date', None),\n",
    "            'lat': location_station_lat, \n",
    "            'lon': location_station_long,\n",
    "            'swe_value': entry.get('Snow Water Equivalent (in) Start of Day Values', None),\n",
    "            'change_in_swe_inch': entry.get('Change In Snow Water Equivalent (in)', None),\n",
    "            'snow_depth': entry.get('Snow Depth (in) Start of Day Values', None),\n",
    "            'change_in_swe_inch': entry.get('Change In Snow Depth (in)', None),\n",
    "            'air_temperature_observed_f': entry.get('Air Temperature Observed (degF) Start of Day Values', None)\n",
    "        }\n",
    "        entries.append(required_data)\n",
    "    except Exception as e:\n",
    "        print(\"entry = \", entry)\n",
    "        raise e\n",
    "```\n",
    "For each entry in the json data\n",
    "- parses the json data\n",
    "- Extracts the required fields and constructs a dictionary (required_data) for each entry.Appends the dictionary to the entries list.\n",
    "\n",
    "At the end converts the list of dictionaries (entries) into a Pandas DataFrame and returns it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f8c62ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     station_name        date    lat        lon swe_value change_in_swe_inch  \\\n",
      "0  Adams Ranch #1  2022-01-03  34.25 -105.41667                                \n",
      "1  Adams Ranch #1  2022-01-04  34.25 -105.41667                                \n",
      "2  Adams Ranch #1  2022-01-05  34.25 -105.41667                                \n",
      "3  Adams Ranch #1  2022-01-06  34.25 -105.41667                                \n",
      "4  Adams Ranch #1  2022-01-07  34.25 -105.41667                                \n",
      "\n",
      "  snow_depth air_temperature_observed_f  \n",
      "0                                  21.7  \n",
      "1                                  36.9  \n",
      "2                                  42.4  \n",
      "3                                  45.3  \n",
      "4                                  45.7  \n"
     ]
    }
   ],
   "source": [
    "# List of delayed computations for each station\n",
    "delayed_results = [process_station(row) for _, row in new_base_df.iterrows()]\n",
    "\n",
    "# Compute the delayed results\n",
    "result_lists = dask.compute(*delayed_results)\n",
    "\n",
    "# Concatenate the lists into a Pandas DataFrame\n",
    "result_df = pd.concat(result_lists, ignore_index=True)\n",
    "\n",
    "# Print the final Pandas DataFrame\n",
    "print(result_df.head())\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d98720",
   "metadata": {},
   "source": [
    "Iterates over each station in the active snotel stations and calls teh `process_station` function to process.\n",
    "\n",
    "collects the delayed computations in a list called `delayed_results`.\n",
    "\n",
    "Uses `dask.compute` to execute all delayed computations in parallel.\n",
    "\n",
    "The `*delayed_results` syntax unpacks the list into individual arguments.\n",
    "\n",
    "The results are stored in result_lists, which is a list of DataFrames.\n",
    "\n",
    "```python\n",
    "result_df = pd.concat(result_lists, ignore_index=True)\n",
    "```\n",
    "Concatenates all DataFrames in result_lists into a single DataFrame result_df.\n",
    "\n",
    "The `ignore_index=True` parameter ensures that the index is reset.\n",
    "\n",
    "Saves the final DataFrame result_df to a CSV file specified by csv_file. The index=False parameter ensures that the index is not included in the CSV file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
