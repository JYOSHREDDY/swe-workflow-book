{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f3ae743c9b5caf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SNOTEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d542203b",
   "metadata": {},
   "source": [
    "Having explored the details of SNOTEL and its significance, let's dive into the exciting journey of collecting and processing the data.\n",
    "\n",
    "This script automates the collection of Snow Water Equivalent (SWE) data from SNOTEL stations, filters it based on geographic criteria, and saves it into CSV files. By the end of this process, we'll have a valuable dataset, ready to provide insights into SWE, snow depth, and temperature trends in the Western United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c538c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf48b29",
   "metadata": {},
   "source": [
    "Common Python libraries for mathematical operations, data handling, HTTP requests, CSV operations, file handling, and parallel computing with Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc8bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# homedir = os.path.expanduser('~')\n",
    "working_dir = f\"../data/snotel_training_data\"\n",
    "southwest_lon = -125.0\n",
    "southwest_lat = 25.0\n",
    "northeast_lon = -100.0\n",
    "northeast_lat = 49.0\n",
    "#for demonstration purposes, we will use a time period of 2022-2023\n",
    "train_start_date = \"2022-01-03\"\n",
    "train_end_date = \"2022-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988206e",
   "metadata": {},
   "source": [
    "We've defined our geographic criteria using latitude and longitude bounds, set the start and end dates, and specified the directory for saving all our work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23891664",
   "metadata": {},
   "source": [
    "The below function downloads station data from a web API if it doesn't already exist locally. It saves the data as a JSON file, then converts it to a CSV file, and filters active stations in the Western US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8aa032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded and saved to ../data/snotel_training_data/all_snotel_cdec_stations.json\n",
      "Data converted and saved to ../data/snotel_training_data/all_snotel_cdec_stations.csv\n",
      "  stationTriplet stationId stateCode networkCode                    name  \\\n",
      "0   2057:AL:SCAN      2057        AL        SCAN                AAMU-JTG   \n",
      "1    ABY:CA:SNOW       ABY        CA        SNOW                   Abbey   \n",
      "2   0010:ID:COOP      0010        ID        COOP  Aberdeen Experimnt Stn   \n",
      "3  1F01A:BC:SNOW     1F01A        BC        SNOW           Aberdeen Lake   \n",
      "4   0041:NM:COOP      0041        NM        COOP             Abiquiu Dam   \n",
      "\n",
      "  dcoCode  countyName           huc  elevation  latitude  longitude  \\\n",
      "0      GC     Madison  6.030002e+10      860.0  34.78333  -86.55000   \n",
      "1      UN      Plumas  1.802012e+11     5650.0  39.95500 -120.53800   \n",
      "2      ID     Bingham  1.704021e+11     4410.0  42.95000 -112.83333   \n",
      "3      OR     UNKNOWN           NaN     4298.0  50.14733 -119.05340   \n",
      "4      UN  Rio Arriba  1.302010e+11     6380.0  36.23333 -106.43333   \n",
      "\n",
      "   dataTimeZone pedonCode shefId         beginDate           endDate  \n",
      "0          -6.0     27979  AAMA1  2002-02-23 00:00  2100-01-01 00:00  \n",
      "1           NaN       NaN    NaN  1963-02-01 00:00  2100-01-01 00:00  \n",
      "2           NaN       NaN  ABDI1  1914-01-01 00:00  2100-01-01 00:00  \n",
      "3           NaN       NaN  ABLQ2  1939-04-01 00:00  2100-01-01 00:00  \n",
      "4           NaN       NaN  ABIN5  1957-01-01 00:00  2100-01-01 00:00  \n",
      "(4412, 16)\n",
      "Filtered DataFrame:\n",
      "(3675, 16)\n"
     ]
    }
   ],
   "source": [
    "def download_station_json():\n",
    "    # https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/stations?activeOnly=true&returnForecastPointMetadata=false&returnReservoirMetadata=false&returnStationElements=false\n",
    "    output_json_file = f'{working_dir}/all_snotel_cdec_stations.json'\n",
    "    if not os.path.exists(output_json_file):\n",
    "        # Fetch data from the URL\n",
    "        response = requests.get(\"https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/stations?activeOnly=true&returnForecastPointMetadata=false&returnReservoirMetadata=false&returnStationElements=false\")\n",
    "        \n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Decode the JSON content\n",
    "            json_content = response.json()\n",
    "\n",
    "            # Save the JSON content to a file\n",
    "            with open(output_json_file, 'w') as json_file:\n",
    "                json.dump(json_content, json_file, indent=2)\n",
    "\n",
    "            print(f\"Data downloaded and saved to {output_json_file}\")\n",
    "        else:\n",
    "            print(f\"Failed to download data. Status code: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"The file {output_json_file} already exists.\")\n",
    "        \n",
    "    \n",
    "    # read the json file and convert it to csv\n",
    "    csv_file_path = f'{working_dir}/all_snotel_cdec_stations.csv'\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        # Read the JSON file\n",
    "        with open(output_json_file, 'r') as json_file:\n",
    "            json_content = json.load(json_file)\n",
    "\n",
    "        # Check the content (print or analyze as needed)\n",
    "        #print(\"JSON Content:\")\n",
    "        #print(json.dumps(json_content, indent=2))\n",
    "\n",
    "        # Convert JSON data to a list of dictionaries (assuming JSON is a list of objects)\n",
    "        data_list = json_content if isinstance(json_content, list) else [json_content]\n",
    "\n",
    "        # Get the header from the keys of the first dictionary (assuming consistent structure)\n",
    "        header = data_list[0].keys()\n",
    "        # Write to CSV file\n",
    "        with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "            csv_writer = csv.DictWriter(csv_file, fieldnames=header)\n",
    "            csv_writer.writeheader()\n",
    "            csv_writer.writerows(data_list)\n",
    "\n",
    "        print(f\"Data converted and saved to {csv_file_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"The csv all snotel/cdec stations exists.\")\n",
    "        \n",
    "        \n",
    "    active_csv_file_path = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv'\n",
    "    if not os.path.exists(active_csv_file_path):\n",
    "        all_df = pd.read_csv(csv_file_path)\n",
    "        print(all_df.head())\n",
    "        all_df['endDate'] = pd.to_datetime(all_df['endDate'])\n",
    "        print(all_df.shape)\n",
    "        end_date = pd.to_datetime('2050-01-01')\n",
    "        filtered_df = all_df[all_df['endDate'] > end_date]\n",
    "        \n",
    "        # Filter rows within the latitude and longitude ranges\n",
    "        filtered_df = filtered_df[\n",
    "            (filtered_df['latitude'] >= southwest_lat) & (filtered_df['latitude'] <= northeast_lat) &\n",
    "            (filtered_df['longitude'] >= southwest_lon) & (filtered_df['longitude'] <= northeast_lon)\n",
    "        ]\n",
    "\n",
    "        # Print the original and filtered DataFrames\n",
    "        print(\"Filtered DataFrame:\")\n",
    "        print(filtered_df.shape)\n",
    "        filtered_df.to_csv(active_csv_file_path, index=False)\n",
    "    else:\n",
    "        print(f\"The active csv already exists: {active_csv_file_path}\")\n",
    "\n",
    "download_station_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c7107",
   "metadata": {},
   "source": [
    "In the above code we have saved the active stations list in file `all_snotel_cdec_stations_active_in_westus.csv` which will be used for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92706ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db890b",
   "metadata": {},
   "source": [
    "Reads a JSON file and returns its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44aad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    d_lat = lat2 - lat1\n",
    "    d_long = lon2 - lon1\n",
    "    a = math.sin(d_lat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(d_long / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = 6371 * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1107978",
   "metadata": {},
   "source": [
    "Calculates the distance between two geographic coordinates using the Haversine formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3721a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_location(locations, target_lat, target_lon):\n",
    "    n_location = None\n",
    "    min_distance = float('inf')\n",
    "    for location in locations:\n",
    "        lat = location['location']['lat']\n",
    "        lon = location['location']['lng']\n",
    "        distance = haversine(lat, lon, target_lat, target_lon)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            n_location = location\n",
    "    return n_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4301f2",
   "metadata": {},
   "source": [
    "Finds the nearest location to given coordinates from a list of locations using the previous haversine method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a046c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_commented_lines(text):\n",
    "    lines = text.split(os.linesep)\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            cleaned_lines.append(line)\n",
    "    cleaned_text = os.linesep.join(cleaned_lines)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dbaa79",
   "metadata": {},
   "source": [
    "Removes lines starting with # from a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b15827",
   "metadata": {},
   "source": [
    "### Function to Retrieve SWE Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b04e9ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    stationTriplet stationId stateCode networkCode                    name  \\\n",
      "0      ABY:CA:SNOW       ABY        CA        SNOW                   Abbey   \n",
      "1     0010:ID:COOP      0010        ID        COOP  Aberdeen Experimnt Stn   \n",
      "2     0041:NM:COOP      0041        NM        COOP             Abiquiu Dam   \n",
      "3  08108010:NM:BOR  08108010        NM         BOR       Abiquiu Reservoir   \n",
      "4    13E19:ID:SNOW     13E19        ID        SNOW           Above Gilmore   \n",
      "\n",
      "  dcoCode  countyName           huc  elevation  latitude  longitude  \\\n",
      "0      UN      Plumas  1.802012e+11     5650.0  39.95500 -120.53800   \n",
      "1      ID     Bingham  1.704021e+11     4410.0  42.95000 -112.83333   \n",
      "2      UN  Rio Arriba  1.302010e+11     6380.0  36.23333 -106.43333   \n",
      "3      CO  Rio Arriba  1.302010e+11     6180.0  36.23700 -106.42912   \n",
      "4      ID       Lemhi  1.706020e+11     8289.0  44.45615 -113.30097   \n",
      "\n",
      "   dataTimeZone  pedonCode shefId         beginDate     endDate  \n",
      "0           NaN        NaN    NaN  1963-02-01 00:00  2100-01-01  \n",
      "1           NaN        NaN  ABDI1  1914-01-01 00:00  2100-01-01  \n",
      "2           NaN        NaN  ABIN5  1957-01-01 00:00  2100-01-01  \n",
      "3           NaN        NaN    NaN  1964-09-01 00:00  2100-01-01  \n",
      "4           NaN        NaN  ABGI1  1961-01-01 00:00  2100-01-01  \n",
      "     station_name        date    lat        lon swe_value change_in_swe_inch  \\\n",
      "0  Adams Ranch #1  2022-01-03  34.25 -105.41667                                \n",
      "1  Adams Ranch #1  2022-01-04  34.25 -105.41667                                \n",
      "2  Adams Ranch #1  2022-01-05  34.25 -105.41667                                \n",
      "3  Adams Ranch #1  2022-01-06  34.25 -105.41667                                \n",
      "4  Adams Ranch #1  2022-01-07  34.25 -105.41667                                \n",
      "\n",
      "  snow_depth air_temperature_observed_f  \n",
      "0                                  21.7  \n",
      "1                                  36.9  \n",
      "2                                  42.4  \n",
      "3                                  45.3  \n",
      "4                                  45.7  \n"
     ]
    }
   ],
   "source": [
    "def get_swe_observations_from_snotel_cdec():\n",
    "    new_base_station_list_file = f\"{working_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\n",
    "    new_base_df = pd.read_csv(new_base_station_list_file)\n",
    "    print(new_base_df.head())\n",
    "  \t\n",
    "    \n",
    "    csv_file = f'{new_base_station_list_file}_swe_restored_dask_all_vars.csv'\n",
    "    start_date = train_start_date\n",
    "    end_date = train_end_date\n",
    "\t\n",
    "    # Create an empty Pandas DataFrame with the desired columns\n",
    "    result_df = pd.DataFrame(columns=[\n",
    "      'station_name', \n",
    "      'date', \n",
    "      'lat', \n",
    "      'lon', \n",
    "      'swe_value', \n",
    "      'change_in_swe_inch', \n",
    "      'snow_depth', \n",
    "      'change_in_swe_inch', \n",
    "      'air_temperature_observed_f'\n",
    "    ])\n",
    "\n",
    "    # Function to process each station\n",
    "    @dask.delayed\n",
    "    def process_station(station):\n",
    "        location_name = station['name']\n",
    "        location_triplet = station['stationTriplet']\n",
    "        location_elevation = station['elevation']\n",
    "        location_station_lat = station['latitude']\n",
    "        location_station_long = station['longitude']\n",
    "\n",
    "        url = f\"https://wcc.sc.egov.usda.gov/reportGenerator/view_csv/customSingleStationReport/daily/{location_triplet}%7Cid%3D%22%22%7Cname/{start_date},{end_date}%2C0/WTEQ%3A%3Avalue%2CWTEQ%3A%3Adelta%2CSNWD%3A%3Avalue%2CSNWD%3A%3Adelta%2CTOBS%3A%3Avalue\"\n",
    "\n",
    "        r = requests.get(url)\n",
    "        text = remove_commented_lines(r.text)\n",
    "        reader = csv.DictReader(io.StringIO(text))\n",
    "        json_data = json.loads(json.dumps(list(reader)))\n",
    "\n",
    "        entries = []\n",
    "        \n",
    "        for entry in json_data:\n",
    "            try:\n",
    "              # {'Date': '2021-06-18', 'Snow Water Equivalent (in) Start of Day Values': '', 'Change In Snow Water Equivalent (in)': '', 'Snow Depth (in) Start of Day Values': '', 'Change In Snow Depth (in)': '', 'Air Temperature Observed (degF) Start of Day Values': '70.5'}\n",
    "              required_data = {\n",
    "                'station_name': location_name,\n",
    "                'date': entry.get('Date', None),\n",
    "                'lat': location_station_lat, \n",
    "                'lon': location_station_long,\n",
    "                'swe_value': entry.get('Snow Water Equivalent (in) Start of Day Values',None),\n",
    "                'change_in_swe_inch': entry.get('Change In Snow Water Equivalent (in)', None),\n",
    "                'snow_depth': entry.get('Snow Depth (in) Start of Day Values',None),\n",
    "                'change_in_swe_inch': entry.get('Change In Snow Depth (in)', None),\n",
    "                'air_temperature_observed_f': entry.get('Air Temperature Observed (degF) Start of Day Values',None)\n",
    "              }\n",
    "              entries.append(required_data)\n",
    "            except Exception as e:\n",
    "              print(\"entry = \", entry)\n",
    "              raise e\n",
    "        return pd.DataFrame(entries)\n",
    "\n",
    "    # List of delayed computations for each station\n",
    "    delayed_results = [process_station(row) for _, row in new_base_df.iterrows()]\n",
    "\n",
    "    # Compute the delayed results\n",
    "    result_lists = dask.compute(*delayed_results)\n",
    "\n",
    "    # Concatenate the lists into a Pandas DataFrame\n",
    "    result_df = pd.concat(result_lists, ignore_index=True)\n",
    "\n",
    "    # Print the final Pandas DataFrame\n",
    "    print(result_df.head())\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    result_df.to_csv(csv_file, index=False)\n",
    "\n",
    "\n",
    "get_swe_observations_from_snotel_cdec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d98720",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
