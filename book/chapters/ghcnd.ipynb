{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GHCNd: Global Historical Climatology Network Daily\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accurate Snow Water Equivalent (SWE) predictions rely heavily on reliable input data, including snow depth measurements. This section details the process of retrieving, processing, and refining snow depth data from the Global Historical Climatology Network Daily (GHCNd) dataset. The script discussed here automates the retrieval of snow depth data from ground stations, filters and cleans this data, and prepares it for use in SWE prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The GHCNd dataset provides daily climate records from numerous weather stations worldwide. This script specifically targets snow depth data (SNWD) from active ground stations within a defined geographical area. The process involves several key steps:\n",
    "\n",
    "1. Downloading and Filtering Station Data: Identifying active stations within the specified region that report snow depth measurements.\n",
    "2. Retrieving Snow Depth Observations: Downloading and processing snow depth data for the specified training period.\n",
    "3. Masking Non-Snow Days: Refining the dataset to exclude non-snow days, focusing the data on relevant observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What data is being downloaded?\n",
    " - **Source:** The data is sourced from the Global Historical Climatology Network Daily (GHCNd), a comprehensive dataset managed by the National Centers for Environmental Information (NCEI). It provides historical daily climate data from thousands of stations worldwide.\n",
    " - **Features:** The key feature targeted by this script is Snow Depth (SNWD), which is the depth of snow on the ground at the time of observation, typically measured in millimeters.\n",
    " - **Station Metadata:** Additional metadata about each station includes the station ID, geographical coordinates (latitude and longitude), and the operational period of the station (start and end years)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Filtering Data\n",
    "\n",
    "The first step in this process is to download the list of all GHCNd stations and filter them based on geographical and operational criteria. This ensures that only active stations within the region of interest are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Converting Station Data\n",
    "\n",
    "The script begins by downloading the GHCNd station inventory file and converting it into a usable format. The relevant stations are then filtered based on their operational status (active in 2024) and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_convert_and_read():\n",
    "  \n",
    "    url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\"\n",
    "    # Download the text file from the URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error: Failed to download the file.\")\n",
    "        return None\n",
    "    \n",
    "    # Parse the text content into columns using regex\n",
    "    pattern = r\"(\\S+)\\s+\"\n",
    "    parsed_data = re.findall(pattern, response.text)\n",
    "    print(\"len(parsed_data) = \", len(parsed_data))\n",
    "    \n",
    "    # Create a new list to store the rows\n",
    "    rows = []\n",
    "    for i in range(0, len(parsed_data), 6):\n",
    "        rows.append(parsed_data[i:i+6])\n",
    "    \n",
    "    print(\"rows[0:20] = \", rows[0:20])\n",
    "    # Convert the rows into a CSV-like format\n",
    "    csv_data = \"\\n\".join([\",\".join(row) for row in rows])\n",
    "    \n",
    "    # Save the CSV-like string to a file\n",
    "    with open(all_ghcd_station_file, \"w\") as file:\n",
    "        file.write(csv_data)\n",
    "    \n",
    "    # Read the CSV-like data into a pandas DataFrame\n",
    "    column_names = ['Station', 'Latitude', 'Longitude', 'Variable', 'Year_Start', 'Year_End']\n",
    "    df = pd.read_csv(all_ghcd_station_file, header=None, names=column_names)\n",
    "    print(df.head())\n",
    "    \n",
    "    # Remove rows where the last column is not equal to \"2024\"\n",
    "    df = df[(df['Year_End'] == 2024) & (df['Variable']=='SNWD')]\n",
    "    print(\"Removed non-active stations: \", df.head())\n",
    "    \n",
    "    # Filter rows within the latitude and longitude ranges\n",
    "    df = df[\n",
    "      (df['Latitude'] >= southwest_lat) & (df['Latitude'] <= northeast_lat) &\n",
    "      (df['Longitude'] >= southwest_lon) & (df['Longitude'] <= northeast_lon)\n",
    "    ]\n",
    "    \n",
    "    df.to_csv(only_active_ghcd_station_in_west_conus_file, index=False)\n",
    "    print(f\"saved to {only_active_ghcd_station_in_west_conus_file}\")\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **URL Download:** The script begins by downloading the GHCNd station inventory file from the National Centers for Environmental Information (NCEI) website.\n",
    " - **Data Parsing and Formatting:** The raw text data is parsed using regular expressions and converted into a CSV format that can be easily processed by pandas.\n",
    " - **Filtering:** The script filters stations based on their operational status and geographical location, focusing on active stations within the defined latitude and longitude bounds.\n",
    " - **Saving Processed Data:** The filtered station data is saved to a CSV file, which will be used in the subsequent steps to retrieve snow depth observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Saving Active Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering, the script saves the list of relevant stations, which will be used in subsequent steps to retrieve snow depth data. This step is crucial for ensuring that the data retrieval process focuses only on the most relevant stations, minimizing unnecessary data processing.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "- **Geographical Filtering:** Ensures that only stations within the defined latitudinal and longitudinal bounds are included.\n",
    "- **Activity Status:** Filters out stations that are no longer active, ensuring the data is current and relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Snow Depth Observations\n",
    "\n",
    "With the relevant stations identified, the next step is to retrieve snow depth observations from the GHCNd dataset for the specified training period. This involves downloading snow depth data from each station and processing it to ensure it is suitable for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Snow Depth Data\n",
    "\n",
    "The script uses Dask to parallelize the data retrieval process, efficiently downloading snow depth records from each station within the defined period. Dask is a powerful tool for handling large datasets, allowing for the concurrent execution of multiple data retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snow_depth_observations_from_ghcn():\n",
    "    \n",
    "    new_base_df = pd.read_csv(only_active_ghcd_station_in_west_conus_file)\n",
    "    print(new_base_df.shape)\n",
    "    \n",
    "    start_date = train_start_date\n",
    "    end_date = train_end_date\n",
    "\t\n",
    "    # Create an empty Pandas DataFrame with the desired columns\n",
    "    result_df = pd.DataFrame(columns=[\n",
    "      'station_name', \n",
    "      'date', \n",
    "      'lat', \n",
    "      'lon', \n",
    "      'snow_depth',\n",
    "    ])\n",
    "    \n",
    "    train_start_date_obj = pd.to_datetime(train_start_date)\n",
    "    train_end_date_obj = pd.to_datetime(train_end_date)\n",
    "\n",
    "    # Function to process each station\n",
    "    @dask.delayed\n",
    "    def process_station(station):\n",
    "        station_name = station['Station']\n",
    "        print(f\"retrieving for {station_name}\")\n",
    "        station_lat = station['Latitude']\n",
    "        station_long = station['Longitude']\n",
    "        try:\n",
    "          url = f\"https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/{station_name}.csv\"\n",
    "          response = requests.get(url)\n",
    "          df = pd.read_csv(StringIO(response.text))\n",
    "          #\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNOW\",\"SNOW_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"PGTM\",\"PGTM_ATTRIBUTES\",\"WDFG\",\"WDFG_ATTRIBUTES\",\"WSFG\",\"WSFG_ATTRIBUTES\",\"WT03\",\"WT03_ATTRIBUTES\",\"WT08\",\"WT08_ATTRIBUTES\",\"WT16\",\"WT16_ATTRIBUTES\"\n",
    "          columns_to_keep = ['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD']\n",
    "          df = df[columns_to_keep]\n",
    "          # Convert the date column to datetime objects\n",
    "          df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "          # Filter rows based on the training period\n",
    "          df = df[(df['DATE'] >= train_start_date_obj) & (df['DATE'] <= train_end_date_obj)]\n",
    "          # print(df.head())\n",
    "          return df\n",
    "        except Exception as e:\n",
    "          print(\"An error occurred:\", e)\n",
    "\n",
    "    # List of delayed computations for each station\n",
    "    delayed_results = [process_station(row) for _, row in new_base_df.iterrows()]\n",
    "\n",
    "    # Compute the delayed results\n",
    "    result_lists = dask.compute(*delayed_results)\n",
    "\n",
    "    # Concatenate the lists into a Pandas DataFrame\n",
    "    result_df = pd.concat(result_lists, ignore_index=True)\n",
    "\n",
    "    # Print the final Pandas DataFrame\n",
    "    print(result_df.head())\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    result_df.to_csv(snowdepth_csv_file, index=False)\n",
    "    print(f\"All the data are saved to {snowdepth_csv_file}\")\n",
    "#     result_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Station Processing with Dask:** Each station's data is processed using Dask's delayed function, allowing for parallel data retrieval. This significantly speeds up the process, especially when dealing with a large number of stations.\n",
    " - **Data Features:** \n",
    " The primary features extracted from each station's dataset are:\n",
    "    - STATION: The unique identifier for the weather station.\n",
    "    - DATE: The date of the observation.\n",
    "    - LATITUDE: The latitude of the station.\n",
    "    - LONGITUDE: The longitude of the station.\n",
    "    - SNWD: Snow depth in millimeters.\n",
    " - **Data Filtering:** For each station, the script retrieves snow depth data (SNWD) and filters it based on the specified training period. This ensures that only relevant data is included in the final dataset.\n",
    " - **Data Concatenation and Saving:** The retrieved data is concatenated into a single DataFrame and saved to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final snow depth data, now filtered and cleaned, is saved to a CSV file. This file serves as the foundation for subsequent analyses and SWE prediction tasks. By automating the data retrieval and cleaning process, the script ensures that the dataset is ready for immediate use in model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking Non-Snow Days\n",
    "\n",
    "To refine the dataset further, the script masks out days with no snow depth observations. This step is important for focusing the analysis on days where snow depth measurements are meaningful, excluding days with zero or missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_out_all_non_zero_snowdepth_days():\n",
    "    print(f\"reading {snowdepth_csv_file}\")\n",
    "    df = pd.read_csv(snowdepth_csv_file)\n",
    "    # Create the new column 'swe_value' and assign values based on conditions\n",
    "    df['swe_value'] = 0  # Initialize all values to 0\n",
    "\n",
    "    # Assign NaN to 'swe_value' where 'snow_depth' is non-zero\n",
    "    df.loc[df['SNWD'] != 0, 'swe_value'] = -999\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(df.head())\n",
    "    df.to_csv(mask_non_snow_days_ghcd_csv_file, index=False)\n",
    "    print(f\"The masked non snow var file is saved to {mask_non_snow_days_ghcd_csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/GHCNd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Masking:** The function creates a new column, swe_value, and assigns it a default value of 0. Where snow depth (SNWD) is non-zero, the swe_value is set to -999, effectively masking out these records.\n",
    "- **Refinement for Analysis:** This refinement step ensures that the dataset focuses on periods with significant snow depth, which are most relevant for SWE predictions. The masked data is then saved to a new CSV file, ready for use in further processing or analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
