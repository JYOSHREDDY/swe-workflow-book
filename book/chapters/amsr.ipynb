{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b77e80",
   "metadata": {},
   "source": [
    "# Advanced Microwave Scanning Radiometer(AMSR)\n",
    "## Introduction:\n",
    "\n",
    "The Advanced Microwave Scanning Radiometer (AMSR) is a satellite-based instrument designed to observe Earth's surface and atmosphere using microwave frequencies. AMSR produces various data products related to weather and climate, including measurements of snow water equivalent (SWE), soil moisture, sea ice concentration, rainfall rates, and ocean surface wind speeds.The AMSR data, collected by satellite-based instruments like the Advanced Microwave Scanning Radiometer (AMSR), is utilized within the snowcast_wormhole workflow to enhance snowfall prediction accuracy.\n",
    "\n",
    "## AMSR Data download\n",
    "In the first step of the data download, we gather the links to daily snow data files from AMSR using the script present in **amsr_swe_data_download** file. We do this by using Python's datetime module to create web links for each day within a range of years. This ensures that we have access to all the necessary data for analysis. By carefully creating these links, we make sure that we can easily get the data we need for predicting snowfall accurately.The **generate_links** function creates download links for AMSR daily snow data files spanning a specified range of years. It initializes variables for the base URL, date format, and time delta. Using a while loop, it iterates through each date within the given years, formats the date string, and constructs the download link by appending the date to the base URL. Finally, it returns a list of download links covering the entire specified time period. This function streamlines the process of generating download links for accessing AMSR snow data.  \n",
    "After gathering the links to AMSR daily snow data files, the next step is to download these files for further analysis.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ce42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def generate_links(start_year, end_year):\n",
    "    '''\n",
    "    Generate a list of download links for AMSR daily snow data files.\n",
    "\n",
    "    Args:\n",
    "        start_year (int): The starting year.\n",
    "        end_year (int): The ending year (inclusive).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of download links for AMSR daily snow data files.\n",
    "    '''\n",
    "    base_url = \"https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/\"\n",
    "    date_format = \"%Y.%m.%d\"\n",
    "    delta = timedelta(days=1)\n",
    "\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year + 1, 1, 1)\n",
    "\n",
    "    links = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date < end_date:\n",
    "        date_str = current_date.strftime(date_format)\n",
    "        link = base_url + date_str + \"/AMSR_U2_L3_DailySnow_B02_\" + date_str + \".he5\"\n",
    "        links.append(link)\n",
    "        current_date += delta\n",
    "\n",
    "    return links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_year = 2019\n",
    "    end_year = 2022\n",
    "\n",
    "    links = generate_links(start_year, end_year)\n",
    "    save_location = \"C:/Users/srila/gridmet_test_run/amsr\"\n",
    "    with open(\"C:/Users/srila/gridmet_test_run/amsr/download_links.txt\", \"w\") as txt_file:\n",
    "      for l in links:\n",
    "        txt_file.write(\" \".join(l) + \"\\n\")\n",
    "\n",
    "    #if not os.path.exists(save_location):\n",
    "    #    os.makedirs(save_location)\n",
    "\n",
    "    #for link in links:\n",
    "    #    filename = link.split(\"/\")[-1]\n",
    "    #    save_path = os.path.join(save_location, filename)\n",
    "    #    curl_cmd = f\"curl -b ~/.urs_cookies -c ~/.urs_cookies -L -n -o {save_path} {link}\"\n",
    "    #    subprocess.run(curl_cmd, shell=True, check=True)\n",
    "        # print(f\"Downloaded: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268fe3d",
   "metadata": {},
   "source": [
    "The **perform_download.sh** script utilizes the wget command-line tool to automate the downloading process. The download_links.txt file contains URLs pointing to the AMSR data files. The script reads each URL from this file, one by one, and downloads the corresponding data file using wget. Before downloading, it sets up some common options for wget, such as authentication credentials, cookie handling, and specifying the output directory. By executing this shell script, we efficiently fetch the required data files and store them in the designated output directory, ensuring that we have the necessary data ready for analysis and modeling tasks in the subsequent steps of our workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ef3e2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Specify the file containing the download links\n",
    "input_file=\"C:/Users/srila/gridmet_test_run/amsr/download_links.txt\"\n",
    "\n",
    "# Specify the base wget command with common options\n",
    "base_wget_command = \"wget --http-user=<your_username> --http-password=<your_password> --load-cookies C:/Users/srila/gridmet_test_run/amsr/mycookies.txt --save-cookies mycookies.txt --keep-session-cookies --no-check-certificate -\"\n",
    "\n",
    "# Specify the output directory for downloaded files\n",
    "output_directory=\"C:/Users/srila/gridmet_test_run/amsr\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "mkdir -p \"$output_directory\"\n",
    "\n",
    "# Loop through each line (URL) in the input file and download it using wget\n",
    "while IFS= read -r url; do\n",
    "    echo \"Downloading: $url\"\n",
    "    $base_wget_command -P \"$output_directory\" \"$url\"\n",
    "done < \"$input_file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f953a4",
   "metadata": {},
   "source": [
    "## Extracting Features from AMSR data:\n",
    "Once the AMSR data files are downloaded, the next step involves extracting relevant features from these files. The script named **amsr_features**, accomplishes this task by processing each AMSR data file and extracting snow water equivalent (SWE) values for specific grid cells corresponding to SNOTEL weather stations. Lets breakdown each step involved in feature extraction. \n",
    "###  copy_he5_files(source_dir, destination_dir)\n",
    "The function **copy_he5_files(source_dir, destination_dir)** is used to copy files with the extension \".he5\" from a specified source directory to a destination directory. It is used to transfer files containing AMSR data from one location to another, for organization or preprocessing purposes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c76b3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def copy_he5_files(source_dir, destination_dir):\n",
    "    '''\n",
    "    Copy .he5 files from the source directory to the destination directory.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): The source directory containing .he5 files to copy.\n",
    "        destination_dir (str): The destination directory where .he5 files will be copied.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Get a list of all subdirectories and files in the source directory\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.he5'):\n",
    "                # Get the absolute path of the source file\n",
    "                source_file_path = os.path.join(root, file)\n",
    "                # Copy the file to the destination directory\n",
    "                shutil.copy(source_file_path, destination_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e1f179",
   "metadata": {},
   "source": [
    "### find_closest_index\n",
    "The function **find_closest_index** in the file is used to find the closest grid cell in an AMSR dataset to the latitude and longitude coordinates of each SNOTEL station, facilitating the mapping of AMSR data to specific geographic locations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee15083",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n",
    "    '''\n",
    "    Find the index of the grid cell with the closest coordinates to the target latitude and longitude.\n",
    "\n",
    "    Args:\n",
    "        target_latitude (float): The target latitude.\n",
    "        target_longitude (float): The target longitude.\n",
    "        lat_grid (numpy.ndarray): An array of latitude values.\n",
    "        lon_grid (numpy.ndarray): An array of longitude values.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int, float, float]: A tuple containing the row index, column index, closest latitude, and closest longitude.\n",
    "    '''\n",
    "    # Compute the absolute differences between target and grid coordinates\n",
    "    lat_diff = np.abs(lat_grid - target_latitude)\n",
    "    lon_diff = np.abs(lon_grid - target_longitude)\n",
    "\n",
    "    # Find the indices corresponding to the minimum differences\n",
    "    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n",
    "\n",
    "    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580125e7",
   "metadata": {},
   "source": [
    "### create_snotel_station_to_amsr_mapper\n",
    "The **create_snotel_station_to_amsr_mapper** function reads a list of SNOTEL (Snow Telemetry) station locations from a CSV file and maps each station to the corresponding grid cell in an AMSR (Advanced Microwave Scanning Radiometer) dataset. The function calculates the closest grid cell in the AMSR dataset to each SNOTEL station using the **find_closest_index** function. It then creates a new CSV file containing the mapped coordinates of each SNOTEL station along with the corresponding coordinates of the closest grid cell in the AMSR dataset. If the CSV file already exists, the function reads it directly instead of recalculating the mappings. Additionally, the function downloads the required AMSR dataset if it is not already available locally. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce19d9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def create_snotel_station_to_amsr_mapper(\n",
    "  new_base_station_list_file, \n",
    "  target_csv_path\n",
    "):\n",
    "    station_data = pd.read_csv(new_base_station_list_file)\n",
    "    \n",
    "    \n",
    "    date = \"2022-10-01\"\n",
    "    date = date.replace(\"-\", \".\")\n",
    "    he5_date = date.replace(\".\", \"\")\n",
    "    \n",
    "    # Check if the CSV already exists\n",
    "    \n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {target_csv_path} already exists, skipping..\")\n",
    "        df = pd.read_csv(target_csv_path)\n",
    "        return df\n",
    "    \n",
    "    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n",
    "    if os.path.exists(target_amsr_hdf_path):\n",
    "        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n",
    "    else:\n",
    "        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n",
    "        print(f'Running command: {cmd}')\n",
    "        subprocess.run(cmd, shell=True)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n",
    "                               'amsr_lat_idx', 'amsr_lon_idx',\n",
    "                               'station_lat', 'station_lon'])\n",
    "    # Read the HDF\n",
    "    file = h5py.File(target_amsr_hdf_path, 'r')\n",
    "    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "    lat = hem_group['lat'][:]\n",
    "    lon = hem_group['lon'][:]\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    lat = np.nan_to_num(lat, nan=0.0)\n",
    "    lon = np.nan_to_num(lon, nan=0.0)\n",
    "    \n",
    "    # Convert the AMSR grid into our gridMET 1km grid\n",
    "    for idx, row in station_data.iterrows():\n",
    "        target_lat = row['latitude']\n",
    "        target_lon = row['longitude']\n",
    "        \n",
    "        # compare the performance and find the fastest way to search nearest point\n",
    "        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n",
    "        df.loc[len(df.index)] = [closest_lat, \n",
    "                                 closest_lon,\n",
    "                                 closest_lat_idx,\n",
    "                                 closest_lon_idx,\n",
    "                                 target_lat,\n",
    "                                 target_lon]\n",
    "    \n",
    "    # Save the new converted AMSR to CSV file\n",
    "    df.to_csv(target_csv_path, index=False)\n",
    "  \n",
    "    print('AMSR mapper csv is created.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97be1bd",
   "metadata": {},
   "source": [
    "### extract_amsr_values_save_to_csv\n",
    "The **extract_amsr_values_save_to_csv** function processes AMSR (Advanced Microwave Scanning Radiometer) data files and extracts relevant information to create a CSV file. It first prepares a mapping between SNOTEL (Snow Telemetry) station coordinates and the corresponding grid cells in the AMSR dataset using the create_snotel_station_to_amsr_mapper function. Then, it iterates through the AMSR data files, extracts the snow water equivalent (SWE) values for each SNOTEL station's mapped grid cell, and saves the results along with the corresponding dates and station coordinates to a CSV file. The function handles parallel processing of multiple data files using Dask, a Python library for parallel computing.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93048e0f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def extract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file, new_base_station_list_file, start_date, end_date):\n",
    "    if os.path.exists(output_csv_file):\n",
    "        os.remove(output_csv_file)\n",
    "    \n",
    "    target_csv_path = f'{work_dir}/training_snotel_station_to_amsr_mapper.csv'\n",
    "    mapper_df = create_snotel_station_to_amsr_mapper(new_base_station_list_file, \n",
    "                                         target_csv_path)\n",
    "        \n",
    "    # station_data = pd.read_csv(new_base_station_list_file)\n",
    "\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    # Create a Dask DataFrame\n",
    "    dask_station_data = dd.from_pandas(mapper_df, npartitions=1)\n",
    "\n",
    "    # Function to process each file\n",
    "    def process_file(filename):\n",
    "        file_path = os.path.join(amsr_data_dir, filename)\n",
    "        print(file_path)\n",
    "        \n",
    "        file = h5py.File(file_path, 'r')\n",
    "        hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "\n",
    "        date_str = filename.split('_')[-1].split('.')[0]\n",
    "        date = datetime.strptime(date_str, '%Y%m%d')\n",
    "\n",
    "        if not (start_date <= date <= end_date):\n",
    "            print(f\"{date} is not in the training period, skipping..\")\n",
    "            return None\n",
    "\n",
    "        new_date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n",
    "        flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n",
    "        # Create an empty Pandas DataFrame with the desired columns\n",
    "        result_df = pd.DataFrame(columns=['date', 'lat', 'lon', 'AMSR_SWE'])\n",
    "\n",
    "        # Sample loop to add rows to the Pandas DataFrame using dask.delayed\n",
    "        @delayed\n",
    "        def process_row(row, swe, new_date_str):\n",
    "          closest_lat_idx = int(row['amsr_lat_idx'])\n",
    "          closest_lon_idx = int(row['amsr_lon_idx'])\n",
    "          closest_swe = swe[closest_lat_idx, closest_lon_idx]\n",
    "          \n",
    "          return pd.DataFrame([[\n",
    "            new_date_str, \n",
    "            row['station_lat'],\n",
    "            row['station_lon'],\n",
    "            closest_swe]], \n",
    "            columns=result_df.columns\n",
    "          )\n",
    "\n",
    "\n",
    "        # List of delayed computations\n",
    "        delayed_results = [process_row(row, swe, new_date_str) for _, row in mapper_df.iterrows()]\n",
    "\n",
    "        # Compute the delayed results and concatenate them into a Pandas DataFrame\n",
    "        result_df = dask.compute(*delayed_results)\n",
    "        result_df = pd.concat(result_df, ignore_index=True)\n",
    "\n",
    "        # Print the final Pandas DataFrame\n",
    "        #print(result_df)\n",
    "          \n",
    "        return result_df\n",
    "\n",
    "    # Get the list of files\n",
    "    files = [f for f in os.listdir(amsr_data_dir) if f.endswith('.he5')]\n",
    "\n",
    "    # Create a Dask Bag from the files\n",
    "    dask_bag = db.from_sequence(files, npartitions=2)\n",
    "\n",
    "    # Process files in parallel\n",
    "    processed_data = dask_bag.map(process_file).filter(lambda x: x is not None).compute()\n",
    "\n",
    "    # Concatenate the processed data\n",
    "    combined_df = pd.concat(processed_data, ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    combined_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "    print(f\"Merged data saved to {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef32bcd",
   "metadata": {},
   "source": [
    "Overall, the   automates the process of extracting relevant AMSR data features and integrating them with SNOTEL station data, streamlining the workflow for further analysis and modeling tasks related to snowfall prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed4b18",
   "metadata": {},
   "source": [
    "## Automating Real-time Retrieval and Processing of AMSR Snow Data\n",
    "The script named amsr_testing_realtime serves the purpose of automating the retrieval and processing of AMSR snow data in near real-time. It facilitates the download of AMSR snow data for a specified date, converts it into a format compatible with Digital Elevation Model (DEM), and subsequently saves it as a CSV file. The script's functionalities encompass downloading AMSR data from an online repository, handling HDF5 file formats using the h5py library, converting AMSR grid data to a format suitable for analysis, and addressing missing values in the dataset through interpolation techniques. To execute these tasks, the script utilizes various libraries including pandas for efficient data manipulation, scipy for spatial operations, and subprocess for executing shell commands. Additionally, it offers flexibility in customizing the target date for data retrieval and processing to suit specific requirements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ff44d",
   "metadata": {},
   "source": [
    "### prepare_amsr_grid_mapper:\n",
    "This function **prepare_amsr_grid_mapper** is responsible for creating a mapping between the AMSR (Advanced Microwave Scanning Radiometer) grid and the GridMET (Grid Point Surface Meteorological Data) grid, facilitating the conversion of AMSR snow data into a format compatible with GridMET. It first reads the AMSR data from a specified HDF5 file, extracting latitude and longitude information. Then, it retrieves the coordinates of GridMET grid points representing stations in the Western US. By finding the nearest AMSR grid points to each GridMET station, it establishes a mapping, storing the relevant information such as latitude, longitude, and grid indices in a CSV file. This mapping is crucial for subsequent steps in the script, enabling efficient retrieval and processing of AMSR snow data for specific locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee00f36",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_amsr_grid_mapper():\n",
    "    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n",
    "                               'amsr_lat_idx', 'amsr_lon_idx',\n",
    "                               'gridmet_lat', 'gridmet_lon'])\n",
    "    date = test_start_date\n",
    "    date = date.replace(\"-\", \".\")\n",
    "    he5_date = date.replace(\".\", \"\")\n",
    "    \n",
    "    # Check if the CSV already exists\n",
    "    target_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {target_csv_path} already exists, skipping..\")\n",
    "        return\n",
    "    \n",
    "    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n",
    "    if os.path.exists(target_amsr_hdf_path):\n",
    "        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n",
    "    else:\n",
    "        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n",
    "        print(f'Running command: {cmd}')\n",
    "        subprocess.run(cmd, shell=True)\n",
    "    \n",
    "    # Read the HDF\n",
    "    file = h5py.File(target_amsr_hdf_path, 'r')\n",
    "    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "    lat = hem_group['lat'][:]\n",
    "    lon = hem_group['lon'][:]\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    lat = np.nan_to_num(lat, nan=0.0)\n",
    "    lon = np.nan_to_num(lon, nan=0.0)\n",
    "    \n",
    "    # Convert the AMSR grid into our gridMET 1km grid\n",
    "    western_us_df = pd.read_csv(western_us_coords)\n",
    "    for idx, row in western_us_df.iterrows():\n",
    "        target_lat = row['Latitude']\n",
    "        target_lon = row['Longitude']\n",
    "        \n",
    "        # compare the performance and find the fastest way to search nearest point\n",
    "        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n",
    "        df.loc[len(df.index)] = [closest_lat, \n",
    "                                 closest_lon,\n",
    "                                 closest_lat_idx,\n",
    "                                 closest_lon_idx,\n",
    "                                 target_lat, \n",
    "                                 target_lon]\n",
    "    \n",
    "    # Save the new converted AMSR to CSV file\n",
    "    df.to_csv(target_csv_path, index=False)\n",
    "  \n",
    "    print('AMSR mapper csv is created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65dccd",
   "metadata": {},
   "source": [
    "### download_amsr_and_convert_grid\n",
    "The download_amsr_and_convert_grid function is a core component of the \"amsr_testing_realtime\" script, responsible for downloading AMSR (Advanced Microwave Scanning Radiometer) snow data for a specific date and converting it into a format compatible with DEM (Digital Elevation Model) grids.\n",
    "Here's a breakdown of its functionality:\n",
    "\n",
    "**Download AMSR Data**: This function first constructs the URL for the AMSR data corresponding to the specified date and attempts to download it using the curl command. It ensures that the necessary cookies are available for authentication.\n",
    "\n",
    "**Read HDF Data**: Once the data is downloaded, the function reads the HDF5 file using the h5py library, extracting the latitude, longitude, snow water equivalent (SWE), and flag information from the file.\n",
    "\n",
    "**Data Conversion**: It then converts the AMSR grid into a format compatible with the DEM grid by finding the corresponding grid points in the DEM grid. This involves identifying the nearest DEM grid points for each AMSR grid point.\n",
    "\n",
    "**Custom Calculation**: For each DEM grid point, the function performs a custom calculation to determine the SWE and flag values based on the nearest AMSR grid points.\n",
    "\n",
    "**Save to CSV**: Finally, the function saves the converted data, including the latitude, longitude, SWE, and flag information, into a CSV file. This file can be further processed or analyzed in subsequent steps of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_amsr_and_convert_grid(target_date = test_start_date):\n",
    "    \"\"\"\n",
    "    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # the mapper\n",
    "    target_mapper_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n",
    "    mapper_df = pd.read_csv(target_mapper_csv_path)\n",
    "    #print(mapper_df.head())\n",
    "    \n",
    "    df = pd.DataFrame(columns=['date', 'lat', \n",
    "                               'lon', 'AMSR_SWE', \n",
    "                               'AMSR_Flag'])\n",
    "    date = target_date\n",
    "    date = date.replace(\"-\", \".\")\n",
    "    he5_date = date.replace(\".\", \"\")\n",
    "    \n",
    "    # Check if the CSV already exists\n",
    "    target_csv_path = f'{work_dir}/testing_ready_amsr_{date}.csv'\n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {target_csv_path} already exists, skipping..\")\n",
    "        return target_csv_path\n",
    "    \n",
    "    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n",
    "    if os.path.exists(target_amsr_hdf_path) and is_binary(target_amsr_hdf_path):\n",
    "        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n",
    "    else:\n",
    "        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n",
    "        print(f'Running command: {cmd}')\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        # Check the exit code\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Command failed with exit code {result.returncode}.\")\n",
    "            if os.path.exists(target_amsr_hdf_path):\n",
    "              os.remove(target_amsr_hdf_path)\n",
    "              print(f\"Wrong {target_amsr_hdf_path} removed successfully.\")\n",
    "            raise Exception(f\"Failed to download {target_amsr_hdf_path} - {result.stderr}\")\n",
    "    \n",
    "    # Read the HDF\n",
    "    print(f\"Reading {target_amsr_hdf_path}\")\n",
    "    file = h5py.File(target_amsr_hdf_path, 'r')\n",
    "    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "    lat = hem_group['lat'][:]\n",
    "    lon = hem_group['lon'][:]\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    lat = np.nan_to_num(lat, nan=0.0)\n",
    "    lon = np.nan_to_num(lon, nan=0.0)\n",
    "    \n",
    "    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n",
    "    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n",
    "    date = datetime.strptime(date, '%Y.%m.%d')\n",
    "    \n",
    "    # Convert the AMSR grid into our DEM 1km grid\n",
    "    \n",
    "    def get_swe(row):\n",
    "        # Perform your custom calculation here\n",
    "        closest_lat_idx = int(row['amsr_lat_idx'])\n",
    "        closest_lon_idx = int(row['amsr_lon_idx'])\n",
    "        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n",
    "        return closest_swe\n",
    "    \n",
    "    def get_swe_flag(row):\n",
    "        # Perform your custom calculation here\n",
    "        closest_lat_idx = int(row['amsr_lat_idx'])\n",
    "        closest_lon_idx = int(row['amsr_lon_idx'])\n",
    "        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n",
    "        return closest_flag\n",
    "    \n",
    "    # Use the apply function to apply the custom function to each row\n",
    "    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n",
    "    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n",
    "    mapper_df['date'] = date\n",
    "    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n",
    "    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n",
    "    mapper_df = mapper_df.drop(columns=['amsr_lat',\n",
    "                                        'amsr_lon',\n",
    "                                        'amsr_lat_idx',\n",
    "                                        'amsr_lon_idx'])\n",
    "    \n",
    "    print(\"result df: \", mapper_df.head())\n",
    "    # Save the new converted AMSR to CSV file\n",
    "    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n",
    "    mapper_df.to_csv(target_csv_path, index=False)\n",
    "    \n",
    "    print('Completed AMSR testing data collection.')\n",
    "    return target_csv_path\n",
    "\n",
    "def add_cumulative_column(df, column_name):\n",
    "    df[f'cumulative_{column_name}'] = df[column_name].sum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c56467",
   "metadata": {},
   "source": [
    "### get_cumulative_amsr_data\n",
    "The **get_cumulative_amsr_data** function in the \"amsr_testing_realtime\" script serves as the backbone for collecting, processing, and aggregating AMSR (Advanced Microwave Scanning Radiometer) snow data. Beginning from October 1st of the previous year up to a specified target date, this function iterates through each date, downloading corresponding AMSR data and converting it to CSV format. It meticulously handles missing values, employing polynomial interpolation techniques while enforcing constraints on snow water equivalent (SWE) values. As it processes data for each date, it aggregates the results into a cohesive DataFrame, incorporating latitude, longitude, SWE, flags, and cumulative SWE values. The function ensures data integrity and consistency, producing a comprehensive CSV dataset containing cumulative AMSR snow data for the specified target date, essential for subsequent analysis or visualization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f841d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def get_cumulative_amsr_data(target_date = test_start_date, force=False):\n",
    "    \n",
    "    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    print(selected_date)\n",
    "    if selected_date.month < 10:\n",
    "      past_october_1 = datetime(selected_date.year - 1, 10, 1)\n",
    "    else:\n",
    "      past_october_1 = datetime(selected_date.year, 10, 1)\n",
    "\n",
    "    # Traverse and print every day from past October 1 to the specific date\n",
    "    current_date = past_october_1\n",
    "    target_csv_path = f'{work_dir}/testing_ready_amsr_{target_date}_cumulative.csv'\n",
    "\n",
    "    columns_to_be_cumulated = [\"AMSR_SWE\"]\n",
    "    \n",
    "    gap_filled_csv = f\"{target_csv_path}_gap_filled.csv\"\n",
    "    if os.path.exists(gap_filled_csv) and not force:\n",
    "      print(f\"{gap_filled_csv} already exists, skipping..\")\n",
    "      df = pd.read_csv(gap_filled_csv)\n",
    "      print(df[\"AMSR_SWE\"].describe())\n",
    "    else:\n",
    "      date_keyed_objects = {}\n",
    "      data_dict = {}\n",
    "      new_df = None\n",
    "      while current_date <= selected_date:\n",
    "        print(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date_str = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n",
    "        current_df = pd.read_csv(data_dict[current_date_str])\n",
    "        current_df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "        if current_date != selected_date:\n",
    "          current_df.rename(columns={\n",
    "            \"AMSR_SWE\": f\"AMSR_SWE_{current_date_str}\",\n",
    "            \"AMSR_Flag\": f\"AMSR_Flag_{current_date_str}\",\n",
    "          }, inplace=True)\n",
    "        #print(current_df.head())\n",
    "\n",
    "        if new_df is None:\n",
    "          new_df = current_df\n",
    "        else:\n",
    "          new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n",
    "          #new_df = new_df.append(current_df, ignore_index=True)\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "      print(\"new_df.columns = \", new_df.columns)\n",
    "      print(\"new_df.head = \", new_df.head())\n",
    "      df = new_df\n",
    "\n",
    "      #df.sort_values(by=['gridmet_lat', 'gridmet_lon', 'date'], inplace=True)\n",
    "      print(\"All current head: \", df.head())\n",
    "      print(\"the new_df.shape: \", df.shape)\n",
    "\n",
    "      print(\"Start to fill in the missing values\")\n",
    "      #grouped = df.groupby(['gridmet_lat', 'gridmet_lon'])\n",
    "      filled_data = pd.DataFrame()\n",
    "\n",
    "      # Apply the function to each group\n",
    "      for column_name in columns_to_be_cumulated:\n",
    "        start_time = time.time()\n",
    "        #filled_data = df.apply(lambda row: interpolate_missing_and_add_cumulative_inplace(row, column_name), axis=1)\n",
    "        #alike_columns = filled_data.filter(like=column_name)\n",
    "        #filled_data[f'cumulative_{column_name}'] = alike_columns.sum(axis=1)\n",
    "        print(\"filled_data.columns = \", filled_data.columns)\n",
    "        filtered_columns = df.filter(like=column_name)\n",
    "        print(filtered_columns.columns)\n",
    "        filtered_columns = filtered_columns.mask(filtered_columns > 240)\n",
    "        filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n",
    "        filtered_columns.fillna(0, inplace=True)\n",
    "        \n",
    "        sum_column = filtered_columns.sum(axis=1)\n",
    "        # Define a specific name for the new column\n",
    "        df[f'cumulative_{column_name}'] = sum_column\n",
    "        df[filtered_columns.columns] = filtered_columns\n",
    "        \n",
    "        if filtered_columns.isnull().any().any():\n",
    "          print(\"filtered_columns :\", filtered_columns)\n",
    "          raise ValueError(\"Single group: shouldn't have null values here\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Concatenate the original DataFrame with the Series containing the sum\n",
    "        #df = pd.concat([df, sum_column.rename(new_column_name)], axis=1)\n",
    "#         cumulative_column = filled_data.filter(like=column_name).sum(axis=1)\n",
    "#         filled_data[f'cumulative_{column_name}'] = cumulative_column\n",
    "        #filled_data = pd.concat([filled_data, cumulative_column], axis=1)\n",
    "        print(\"filled_data.columns: \", filled_data.columns)\n",
    "        end_time = time.time()\n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"calculate column {column_name} elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "#       if any(filled_data['AMSR_SWE'] > 240):\n",
    "#         raise ValueError(\"Error: shouldn't have AMSR_SWE > 240 at this point\")\n",
    "      filled_data = df\n",
    "      filled_data[\"date\"] = target_date\n",
    "      print(\"Finished correctly \", filled_data.head())\n",
    "      filled_data.to_csv(gap_filled_csv, index=False)\n",
    "      print(f\"New filled values csv is saved to {gap_filled_csv}\")\n",
    "      df = filled_data\n",
    "    \n",
    "    result = df\n",
    "    print(\"result.head = \", result.head())\n",
    "    # fill in the rest NA as 0\n",
    "    if result.isnull().any().any():\n",
    "      print(\"result :\", result)\n",
    "      raise ValueError(\"Single group: shouldn't have null values here\")\n",
    "    \n",
    "    # only retain the rows of the target date\n",
    "    print(result['date'].unique())\n",
    "    print(result.shape)\n",
    "    print(result[[\"AMSR_SWE\", \"AMSR_Flag\"]].describe())\n",
    "    result.to_csv(target_csv_path, index=False)\n",
    "    print(f\"New data is saved to {target_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de48c38",
   "metadata": {},
   "source": [
    "### interpolate_missing_and_add_cumulative_inplace\n",
    "The **interpolate_missing_and_add_cumulative_inplace** function within the \"amsr_testing_realtime\" script serves a crucial role in filling missing values in the AMSR snow data and computing cumulative values. It operates on a row-by-row basis, iterating through each row of the DataFrame containing AMSR data. For a specified column (e.g., snow water equivalent or SWE), it performs polynomial interpolation to estimate missing values, ensuring a continuous and smooth representation of the data. Additionally, it enforces constraints on the interpolated values, ensuring that they fall within reasonable bounds (e.g., 0 to 240 for SWE). After interpolation, the function calculates the cumulative sum of the interpolated values for each row, facilitating the aggregation of cumulative AMSR snow data. By incorporating these operations in place, the function optimizes memory usage and computational efficiency, making it suitable for processing large datasets efficiently within the real-time AMSR data collection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n",
    "  \"\"\"\n",
    "  Interpolate missing values in a Pandas Series using polynomial interpolation\n",
    "  and add a cumulative column.\n",
    "\n",
    "  Parameters:\n",
    "    - row (pd.Series): The input row containing the data to be interpolated.\n",
    "    - column_name (str): The name of the column to be interpolated.\n",
    "    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n",
    "\n",
    "  Returns:\n",
    "    - pd.Series: The row with interpolated values and a cumulative column.\n",
    "\n",
    "  Raises:\n",
    "    - ValueError: If there are unexpected null values after interpolation.\n",
    "\n",
    "  Note:\n",
    "    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n",
    "    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n",
    "\n",
    "  Examples:\n",
    "    ```python\n",
    "    # Example usage:\n",
    "    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n",
    "    ```\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  # Extract X series (column names)\n",
    "  x_all_key = row.index\n",
    "  \n",
    "  x_subset_key = x_all_key[x_all_key.str.startswith(column_name)]\n",
    "  are_all_values_between_0_and_240 = row[x_subset_key].between(1, 239).all()\n",
    "  if are_all_values_between_0_and_240:\n",
    "    print(\"row[x_subset_key] = \", row[x_subset_key])\n",
    "    print(\"row[x_subset_key].sum() = \", row[x_subset_key].sum())\n",
    "  # create the cumulative column after interpolation\n",
    "  row[f\"cumulative_{column_name}\"] = row[x_subset_key].sum()\n",
    "  return row\n",
    "    \n",
    "    \n",
    "def get_cumulative_amsr_data(target_date = test_start_date, force=False):\n",
    "    \n",
    "    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    print(selected_date)\n",
    "    if selected_date.month < 10:\n",
    "      past_october_1 = datetime(selected_date.year - 1, 10, 1)\n",
    "    else:\n",
    "      past_october_1 = datetime(selected_date.year, 10, 1)\n",
    "\n",
    "    # Traverse and print every day from past October 1 to the specific date\n",
    "    current_date = past_october_1\n",
    "    target_csv_path = f'{work_dir}/testing_ready_amsr_{target_date}_cumulative.csv'\n",
    "\n",
    "    columns_to_be_cumulated = [\"AMSR_SWE\"]\n",
    "    \n",
    "    gap_filled_csv = f\"{target_csv_path}_gap_filled.csv\"\n",
    "    if os.path.exists(gap_filled_csv) and not force:\n",
    "      print(f\"{gap_filled_csv} already exists, skipping..\")\n",
    "      df = pd.read_csv(gap_filled_csv)\n",
    "      print(df[\"AMSR_SWE\"].describe())\n",
    "    else:\n",
    "      date_keyed_objects = {}\n",
    "      data_dict = {}\n",
    "      new_df = None\n",
    "      while current_date <= selected_date:\n",
    "        print(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date_str = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n",
    "        current_df = pd.read_csv(data_dict[current_date_str])\n",
    "        current_df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "        if current_date != selected_date:\n",
    "          current_df.rename(columns={\n",
    "            \"AMSR_SWE\": f\"AMSR_SWE_{current_date_str}\",\n",
    "            \"AMSR_Flag\": f\"AMSR_Flag_{current_date_str}\",\n",
    "          }, inplace=True)\n",
    "        #print(current_df.head())\n",
    "\n",
    "        if new_df is None:\n",
    "          new_df = current_df\n",
    "        else:\n",
    "          new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n",
    "          #new_df = new_df.append(current_df, ignore_index=True)\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "      print(\"new_df.columns = \", new_df.columns)\n",
    "      print(\"new_df.head = \", new_df.head())\n",
    "      df = new_df\n",
    "\n",
    "      #df.sort_values(by=['gridmet_lat', 'gridmet_lon', 'date'], inplace=True)\n",
    "      print(\"All current head: \", df.head())\n",
    "      print(\"the new_df.shape: \", df.shape)\n",
    "\n",
    "      print(\"Start to fill in the missing values\")\n",
    "      #grouped = df.groupby(['gridmet_lat', 'gridmet_lon'])\n",
    "      filled_data = pd.DataFrame()\n",
    "\n",
    "      # Apply the function to each group\n",
    "      for column_name in columns_to_be_cumulated:\n",
    "        start_time = time.time()\n",
    "        #filled_data = df.apply(lambda row: interpolate_missing_and_add_cumulative_inplace(row, column_name), axis=1)\n",
    "        #alike_columns = filled_data.filter(like=column_name)\n",
    "        #filled_data[f'cumulative_{column_name}'] = alike_columns.sum(axis=1)\n",
    "        print(\"filled_data.columns = \", filled_data.columns)\n",
    "        filtered_columns = df.filter(like=column_name)\n",
    "        print(filtered_columns.columns)\n",
    "        filtered_columns = filtered_columns.mask(filtered_columns > 240)\n",
    "        filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n",
    "        filtered_columns.fillna(0, inplace=True)\n",
    "        \n",
    "        sum_column = filtered_columns.sum(axis=1)\n",
    "        # Define a specific name for the new column\n",
    "        df[f'cumulative_{column_name}'] = sum_column\n",
    "        df[filtered_columns.columns] = filtered_columns\n",
    "        \n",
    "        if filtered_columns.isnull().any().any():\n",
    "          print(\"filtered_columns :\", filtered_columns)\n",
    "          raise ValueError(\"Single group: shouldn't have null values here\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Concatenate the original DataFrame with the Series containing the sum\n",
    "        #df = pd.concat([df, sum_column.rename(new_column_name)], axis=1)\n",
    "#         cumulative_column = filled_data.filter(like=column_name).sum(axis=1)\n",
    "#         filled_data[f'cumulative_{column_name}'] = cumulative_column\n",
    "        #filled_data = pd.concat([filled_data, cumulative_column], axis=1)\n",
    "        print(\"filled_data.columns: \", filled_data.columns)\n",
    "        end_time = time.time()\n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"calculate column {column_name} elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "#       if any(filled_data['AMSR_SWE'] > 240):\n",
    "#         raise ValueError(\"Error: shouldn't have AMSR_SWE > 240 at this point\")\n",
    "      filled_data = df\n",
    "      filled_data[\"date\"] = target_date\n",
    "      print(\"Finished correctly \", filled_data.head())\n",
    "      filled_data.to_csv(gap_filled_csv, index=False)\n",
    "      print(f\"New filled values csv is saved to {gap_filled_csv}\")\n",
    "      df = filled_data\n",
    "    \n",
    "    result = df\n",
    "    print(\"result.head = \", result.head())\n",
    "    # fill in the rest NA as 0\n",
    "    if result.isnull().any().any():\n",
    "      print(\"result :\", result)\n",
    "      raise ValueError(\"Single group: shouldn't have null values here\")\n",
    "    \n",
    "    # only retain the rows of the target date\n",
    "    print(result['date'].unique())\n",
    "    print(result.shape)\n",
    "    print(result[[\"AMSR_SWE\", \"AMSR_Flag\"]].describe())\n",
    "    result.to_csv(target_csv_path, index=False)\n",
    "    print(f\"New data is saved to {target_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82391f42",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
