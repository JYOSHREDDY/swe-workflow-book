{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b77e80",
   "metadata": {},
   "source": [
    "# 3.6 Advanced Microwave Scanning Radiometer(AMSR)\n",
    "\n",
    "AMSR (Advanced Microwave Scanning Radiometer) is a satellite sensor that measures snow cover and water content by detecting Earth's microwave emissions.\n",
    "\n",
    "This chapter focuses on the use of AMSR satellite data to monitor `snow cover` and `snow water equivalent (SWE)`, key indicators in understanding water resources and climate patterns. \n",
    "\n",
    "This chapter demonstrates the practical applications of transforming AMSR data into actionable information for scientific and environmental purposes.\n",
    "\n",
    "\n",
    "## 3.6.1 Generating Daily Snow Data Links from AMSR\n",
    "\n",
    "Our goal is to generate a list of download links for AMSR daily snow data files for a specified date range.\n",
    "\n",
    "- `AMSR`: Satellite sensor providing daily snow data.\n",
    "- `start_year`, `end_year`: Define the date range for which data links are generated.\n",
    "- `base_url`: The starting point of each download link.\n",
    "- `timedelta`: Used to iterate over each day within the date range.\n",
    "- `generate_links`: Function that builds and returns the list of URLs.\n",
    "\n",
    "\n",
    "Here we create list of download links for `AMSR snow data` by iterating over a date range from the year 2019 to 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "612ce42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def generate_links(start_year, end_year):\n",
    "    '''\n",
    "    Generate a list of download links for AMSR daily snow data files.\n",
    "\n",
    "    Args:\n",
    "        start_year (int): The starting year.\n",
    "        end_year (int): The ending year (inclusive).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of download links for AMSR daily snow data files.\n",
    "    '''\n",
    "    base_url = \"https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/\"\n",
    "    url_date_format = \"%Y.%m.%d\"\n",
    "    file_date_format=\"%Y%m%d\"\n",
    "    delta = timedelta(days=1)\n",
    "\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year + 1, 1, 1)\n",
    "\n",
    "    links = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date < end_date:\n",
    "        url_date_format1 = current_date.strftime(url_date_format)\n",
    "        file_date_format1= current_date.strftime(file_date_format)\n",
    "        link = base_url + url_date_format1 + \"/AMSR_U2_L3_DailySnow_B02_\" + file_date_format1 + \".he5\"\n",
    "        links.append(link)\n",
    "        current_date += delta\n",
    "\n",
    "    return links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_year = 2019\n",
    "    end_year = 2022\n",
    "    work_dir = f\"./gridmet_test_run/amsr\"\n",
    "    links = generate_links(start_year, end_year)\n",
    "    save_location = f'{work_dir}/amsr'\n",
    "    with open(f'{work_dir}/download_links.txt', \"w\") as txt_file:\n",
    "      for l in links:\n",
    "        txt_file.write(l + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268fe3d",
   "metadata": {},
   "source": [
    "## 3.6.2 Automated Script for Secure File Download with Wget from a List of URLs\n",
    "\n",
    "The follwing script is to automate the downloading of files by looping through the links we obtained from `3.6.1`.\n",
    "\n",
    "- `work_dir`: Specifies the working directory where the download links and files will be stored.\n",
    "- `input_file`: The text file containing the list of URLs to be downloaded.\n",
    "- `base_wget_command`: The core command used to download files with options for authentication, session management, and secure connections.\n",
    "- `output_directory`: The folder where the downloaded files will be saved.\n",
    "- Loop: Iterates over each URL in the input file, ensuring all files are downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ef3e2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "work_dir = f\"{homedir}/gridmet_test_run/amsr\"\n",
    "# Specify the file containing the download links\n",
    "\n",
    "input_file=\"${work_dir}/download_links.txt\"\n",
    "\n",
    "\n",
    "# Specify the base wget command with common options\n",
    "base_wget_command=\"wget --http-user=<your_username> --http-password=<your_password> --load-cookies /Users/meghana/Documents/projects/swe-workflow-book/mycookies.txt --save-cookies mycookies.txt --keep-session-cookies --no-check-certificate > /dev/null 2>&1 &\"\n",
    "\n",
    "# Specify the output directory for downloaded files\n",
    "output_directory=\"$work_dir\"\n",
    "\n",
    "\n",
    "# Ensure the output directory exists\n",
    "mkdir -p \"$output_directory\"\n",
    "\n",
    "# Loop through each line (URL) in the input file and download it using wget\n",
    "while IFS= read -r line; do\n",
    "    url=$(echo \"$line\" | tr -d '\\r') #adding this line to remove the carriage return character before using the URL. \n",
    "    $base_wget_command -P \"$output_directory\" \"$url\"\n",
    "done < \"$input_file\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f953a4",
   "metadata": {},
   "source": [
    "## 3.6.3 Extracting Features from AMSR data:\n",
    "\n",
    "Once the AMSR data files are downloaded, the next step is to extract relevant features from these files. \n",
    "\n",
    "The following script accomplishes this task by processing each AMSR data file and extracting `snow water equivalent (SWE)` values for specific grid cells corresponding to SNOTEL weather stations. \n",
    "\n",
    "Lets breakdown each step involved in feature extraction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4266f6f",
   "metadata": {},
   "source": [
    "### 3.6.3.1 Importing required python libraries to run the script\n",
    "\n",
    "- **Importing Libraries**: Essential libraries are imported for handling files, processing large datasets, and performing complex calculations.\n",
    "  - **`os`, `shutil`, `subprocess`**: For file handling, copying, and executing shell commands.\n",
    "  - **`csv`, `h5py`, `numpy`, `pandas`**: For reading/writing files, handling HDF5 datasets, numerical computations, and data manipulation.\n",
    "  - **`dask`, `xarray`**: To manage and process large datasets efficiently using parallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1345cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import h5py\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.delayed as delayed\n",
    "import dask.bag as db\n",
    "import xarray as xr\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# For demonstration purposes, we're using one week of data for training.\n",
    "# The training period is set from December 24, 2022, to December 31, 2022.\n",
    "train_start_date = \"2022-12-24\"\n",
    "train_end_date = \"2022-12-31\"\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "work_dir = f\"{homedir}/gridmet_test_run\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c9530",
   "metadata": {},
   "source": [
    "###  3.6.3.2 Function to Copy .he5 Files from Source to Destination Directory\n",
    "\n",
    "The goal here is to copy all `.he5` files from a specified source directory to a destination directory.\n",
    "\n",
    "- **`source_dir`**: The directory where the `.he5` files are originally located.\n",
    "- **`destination_dir`**: The target directory where the `.he5` files will be copied.\n",
    "- **`os.walk`**: A function that traverses the directory tree, accessing all subdirectories and files.\n",
    "- **`shutil.copy`**: A method used to copy the files from the source to the destination.\n",
    "\n",
    "The code specifically looks for files with the `.he5` extension to identify the relevant files for copying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0f8c76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_he5_files(source_dir, destination_dir):\n",
    "    '''\n",
    "    Copy .he5 files from the source directory to the destination directory.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): The source directory containing .he5 files to copy.\n",
    "        destination_dir (str): The destination directory where .he5 files will be copied.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Get a list of all subdirectories and files in the source directory\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.he5'):\n",
    "                # Get the absolute path of the source file\n",
    "                source_file_path = os.path.join(root, file)\n",
    "                # Copy the file to the destination directory\n",
    "                shutil.copy(source_file_path, destination_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e1f179",
   "metadata": {},
   "source": [
    "### 3.6.3.2 Finding the Closest Grid Cell Index for Given Latitude and Longitude\n",
    "\n",
    "- `target_latitude`, `target_longitude`: The coordinates of the specific location you want to match to a grid cell.\n",
    "- `lat_grid`, `lon_grid`: Arrays representing the grid of latitude and longitude values across a region.\n",
    "- `np.unravel_index`: It identifies the indices in the grid where the sum of latitude and longitude differences is minimized.\n",
    "\n",
    "Here we calculate the absolute differences between the target location and each point in the grid to find the closest match:\n",
    "- `lat_diff = np.abs(lat_grid - target_latitude)`\n",
    "- `lon_diff = np.abs(lon_grid - target_longitude)`\n",
    "\n",
    "\n",
    "Through this code snippet, we get the row index (`lat_idx`), column index (`lon_idx`), and the actual latitude and longitude of the closest grid cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fee15083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n",
    "    '''\n",
    "    Find the index of the grid cell with the closest coordinates to the target latitude and longitude.\n",
    "\n",
    "    Args:\n",
    "        target_latitude (float): The target latitude.\n",
    "        target_longitude (float): The target longitude.\n",
    "        lat_grid (numpy.ndarray): An array of latitude values.\n",
    "        lon_grid (numpy.ndarray): An array of longitude values.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int, float, float]: A tuple containing the row index, column index, closest latitude, and closest longitude.\n",
    "    '''\n",
    "    # Compute the absolute differences between target and grid coordinates\n",
    "    lat_diff = np.abs(lat_grid - target_latitude)\n",
    "    lon_diff = np.abs(lon_grid - target_longitude)\n",
    "\n",
    "    # Find the indices corresponding to the minimum differences\n",
    "    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n",
    "\n",
    "    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580125e7",
   "metadata": {},
   "source": [
    "### 3.6.3.3 Function to Map SNOTEL Stations to AMSR Grid Coordinates and Create a CSV Mapper\n",
    "\n",
    "Next we map SNOTEL station locations to the nearest AMSR grid cells and save this mapping as a CSV file.\n",
    "\n",
    "- `new_base_station_list_file`: This is a CSV file containing thethe latitude and longitude of various SNOTEL stations.\n",
    "- `target_csv_path`: The file path where the output CSV file will be saved.\n",
    "\n",
    "- `hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']`: Accesses the Northern Hemisphere data group within the HDF5 file.\n",
    "\n",
    "\n",
    "Here we read station data, and check if a mapping file already exists, and downloads the necessary AMSR data file if not already present by using `cmd = f\"curl --output {target_amsr_hdf_path} ...\"`.\n",
    "\n",
    "And for each SNOTEL station, we try to identify the closest AMSR grid cell using latitude and longitude comparisons. `df.to_csv(target_csv_path, index=False)`\n",
    "\n",
    "And we map SNOTEL stations to AMSR grid cells and save it in CSV file.\n",
    "\n",
    "This is useful for comparing ground-based measurements with satellite observations. By finding the closest grid point on the AMSR dataset to each SNOTEL station, scientists and researchers can analyze and compare the data more effectively. This helps in improving weather predictions, studying climate patterns, and better understanding environmental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "adce19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snotel_station_to_amsr_mapper(new_base_station_list_file, target_csv_path):\n",
    "    station_data = pd.read_csv(new_base_station_list_file)\n",
    "    \n",
    "    \n",
    "    date = \"2023-01-01\"\n",
    "    date = date.replace(\"-\", \".\")\n",
    "    he5_date = date.replace(\".\", \"\")\n",
    "    \n",
    "    # Check if the CSV already exists\n",
    "    \n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {target_csv_path} already exists, skipping..\")\n",
    "        df = pd.read_csv(target_csv_path)\n",
    "        return df\n",
    "    \n",
    "    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n",
    "    if os.path.exists(target_amsr_hdf_path):\n",
    "        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n",
    "    else:\n",
    "        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n",
    "        print(f'Running command: {cmd}')\n",
    "        subprocess.run(cmd, shell=True)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n",
    "                               'amsr_lat_idx', 'amsr_lon_idx',\n",
    "                               'station_lat', 'station_lon'])\n",
    "    # Read the HDF\n",
    "    file = h5py.File(target_amsr_hdf_path, 'r')\n",
    "    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "    lat = hem_group['lat'][:]\n",
    "    lon = hem_group['lon'][:]\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    lat = np.nan_to_num(lat, nan=0.0)\n",
    "    lon = np.nan_to_num(lon, nan=0.0)\n",
    "    \n",
    "    # Convert the AMSR grid into our gridMET 1km grid\n",
    "    for idx, row in station_data.iterrows():\n",
    "        target_lat = row['latitude']\n",
    "        target_lon = row['longitude']\n",
    "        \n",
    "        # compare the performance and find the fastest way to search nearest point\n",
    "        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n",
    "        df.loc[len(df.index)] = [closest_lat, \n",
    "                                 closest_lon,\n",
    "                                 closest_lat_idx,\n",
    "                                 closest_lon_idx,\n",
    "                                 target_lat,\n",
    "                                 target_lon]\n",
    "    \n",
    "    # Save the new converted AMSR to CSV file\n",
    "    df.to_csv(target_csv_path, index=False)\n",
    "  \n",
    "    print('AMSR mapper csv is created.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97be1bd",
   "metadata": {},
   "source": [
    "### 3.6.3.4 Extracting and Saving AMSR Snow Data to CSV\n",
    "\n",
    "Next, we extract `snow water equivalent (SWE)` data from AMSR files for a range of dates, match it to specific locations (such as SNOTEL stations), and save the processed data into a CSV file. \n",
    "\n",
    "- `amsr_data_dir`: Directory containing the AMSR `.he5` files.\n",
    "\n",
    "\n",
    "Here we use a pre-generated mapping of SNOTEL stations to AMSR grid cells obtained from`3.6.3.3`  to extract relevant SWE values.\n",
    "\n",
    "- Parallel Processing with Dask: Dask is utilized to efficiently process large datasets in parallel, making the extraction and processing faster.\n",
    "\n",
    "- `dask_station_data = dd.from_pandas(mapper_df, npartitions=1)`: Converts the DataFrame into a Dask DataFrame for parallel processing.\n",
    "\n",
    "- `mapper_df = create_snotel_station_to_amsr_mapper(new_base_station_list_file, target_csv_path)`: Creates a mapping between SNOTEL stations and AMSR grid cells.\n",
    "\n",
    "- `swe = hem_group['Data Fields/SWE_NorthernDaily'][:]`: Extracts the Snow Water Equivalent (SWE) data from the AMSR HDF5 file.\n",
    "\n",
    "- `delayed_results = [process_row(row, swe, new_date_str) for _, row in mapper_df.iterrows()]`: Uses Dask's delayed function to process each row of the DataFrame in parallel.\n",
    "\n",
    "- `processed_data = dask_bag.map(process_file).filter(lambda x: x is not None).compute()`: Processes all the AMSR files in parallel, filtering out any None results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "93048e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file, new_base_station_list_file, start_date, end_date):\n",
    "    if os.path.exists(output_csv_file):\n",
    "        os.remove(output_csv_file)\n",
    "    \n",
    "    target_csv_path = f'{work_dir}/training_snotel_station_to_amsr_mapper.csv'\n",
    "    mapper_df = create_snotel_station_to_amsr_mapper(new_base_station_list_file, \n",
    "                                         target_csv_path)\n",
    "        \n",
    "    # station_data = pd.read_csv(new_base_station_list_file)\n",
    "\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    # Create a Dask DataFrame\n",
    "    dask_station_data = dd.from_pandas(mapper_df, npartitions=1)\n",
    "\n",
    "    # Function to process each file\n",
    "    def process_file(filename):\n",
    "        file_path = os.path.join(amsr_data_dir, filename)\n",
    "        print(file_path)\n",
    "        \n",
    "        file = h5py.File(file_path, 'r')\n",
    "        hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "\n",
    "        date_str = filename.split('_')[-1].split('.')[0]\n",
    "        date = datetime.strptime(date_str, '%Y%m%d')\n",
    "\n",
    "        if not (start_date <= date <= end_date):\n",
    "            print(f\"{date} is not in the training period, skipping..\")\n",
    "            return None\n",
    "\n",
    "        new_date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n",
    "        flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n",
    "        # Create an empty Pandas DataFrame with the desired columns\n",
    "        result_df = pd.DataFrame(columns=['date', 'lat', 'lon', 'AMSR_SWE'])\n",
    "\n",
    "        # Sample loop to add rows to the Pandas DataFrame using dask.delayed\n",
    "        @delayed\n",
    "        def process_row(row, swe, new_date_str):\n",
    "          closest_lat_idx = int(row['amsr_lat_idx'])\n",
    "          closest_lon_idx = int(row['amsr_lon_idx'])\n",
    "          closest_swe = swe[closest_lat_idx, closest_lon_idx]\n",
    "          \n",
    "          return pd.DataFrame([[\n",
    "            new_date_str, \n",
    "            row['station_lat'],\n",
    "            row['station_lon'],\n",
    "            closest_swe]], \n",
    "            columns=result_df.columns\n",
    "          )\n",
    "\n",
    "\n",
    "        # List of delayed computations\n",
    "        delayed_results = [process_row(row, swe, new_date_str) for _, row in mapper_df.iterrows()]\n",
    "\n",
    "        # Compute the delayed results and concatenate them into a Pandas DataFrame\n",
    "        result_df = dask.compute(*delayed_results)\n",
    "        result_df = pd.concat(result_df, ignore_index=True)\n",
    "\n",
    "        # Print the final Pandas DataFrame\n",
    "        #print(result_df)\n",
    "          \n",
    "        return result_df\n",
    "\n",
    "    # Get the list of files\n",
    "    files = [f for f in os.listdir(amsr_data_dir) if f.endswith('.he5')]\n",
    "\n",
    "    # Create a Dask Bag from the files\n",
    "    dask_bag = db.from_sequence(files, npartitions=2)\n",
    "\n",
    "    # Process files in parallel\n",
    "    processed_data = dask_bag.map(process_file).filter(lambda x: x is not None).compute()\n",
    "\n",
    "    # Concatenate the processed data\n",
    "    combined_df = pd.concat(processed_data, ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    combined_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "    print(f\"Merged data saved to {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971336a0",
   "metadata": {},
   "source": [
    "### 3.6.3.5 Running the AMSR Data Extraction Process\n",
    "\n",
    "Here we extract and save AMSR snow data for a specified range of dates, linking it to SNOTEL stations, and storing the results in a CSV file.\n",
    "\n",
    "- `new_base_station_list_file`: CSV file containing a list of active SNOTEL stations in the western U.S.\n",
    "- Date Range: The start and end dates (`train_start_date` and `train_end_date`) define the period for which the data will be processed.\n",
    "- `extract_amsr_values_save_to_csv`: Function call discussed in `3.6.3.4`, it is to processes the AMSR data and saves the output to a CSV file.\n",
    "\n",
    "Here is the main entry point for a script that prepares and processes AMSR data, mapping it to specific training points that include SNOTEL and GHCND (Global Historical Climatology Network Daily) stations.\n",
    "\n",
    "By mapping SNOTEL and GHCND stations to the closest AMSR grid points and extracting relevant data over a specified time period, this script prepares a dataset that can be used for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "977fb644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   latitude  longitude  modis_x  modis_y\n",
      "0  39.95500 -120.53800      123      251\n",
      "1  42.95000 -112.83333      337      168\n",
      "2  36.23333 -106.43333      515      354\n",
      "3  36.23700 -106.42912      515      354\n",
      "4  44.45615 -113.30097      324      126\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/training_snotel_station_to_amsr_mapper.csv already exists, skipping..\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20221226.he5\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20221224.he5\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20221230.he5\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20221231.he5\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20221225.he5\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20210504.he5\n",
      "2021-05-04 00:00:00 is not in the training period, skipping..\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20221228.he5\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20221229.he5\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20220914.he5\n",
      "2022-09-14 00:00:00 is not in the training period, skipping..\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20190111.he5\n",
      "2019-01-11 00:00:00 is not in the training period, skipping..\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20200210.he5\n",
      "2020-02-10 00:00:00 is not in the training period, skipping..\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/amsr/AMSR_U2_L3_DailySnow_B02_20221227.he5\n",
      "Merged data saved to /Users/meghana/Documents/projects/swe-workflow-book/all_training_points_snotel_ghcnd_in_westus.csv_amsr_dask_all_training_ponits_with_ghcnd.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    amsr_data_dir = f\"{work_dir}/amsr\"\n",
    "    all_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\n",
    "    new_base_df = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\n",
    "    print(new_base_df.head())\n",
    "    output_csv_file = f\"{all_training_points_with_snotel_ghcnd_file}_amsr_dask_all_training_ponits_with_ghcnd.csv\"\n",
    "    \n",
    "    start_date = train_start_date\n",
    "    end_date = train_end_date\n",
    "\n",
    "    extract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file, all_training_points_with_snotel_ghcnd_file, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef32bcd",
   "metadata": {},
   "source": [
    "Overall, the script **amsr_features** automates the process of extracting relevant AMSR data features and integrating them with SNOTEL station data, streamlining the workflow for further analysis and modeling tasks related to snowfall prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed4b18",
   "metadata": {},
   "source": [
    "## 3.6.4 AMSR Data Processing and Analysis Pipeline\n",
    "In this section we see, how we automate the process of downloading, mapping, processing, and analyzing AMSR (Advanced Microwave Scanning Radiometer) data. \n",
    "\n",
    "It provides a streamlined and automated pipeline for handling AMSR data, from initial download and grid alignment to final data processing and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0828a7",
   "metadata": {},
   "source": [
    "### 3.6.4.1 Importing Libraries and Setting Up for Snow Data Processing\n",
    "\n",
    "- `KDTree`: From `scipy.spatial`, used for performing efficient nearest-neighbor searches in spatial datasets.\n",
    "- `plot_all_variables_in_one_csv`: A custom function from `convert_results_to_images`, used for visualizing processed data.\n",
    "- `warnings`: To manage and suppress runtime warnings that might clutter the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1fc38737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.spatial import KDTree\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "import warnings\n",
    "import sys\n",
    "from convert_results_to_images import plot_all_variables_in_one_c\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "work_dir = f\"{homedir}/gridmet_test_run\"\n",
    "test_start_date = \"2024-07-18\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3f193",
   "metadata": {},
   "source": [
    "### 3.6.4.2 Suppressing Warnings and Setting Up Variables\n",
    "\n",
    "Here we suppress runtime warnings and set up variables for later use in processing geographical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f44404bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "western_us_coords = f'{work_dir}/dem_file.tif.csv'\n",
    "latlontree = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ff44d",
   "metadata": {},
   "source": [
    "### 3.6.4.3 Preparing the AMSR to GridMET Mapper\n",
    "\n",
    "The goal here is to create a mapping between AMSR grid data and GridMET grid points, saving the results to a CSV file. In `3.6.3.3`, we mapped SNOTEL stations to AMSR grid coordinates. Here, we map the gridMET grid to AMSR coordinates.\n",
    "\n",
    "- `target_csv_path`: The file path where the mapping between AMSR and GridMET grid points will be saved as a CSV file.\n",
    "- `target_amsr_hdf_path`: The path where the AMSR data file is stored or will be downloaded to if it doesn’t exist.- `western_us_coords`: A CSV file containing the latitude and longitude of GridMET grid points for the western U.S.\n",
    "\n",
    "- `file = h5py.File(target_amsr_hdf_path, 'r')`: Opens the AMSR HDF5 file for reading, allowing access to its contents.\n",
    "\n",
    "- `lat = np.nan_to_num(lat, nan=0.0)`: Replaces any `NaN` values in the latitude data with zeros to avoid errors during processing.\n",
    "\n",
    "- `closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)`: Finds the closest AMSR grid point to each target GridMET point, which is crucial for accurate data mapping.\n",
    "\n",
    "This code is essential for linking the satellite-based AMSR data grid with the ground-based GridMET grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fee00f36",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_amsr_grid_mapper():\n",
    "    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n",
    "                               'amsr_lat_idx', 'amsr_lon_idx',\n",
    "                               'gridmet_lat', 'gridmet_lon'])\n",
    "    date = test_start_date\n",
    "    date = date.replace(\"-\", \".\")\n",
    "    he5_date = date.replace(\".\", \"\")\n",
    "    \n",
    "    # Check if the CSV already exists\n",
    "    target_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {target_csv_path} already exists, skipping..\")\n",
    "        return\n",
    "    \n",
    "    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n",
    "    if os.path.exists(target_amsr_hdf_path):\n",
    "        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n",
    "    else:\n",
    "        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n",
    "        print(f'Running command: {cmd}')\n",
    "        subprocess.run(cmd, shell=True)\n",
    "    \n",
    "    # Read the HDF\n",
    "    file = h5py.File(target_amsr_hdf_path, 'r')\n",
    "    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "    lat = hem_group['lat'][:]\n",
    "    lon = hem_group['lon'][:]\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    lat = np.nan_to_num(lat, nan=0.0)\n",
    "    lon = np.nan_to_num(lon, nan=0.0)\n",
    "    \n",
    "    # Set the number of rows to process\n",
    "    num_rows_to_process = 100  # Change this value to the desired number of rows\n",
    "    # Convert the AMSR grid into our gridMET 1km grid\n",
    "    western_us_df = pd.read_csv(western_us_coords)\n",
    "    for idx, row in western_us_df.iterrows():\n",
    "        if idx >= num_rows_to_process:\n",
    "            break\n",
    "\n",
    "        target_lat = row['Latitude']\n",
    "        target_lon = row['Longitude']\n",
    "        \n",
    "        # compare the performance and find the fastest way to search nearest point\n",
    "        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n",
    "        df.loc[len(df.index)] = [closest_lat, \n",
    "                                 closest_lon,\n",
    "                                 closest_lat_idx,\n",
    "                                 closest_lon_idx,\n",
    "                                 target_lat, \n",
    "                                 target_lon]\n",
    "    \n",
    "    # Save the new converted AMSR to CSV file\n",
    "    df.to_csv(target_csv_path, index=False)\n",
    "  \n",
    "    print('AMSR mapper csv is created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65dccd",
   "metadata": {},
   "source": [
    "### 3.6.4.4. Downloading and Converting AMSR Snow Data to DEM Format\n",
    "\n",
    "Here we automate the downloading, conversion, and saving of AMSR data aligned with a DEM grid.\n",
    "And also adds a cumulative sum column to a DataFrame, useful for tracking cumulative metrics over time.\n",
    "\n",
    "- `target_date`: The specific date for which AMSR data is being processed, initially set to `test_start_date`.\n",
    "- `target_mapper_csv_path`: The path to the CSV file that maps AMSR grid points to GridMET grid points.\n",
    "- `target_csv_path`: The file path where the final converted data will be saved as a CSV.\n",
    "- `target_amsr_hdf_path`: The path where the downloaded AMSR HDF5 file will be stored.\n",
    "\n",
    "- `mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)`: Applies the `get_swe` function to each row in the mapping DataFrame to calculate the SWE value for each GridMET point.\n",
    "\n",
    "- `mapper_df.drop(columns=['amsr_lat', 'amsr_lon', ...])`: Removes unnecessary columns from the DataFrame before saving the final results.\n",
    "\n",
    "Here we first constructs the URL for the AMSR data corresponding to the specified date and attempts to download it using the curl command. It ensures that the necessary cookies are available for authentication.\n",
    "\n",
    "And once the data is downloaded, we read the HDF5 file using the h5py library, extracting the latitude, longitude, snow water equivalent (SWE), and flag information from the file.\n",
    "\n",
    "And then we convert the AMSR grid into a format compatible with the DEM grid by finding the corresponding grid points in the DEM grid. This involves identifying the nearest DEM grid points for each AMSR grid point.\n",
    "\n",
    "For each DEM grid point, we perform a custom calculation to determine the SWE and flag values based on the nearest AMSR grid points.\n",
    "\n",
    "And finally we save the converted data, including the latitude, longitude, SWE, and flag information, into a CSV file. This file can be further processed or analyzed in subsequent steps of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1f80846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_amsr_and_convert_grid(target_date = test_start_date):\n",
    "    \"\"\"\n",
    "    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n",
    "    \"\"\"\n",
    "    # the mapper\n",
    "    target_mapper_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n",
    "    mapper_df = pd.read_csv(target_mapper_csv_path)\n",
    "    #print(mapper_df.head())\n",
    "    \n",
    "    df = pd.DataFrame(columns=['date', 'lat', \n",
    "                               'lon', 'AMSR_SWE', \n",
    "                               'AMSR_Flag'])\n",
    "    date = target_date\n",
    "    date = date.replace(\"-\", \".\")\n",
    "    he5_date = date.replace(\".\", \"\")\n",
    "    \n",
    "    # Check if the CSV already exists\n",
    "    target_csv_path = f'{work_dir}/testing_ready_amsr_{date}.csv'\n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {target_csv_path} already exists, skipping..\")\n",
    "        return target_csv_path\n",
    "    \n",
    "    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n",
    "    if os.path.exists(target_amsr_hdf_path) and is_binary(target_amsr_hdf_path):\n",
    "        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n",
    "    else:\n",
    "        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n",
    "        print(f'Running command: {cmd}')\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        # Check the exit code\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Command failed with exit code {result.returncode}.\")\n",
    "            if os.path.exists(target_amsr_hdf_path):\n",
    "              os.remove(target_amsr_hdf_path)\n",
    "              print(f\"Wrong {target_amsr_hdf_path} removed successfully.\")\n",
    "            raise Exception(f\"Failed to download {target_amsr_hdf_path} - {result.stderr}\")\n",
    "    \n",
    "    # Read the HDF\n",
    "    print(f\"Reading {target_amsr_hdf_path}\")\n",
    "    file = h5py.File(target_amsr_hdf_path, 'r')\n",
    "    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "    lat = hem_group['lat'][:]\n",
    "    lon = hem_group['lon'][:]\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    lat = np.nan_to_num(lat, nan=0.0)\n",
    "    lon = np.nan_to_num(lon, nan=0.0)\n",
    "    \n",
    "    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n",
    "    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n",
    "    date = datetime.strptime(date, '%Y.%m.%d')\n",
    "    \n",
    "    # Convert the AMSR grid into our DEM 1km grid\n",
    "    \n",
    "    def get_swe(row):\n",
    "        # Perform your custom calculation here\n",
    "        closest_lat_idx = int(row['amsr_lat_idx'])\n",
    "        closest_lon_idx = int(row['amsr_lon_idx'])\n",
    "        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n",
    "        return closest_swe\n",
    "    \n",
    "    def get_swe_flag(row):\n",
    "        # Perform your custom calculation here\n",
    "        closest_lat_idx = int(row['amsr_lat_idx'])\n",
    "        closest_lon_idx = int(row['amsr_lon_idx'])\n",
    "        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n",
    "        return closest_flag\n",
    "    \n",
    "    # Use the apply function to apply the custom function to each row\n",
    "    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n",
    "    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n",
    "    mapper_df['date'] = date\n",
    "    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n",
    "    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n",
    "    mapper_df = mapper_df.drop(columns=['amsr_lat',\n",
    "                                        'amsr_lon',\n",
    "                                        'amsr_lat_idx',\n",
    "                                        'amsr_lon_idx'])\n",
    "    \n",
    "    print(\"result df: \", mapper_df.head())\n",
    "    # Save the new converted AMSR to CSV file\n",
    "    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n",
    "    mapper_df.to_csv(target_csv_path, index=False)\n",
    "    \n",
    "    print('Completed AMSR testing data collection.')\n",
    "    return target_csv_path\n",
    "\n",
    "def add_cumulative_column(df, column_name):\n",
    "    df[f'cumulative_{column_name}'] = df[column_name].sum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c56467",
   "metadata": {},
   "source": [
    "### 3.6.4.5 Aggregate Cumulative AMSR Snow Data and Export to CSV\n",
    "\n",
    "The goal of this code is to calculate the `cumulative Snow Water Equivalent (SWE)` values from AMSR data over a specific period, filling any gaps in the data, and saving the cumulative results into a CSV file. This is particularly useful for analyzing long-term snow accumulation trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e118309",
   "metadata": {},
   "source": [
    "- `target_date`: The specific date for which cumulative AMSR data is calculated.\n",
    "- `past_october_1`: The date of October 1st in the previous or current year, serving as the start of the  period.\n",
    "- `target_csv_path`: The path where the cumulative AMSR data will be saved.\n",
    "- `gap_filled_csv`: The path where the gap-filled version of the cumulative data will be saved.\n",
    "\n",
    "- `past_october_1 = datetime(selected_date.year - 1, 10, 1)`: Sets the start date for cumulative calculations to October 1st of the previous year if the target date is before October, otherwise, it uses October 1st of the current year.\n",
    "\n",
    "- `data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)`: Downloads and converts AMSR data for each day from October 1st to the target date, storing the results in a dictionary.\n",
    "\n",
    "- `filtered_columns.interpolate(axis=1, method='linear', inplace=True)`: Fills in missing values in the SWE data using linear interpolation across time.\n",
    "\n",
    "- `df[f'cumulative_{column_name}'] = sum_column`: Adds a new column to the DataFrame containing the cumulative sum of SWE values.\n",
    "\n",
    "- `filled_data.to_csv(gap_filled_csv, index=False)`: Saves the gap-filled cumulative data to a CSV file.\n",
    "\n",
    "- `result.to_csv(target_csv_path, index=False)`: Saves the final cumulative data for the target date into a CSV file.\n",
    "\n",
    "This cumulative information is valuable for understanding seasonal snow accumulation and can be used in various environmental analyses and forecasting models. The approach of handling missing data ensures the integrity and completeness of the dataset before it’s saved and used for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2f841d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def get_cumulative_amsr_data(target_date = test_start_date, force=False):\n",
    "    \n",
    "    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    print(selected_date)\n",
    "    if selected_date.month < 10:\n",
    "      past_october_1 = datetime(selected_date.year - 1, 10, 1)\n",
    "    else:\n",
    "      past_october_1 = datetime(selected_date.year, 10, 1)\n",
    "\n",
    "    # Traverse and print every day from past October 1 to the specific date\n",
    "    current_date = past_october_1\n",
    "    target_csv_path = f'{work_dir}/testing_ready_amsr_{target_date}_cumulative.csv'\n",
    "\n",
    "    columns_to_be_cumulated = [\"AMSR_SWE\"]\n",
    "    \n",
    "    gap_filled_csv = f\"{target_csv_path}_gap_filled.csv\"\n",
    "    if os.path.exists(gap_filled_csv) and not force:\n",
    "      print(f\"{gap_filled_csv} already exists, skipping..\")\n",
    "      df = pd.read_csv(gap_filled_csv)\n",
    "      print(df[\"AMSR_SWE\"].describe())\n",
    "    else:\n",
    "      date_keyed_objects = {}\n",
    "      data_dict = {}\n",
    "      new_df = None\n",
    "      while current_date <= selected_date:\n",
    "        print(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date_str = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n",
    "        current_df = pd.read_csv(data_dict[current_date_str])\n",
    "        current_df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "        if current_date != selected_date:\n",
    "          current_df.rename(columns={\n",
    "            \"AMSR_SWE\": f\"AMSR_SWE_{current_date_str}\",\n",
    "            \"AMSR_Flag\": f\"AMSR_Flag_{current_date_str}\",\n",
    "          }, inplace=True)\n",
    "        #print(current_df.head())\n",
    "\n",
    "        if new_df is None:\n",
    "          new_df = current_df\n",
    "        else:\n",
    "          new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n",
    "          #new_df = new_df.append(current_df, ignore_index=True)\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "      print(\"new_df.columns = \", new_df.columns)\n",
    "      print(\"new_df.head = \", new_df.head())\n",
    "      df = new_df\n",
    "\n",
    "      #df.sort_values(by=['gridmet_lat', 'gridmet_lon', 'date'], inplace=True)\n",
    "      print(\"All current head: \", df.head())\n",
    "      print(\"the new_df.shape: \", df.shape)\n",
    "\n",
    "      print(\"Start to fill in the missing values\")\n",
    "      #grouped = df.groupby(['gridmet_lat', 'gridmet_lon'])\n",
    "      filled_data = pd.DataFrame()\n",
    "\n",
    "      # Apply the function to each group\n",
    "      for column_name in columns_to_be_cumulated:\n",
    "        start_time = time.time()\n",
    "        #filled_data = df.apply(lambda row: interpolate_missing_and_add_cumulative_inplace(row, column_name), axis=1)\n",
    "        #alike_columns = filled_data.filter(like=column_name)\n",
    "        #filled_data[f'cumulative_{column_name}'] = alike_columns.sum(axis=1)\n",
    "        print(\"filled_data.columns = \", filled_data.columns)\n",
    "        filtered_columns = df.filter(like=column_name)\n",
    "        print(filtered_columns.columns)\n",
    "        filtered_columns = filtered_columns.mask(filtered_columns > 240)\n",
    "        filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n",
    "        filtered_columns.fillna(0, inplace=True)\n",
    "        \n",
    "        sum_column = filtered_columns.sum(axis=1)\n",
    "        # Define a specific name for the new column\n",
    "        df[f'cumulative_{column_name}'] = sum_column\n",
    "        df[filtered_columns.columns] = filtered_columns\n",
    "        \n",
    "        if filtered_columns.isnull().any().any():\n",
    "          print(\"filtered_columns :\", filtered_columns)\n",
    "          raise ValueError(\"Single group: shouldn't have null values here\")\n",
    "      \n",
    "        print(\"filled_data.columns: \", filled_data.columns)\n",
    "        end_time = time.time()\n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"calculate column {column_name} elapsed time: {elapsed_time} seconds\")\n",
    "        \n",
    "      filled_data = df\n",
    "      filled_data[\"date\"] = target_date\n",
    "      print(\"Finished correctly \", filled_data.head())\n",
    "      filled_data.to_csv(gap_filled_csv, index=False)\n",
    "      print(f\"New filled values csv is saved to {gap_filled_csv}\")\n",
    "      df = filled_data\n",
    "    \n",
    "    result = df\n",
    "    print(\"result.head = \", result.head())\n",
    "    # fill in the rest NA as 0\n",
    "    if result.isnull().any().any():\n",
    "      print(\"result :\", result)\n",
    "      raise ValueError(\"Single group: shouldn't have null values here\")\n",
    "    \n",
    "    # only retain the rows of the target date\n",
    "    print(result['date'].unique())\n",
    "    print(result.shape)\n",
    "    print(result[[\"AMSR_SWE\", \"AMSR_Flag\"]].describe())\n",
    "    result.to_csv(target_csv_path, index=False)\n",
    "    print(f\"New data is saved to {target_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de48c38",
   "metadata": {},
   "source": [
    "### 3.6.4.6 Interpolate Missing Values and Calculate Cumulative SWE In-Place for AMSR Data\n",
    "\n",
    "Here we aim to ensure that any missing or anomalous data points within a specific column are handled appropriately through interpolation, and then a cumulative sum is calculated. \n",
    "\n",
    "This is particularly useful in time series data, where continuity is crucial, and missing data could skew analysis. \n",
    "\n",
    "The cumulative sum provides an aggregated measure that can be used for further analysis, such as tracking total snowfall or snow cover over a period.\n",
    "\n",
    "- `row`: A Pandas Series representing a single row of data from a DataFrame, which contains the values to be interpolated.\n",
    "- `column_name`: The specific column in the DataFrame that needs interpolation and cumulative calculation.\n",
    "- `degree`: The degree of the polynomial used for interpolation, with a default of 1 (linear).\n",
    "\n",
    "- `x_subset_key = x_all_key[x_all_key.str.startswith(column_name)]`: Identifies the subset of columns within the row that corresponds to the specified `column_name`, allowing focused interpolation on the relevant data.\n",
    "\n",
    "- `are_all_values_between_0_and_240 = row[x_subset_key].between(1, 239).all()`: Checks if all values in the specified subset are within a valid range (for SWE, between 1 and 239), which helps ensure that the data is suitable for interpolation.\n",
    "\n",
    "- `row[f\"cumulative_{column_name}\"] = row[x_subset_key].sum()`: After interpolation, this line calculates the cumulative sum of the values in the subset and adds it as a new column in the row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f290e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n",
    "  \"\"\"\n",
    "  Interpolate missing values in a Pandas Series using polynomial interpolation\n",
    "  and add a cumulative column.\n",
    "\n",
    "  Parameters:\n",
    "    - row (pd.Series): The input row containing the data to be interpolated.\n",
    "    - column_name (str): The name of the column to be interpolated.\n",
    "    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n",
    "\n",
    "  Returns:\n",
    "    - pd.Series: The row with interpolated values and a cumulative column.\n",
    "\n",
    "  Raises:\n",
    "    - ValueError: If there are unexpected null values after interpolation.\n",
    "\n",
    "  Note:\n",
    "    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n",
    "    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n",
    "\n",
    "  Examples:\n",
    "    ```python\n",
    "    # Example usage:\n",
    "    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n",
    "    ```\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  # Extract X series (column names)\n",
    "  x_all_key = row.index\n",
    "  \n",
    "  x_subset_key = x_all_key[x_all_key.str.startswith(column_name)]\n",
    "  are_all_values_between_0_and_240 = row[x_subset_key].between(1, 239).all()\n",
    "  if are_all_values_between_0_and_240:\n",
    "    print(\"row[x_subset_key] = \", row[x_subset_key])\n",
    "    print(\"row[x_subset_key].sum() = \", row[x_subset_key].sum())\n",
    "  # create the cumulative column after interpolation\n",
    "  row[f\"cumulative_{column_name}\"] = row[x_subset_key].sum()\n",
    "  return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e4d15",
   "metadata": {},
   "source": [
    "### 3.6.4.7 Running the AMSR Data Extraction Process\n",
    "This script is to handle the entire workflow, from data preparation to the generation of cumulative time series data.\n",
    "\n",
    "- `prepare_amsr_grid_mapper()`: It maps the AMSR grid to the gridMET grid, preparing the necessary data for further processing.\n",
    "\n",
    "- `get_cumulative_amsr_data(force=False)`: This calculates cumulative AMSR data for a specific date range and handles any missing data. The force parameter determines whether to overwrite existing processed data.\n",
    "\n",
    "- `input_time_series_file`: Defines the file path for the cumulative AMSR data, which will be used in subsequent analyses or processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7575f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/meghana/Documents/projects/swe-workflow-book/amsr_to_gridmet_mapper.csv already exists, skipping..\n",
      "2024-07-18 00:00:00\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/testing_ready_amsr_2024-07-18_cumulative.csv_gap_filled.csv already exists, skipping..\n",
      "count    462204.000000\n",
      "mean          0.005612\n",
      "std           0.371249\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max          26.000000\n",
      "Name: AMSR_SWE, dtype: float64\n",
      "result.head =     gridmet_lat  gridmet_lon  AMSR_SWE_2023-10-01  AMSR_Flag_2023-10-01  \\\n",
      "0         49.0     -125.000                  0.0                   241   \n",
      "1         49.0     -124.964                  0.0                   241   \n",
      "2         49.0     -124.928                  0.0                   241   \n",
      "3         49.0     -124.892                  0.0                   241   \n",
      "4         49.0     -124.856                  0.0                   241   \n",
      "\n",
      "   AMSR_SWE_2023-10-02  AMSR_Flag_2023-10-02  AMSR_SWE_2023-10-03  \\\n",
      "0                  0.0                   241                  0.0   \n",
      "1                  0.0                   241                  0.0   \n",
      "2                  0.0                   241                  0.0   \n",
      "3                  0.0                   241                  0.0   \n",
      "4                  0.0                   241                  0.0   \n",
      "\n",
      "   AMSR_Flag_2023-10-03  AMSR_SWE_2023-10-04  AMSR_Flag_2023-10-04  ...  \\\n",
      "0                   241                  0.0                   241  ...   \n",
      "1                   241                  0.0                   241  ...   \n",
      "2                   241                  0.0                   241  ...   \n",
      "3                   241                  0.0                   241  ...   \n",
      "4                   241                  0.0                   241  ...   \n",
      "\n",
      "   AMSR_SWE_2024-07-15  AMSR_Flag_2024-07-15  AMSR_SWE_2024-07-16  \\\n",
      "0                  0.0                   241                  0.0   \n",
      "1                  0.0                   241                  0.0   \n",
      "2                  0.0                   241                  0.0   \n",
      "3                  0.0                   241                  0.0   \n",
      "4                  0.0                   241                  0.0   \n",
      "\n",
      "   AMSR_Flag_2024-07-16  AMSR_SWE_2024-07-17  AMSR_Flag_2024-07-17  AMSR_SWE  \\\n",
      "0                   241                  0.0                   241       0.0   \n",
      "1                   241                  0.0                   241       0.0   \n",
      "2                   241                  0.0                   241       0.0   \n",
      "3                   241                  0.0                   241       0.0   \n",
      "4                   241                  0.0                   241       0.0   \n",
      "\n",
      "   AMSR_Flag  cumulative_AMSR_SWE        date  \n",
      "0        241                  7.5  2024-07-18  \n",
      "1        241                  7.5  2024-07-18  \n",
      "2        241                  7.5  2024-07-18  \n",
      "3        241                  7.5  2024-07-18  \n",
      "4        241                  7.5  2024-07-18  \n",
      "\n",
      "[5 rows x 588 columns]\n",
      "['2024-07-18']\n",
      "(462204, 588)\n",
      "            AMSR_SWE      AMSR_Flag\n",
      "count  462204.000000  462204.000000\n",
      "mean        0.005612     247.367504\n",
      "std         0.371249       6.631336\n",
      "min         0.000000     241.000000\n",
      "25%         0.000000     241.000000\n",
      "50%         0.000000     241.000000\n",
      "75%         0.000000     254.000000\n",
      "max        26.000000     255.000000\n",
      "New data is saved to /Users/meghana/Documents/projects/swe-workflow-book/testing_ready_amsr_2024-07-18_cumulative.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run the download and conversion function\n",
    "    #prepare_amsr_grid_mapper()\n",
    "    prepare_amsr_grid_mapper()\n",
    "#     download_amsr_and_convert_grid()\n",
    "    \n",
    "    get_cumulative_amsr_data(force=False)\n",
    "    input_time_series_file = f'{work_dir}/testing_ready_amsr_{test_start_date}_cumulative.csv_gap_filled.csv'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
