{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6098a345",
   "metadata": {},
   "source": [
    "# 3 GridMET Climatology Data Downloader\n",
    "\n",
    "In this chapter, we aim to:\n",
    "\n",
    "- `Download GridMET Datan`: Obtain specific meteorological variables from the GridMET climatology dataset for a user-specified year. This involves utilizing libraries such as netCDF4 and urllib for file handling and downloading.\n",
    "\n",
    "- `Visualize Data`: Create custom color maps and scatter plots to visualize meteorological variables spatially across the western United States. This functionality aids in understanding geographical patterns and trends in meteorological data.\n",
    "\n",
    "- `Generate Cumulative history CSVs`: Generate cumulative history CSVs to aggregate meteorological data over a specified date range. This feature allows users to analyze historical meteorological patterns and long-term trends for decision-making purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c3a47d3070625",
   "metadata": {},
   "source": [
    "## 3.1.1 Setup and Variable Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adedb3",
   "metadata": {},
   "source": [
    "The following code snippet sets up the environment by importing necessary libraries, defining a workspace, and mapping variables.\n",
    "\n",
    "- `gridmet_var_mapping`: A dictionary that associates short-form variable names with their full descriptive names. This helps in translating shorthand notations in your data to more meaningful terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "174c8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import urllib.request\n",
    "from datetime import datetime, timedelta, date\n",
    "# from snowcast_utils import test_start_date, work_dir\n",
    "import matplotlib.pyplot as plt\n",
    "work_dir = \"/Users/meghana/Documents/projects/swe-workflow-book\"\n",
    "\n",
    "gridmet_var_mapping = {\n",
    "  \"etr\": \"potential_evapotranspiration\",\n",
    "  \"pr\":\"precipitation_amount\",\n",
    "  \"rmax\":\"relative_humidity\",\n",
    "  \"rmin\":\"relative_humidity\",\n",
    "  \"tmmn\":\"air_temperature\",\n",
    "  \"tmmx\":\"air_temperature\",\n",
    "  \"vpd\":\"mean_vapor_pressure_deficit\",\n",
    "  \"vs\":\"wind_speed\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d9f43",
   "metadata": {},
   "source": [
    "## 3.1.2 Retrives the Current Year\n",
    "\n",
    "The following code snippet retrives the current year from the system's date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92e47a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_year():\n",
    "    \"\"\"\n",
    "    Get the current year.\n",
    "\n",
    "    Returns:\n",
    "        int: The current year.\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    current_year = now.year\n",
    "    return current_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84bca49",
   "metadata": {},
   "source": [
    "## 3.1.3 Removes Specific Files in a Folder\n",
    "\n",
    "We remove all files within the specified folder.\n",
    "\n",
    "- `folder_path`: A string representing the directory where files are to be removed.\n",
    "- `current_year`: An integer representing the current year, used to filter which files should be deleted.\n",
    "- `files`: A list containing the names of all items (files and directories) within the specified folder.\n",
    "- `file_path`: A string representing the full path to each file in the folder, constructed by joining folder_path and the file name.\n",
    "\n",
    "- The function then loops through each item in the `files` list. For each item:\n",
    "    - `file_path = os.path.join(folder_path, file)` constructs the full path to the file by combining the folder path and the file name.\n",
    "    - `if os.path.isfile(file_path) and str(current_year) in file_path and file_path.endswith(\".nc\"):` checks if the item is a file (not a directory), if the file name contains the current year as a substring, and if the file has a `.nc` extension (indicating a NetCDF file).\n",
    "  - If all these conditions are met, the file is deleted using `os.remove(file_path)`, and a message is printed to confirm the deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b96876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files_in_folder(folder_path, current_year):\n",
    "    \"\"\"\n",
    "    Remove all files in a specified folder.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder to remove files from.\n",
    "    \"\"\"\n",
    "    # Get a list of files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Loop through the files and remove them\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if os.path.isfile(file_path) and str(current_year) in file_path and file_path.endswith(\".nc\"):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted file: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f22a3",
   "metadata": {},
   "source": [
    "## 3.1.4 Download File from a URL\n",
    "\n",
    "Here we download a file from a given URL and saves it to a specified location.\n",
    "\n",
    "- `url`: A string representing the URL from which the file is to be downloaded.\n",
    "    - `target_file_path`: A string representing the path where the downloaded file should be saved.\n",
    "    - `variable`: A string representing the name of the meteorological variable being downloaded.\n",
    "\n",
    "- `with urllib.request.urlopen(url) as response:` opens a connection to the provided URL.\n",
    "   - `file_content = response.read()` reads the contents of the file from the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d7c28a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, target_file_path, variable):\n",
    "    \"\"\"\n",
    "    Download a file from a URL and save it to a specified location.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): URL of the file to download.\n",
    "        target_file_path (str): Path where the downloaded file should be saved.\n",
    "        variable (str): Name of the meteorological variable being downloaded.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            print(f\"Downloading {url}\")\n",
    "            file_content = response.read()\n",
    "        save_path = target_file_path\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(file_content)\n",
    "        print(f\"File downloaded successfully and saved as: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525baae",
   "metadata": {},
   "source": [
    "## 3.1.5 Downloads Specific Meteorological Variables\n",
    "\n",
    "It downloads specific meteorological variables from the GridMET climatology dataset for a list of years provided as input.\n",
    "\n",
    "- `year_list`: A list of years for which the meteorological data is to be downloaded.\n",
    "- `base_metadata_url` is a string that stores the base URL from where the meteorological data files will be downloaded.\n",
    "- `variables_list` is a list containing the short names of the meteorological variables to be downloaded, such as 'tmmn' (minimum temperature), 'pr' (precipitation), and others.\n",
    "- The function loops through each variable in `variables_list`.\n",
    "- For each variable, it further loops through each year in `year_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b73d3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gridmet_of_specific_variables(year_list):\n",
    "    \"\"\"\n",
    "    Download specific meteorological variables from the GridMET climatology dataset.\n",
    "    \"\"\"\n",
    "    # Make a directory to store the downloaded files\n",
    "\n",
    "    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n",
    "    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n",
    "\n",
    "    for var in variables_list:\n",
    "        for y in year_list:\n",
    "            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n",
    "            target_file_path = os.path.join(gridmet_folder_name, var + '_' + '%s' % y + '.nc')\n",
    "            if not os.path.exists(target_file_path):\n",
    "                download_file(download_link, target_file_path, var)\n",
    "            else:\n",
    "                print(f\"File {target_file_path} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b8ece",
   "metadata": {},
   "source": [
    "## 3.1.6 Extract File Name from File Path\n",
    "\n",
    "Here we extracts the file name from a given file path.\n",
    "- `file_path`: A string representing the full path to a file.\n",
    "- `file_name = os.path.basename(file_path)` uses the `os.path.basename()` function to extract the file name from the complete file path. The `basename()` function returns the last component of the path, which is the file name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "82adf8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name_from_path(file_path):\n",
    "    # Get the file name from the file path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c5e17f",
   "metadata": {},
   "source": [
    "## 3.1.7 Extract Variable Name from File Name\n",
    "\n",
    "The code snippet extracts the variable name from a given file name, assuming the file name follows a specific format.\n",
    "\n",
    "- `file_name`: A string representing the name of the file from which the variable name will be extracted.\n",
    "- `var_name = str(file_name.split('_')[0])` splits the file name at the underscore and takes the first part (index `0`), which is expected to be the variable name. The `str()` function ensures that `var_name` is treated as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2704d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_from_file_name(file_name):\n",
    "    # Assuming the file name format is \"tmmm_year.csv\"\n",
    "    var_name = str(file_name.split('_')[0])\n",
    "    return var_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2dcf9",
   "metadata": {},
   "source": [
    "## 3.1.8 Extracts Coordinates from a CSV File\n",
    "\n",
    "Here we extracts geographical coordinates (longitude and latitude) from a CSV file and returns them as a list of tuples.\n",
    "\n",
    "- `coordinates`: A list initialized as empty and used to store tuples of longitude and latitude values extracted from the CSV file.\n",
    "- `df`: A DataFrame created by loading the CSV file using `pandas`. It contains the data from the CSV, including the `Latitude` and `Longitude` columns.\n",
    "- `lon`: A floating-point number representing the longitude extracted from the current row of the DataFrame.\n",
    "- `lat`: A floating-point number representing the latitude extracted from the current row of the DataFrame.\n",
    "\n",
    "\n",
    "- An empty list `coordinates = []` is initialized to store the extracted coordinates.\n",
    "- The function iterates over each row in the DataFrame using a `for` loop: `for index, row in df.iterrows():`.\n",
    "- For each row, the function extracts the `Latitude` and `Longitude` values, converting them to floating-point numbers: `lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])`.\n",
    "- These coordinates are appended to the `coordinates` list as a tuple: `coordinates.append((lon, lat))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a0eca065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_of_template_tif():\n",
    "  \t# Load the CSV file and extract coordinates\n",
    "    coordinates = []\n",
    "    df = pd.read_csv(dem_csv)\n",
    "    for index, row in df.iterrows():\n",
    "        # Process each row here\n",
    "        lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])\n",
    "        coordinates.append((lon, lat))\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6869942",
   "metadata": {},
   "source": [
    "## 3.1.9 Find the Nearest Index in an Array\n",
    "\n",
    "- `array`: A `numpy` array of numerical values from which the closest element to `value` is to be found.\n",
    "- `value`: A numerical value for which the closest corresponding element in `array` is sought.\n",
    "\n",
    "The code snippet returns the index of the element in the `array` that is closest to the given `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d20022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_index(array, value):\n",
    "    # Find the index of the element in the array that is closest to the given value\n",
    "    return (abs(array - value)).argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba26855",
   "metadata": {},
   "source": [
    "## 3.1.10 Create a GridMET to DEM Mapper\n",
    "\n",
    "Here we generates a mapping between the coordinates in a DEM (Digital Elevation Model) and the corresponding coordinates in a GridMET dataset, saving the result to a CSV file.\n",
    "\n",
    "- `nc_file`: A NetCDF file containing GridMET data, from which latitude and longitude arrays are extracted.\n",
    "- `western_us_dem_df`: A DataFrame containing DEM coordinates loaded from a CSV file.\n",
    "- `target_csv_path`: A string representing the file path where the resulting mapping CSV will be saved.\n",
    "- `latitudes`: A `numpy` array of latitude values extracted from the NetCDF file.\n",
    "- `longitudes`: A `numpy` array of longitude values extracted from the NetCDF file.\n",
    "- `get_gridmet_var_value(row)`: A function that finds the nearest GridMET coordinates for a given DEM coordinate and returns those coordinates along with their indices.\n",
    "\n",
    "- **Geospatial Data Alignment:** This function creates a detailed mapping between DEM coordinates and GridMET coordinates, facilitating the integration of data from different sources. This is crucial for tasks like spatial analysis, where accurate alignment between datasets is required.\n",
    "- **Automation:** By automating the mapping process and saving the results, the function ensures consistency and reduces the manual effort involved in aligning datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6173cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gridmet_to_dem_mapper(nc_file):\n",
    "    western_us_dem_df = pd.read_csv(western_us_coords)\n",
    "    # Check if the CSV already exists\n",
    "    target_csv_path = f'{work_dir}/gridmet_to_dem_mapper.csv'\n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {target_csv_path} already exists, skipping..\")\n",
    "        return\n",
    "    \n",
    "    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n",
    "    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n",
    "    # Read the NetCDF file\n",
    "    with nc.Dataset(nc_file) as nc_file:\n",
    "      \n",
    "      # Get the values at each coordinate using rasterio's sample function\n",
    "      latitudes = nc_file.variables['lat'][:]\n",
    "      longitudes = nc_file.variables['lon'][:]\n",
    "      \n",
    "      def get_gridmet_var_value(row):\n",
    "        # Perform your custom calculation here\n",
    "        gridmet_lat_index = find_nearest_index(latitudes, float(row[\"Latitude\"]))\n",
    "        gridmet_lon_index = find_nearest_index(longitudes, float(row[\"Longitude\"]))\n",
    "        return latitudes[gridmet_lat_index], longitudes[gridmet_lon_index], gridmet_lat_index, gridmet_lon_index\n",
    "    \n",
    "      # Use the apply function to apply the custom function to each row\n",
    "      western_us_dem_df[['gridmet_lat', 'gridmet_lon', \n",
    "                         'gridmet_lat_idx', 'gridmet_lon_idx',]] = western_us_dem_df.apply(lambda row: pd.Series(get_gridmet_var_value(row)), axis=1)\n",
    "      western_us_dem_df.rename(columns={\"Latitude\": \"dem_lat\", \n",
    "                                        \"Longitude\": \"dem_lon\"}, inplace=True)\n",
    "    \n",
    "    # Save the new converted AMSR to CSV file\n",
    "    western_us_dem_df.to_csv(target_csv_path, index=False)\n",
    "    \n",
    "    return western_us_dem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df72f52",
   "metadata": {},
   "source": [
    "We generate a comprehensive mapping between the geographic coordinates present in a DEM template CSV file and their respective counterparts within a GridMET netCDF file. This mapping serves as a vital bridge, facilitating the seamless association and alignment of DEM and GridMET datasets. This integration is crucial for a wide array of spatial analyses, environmental modeling endeavors, and geographical studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f58a20",
   "metadata": {},
   "source": [
    "## 3.1.11 Calculate specific Operation Day\n",
    "\n",
    "Here we calculate the date exactly three days before the current date and returns it as a formatted string.\n",
    "\n",
    "- `current_date`: A `datetime` object representing the current date and time.\n",
    "- `three_days_ago`: A `datetime` object representing the date three days before the current date.\n",
    "- `three_days_ago_string`: A string representing the date three days ago, formatted as \"YYYY-MM-DD\".\n",
    "- `test_start_date`: A string that stores the returned value from `get_operation_day()`, representing the operation day used in other parts of the code.\n",
    "\n",
    "- **Date Calculation:** This function is useful when you need to determine a specific date relative to the current date, such as for time-sensitive operations, data filtering, or logging purposes.\n",
    "- **Consistency:** By formatting the date into a standard string format, the function ensures that the date can be easily compared, stored, or passed to other functions in a consistent manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6dad59c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-12\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def get_operation_day():\n",
    "  # Get the current date and time\n",
    "  current_date = datetime.now()\n",
    "\n",
    "  # Calculate three days ago\n",
    "  three_days_ago = current_date - timedelta(days=3)\n",
    "\n",
    "  # Format the date as a string\n",
    "  three_days_ago_string = three_days_ago.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "  print(three_days_ago_string)\n",
    "\n",
    "  return three_days_ago_string\n",
    "\n",
    "\n",
    "test_start_date = get_operation_day()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f963a",
   "metadata": {},
   "source": [
    "## 3.1.12 Extracts NetCDF Data by Coordinates and Variable\n",
    "\n",
    "The following code extracts specific meteorological data from a NetCDF file based on provided coordinates and a variable name, and returns the data in a pandas DataFrame.\n",
    "\n",
    "- `nc_file`: The path to the NetCDF file containing the meteorological data.\n",
    "- `var_name`: The short name of the variable to be extracted (e.g., 'tmmn' for minimum temperature).\n",
    "- `target_date`: The date for which the data is extracted, formatted as \"YYYY-MM-DD\".\n",
    "- `mapper_df`: A DataFrame containing the mapping between DEM coordinates and GridMET coordinates.\n",
    "- `latitudes`: A `numpy` array of latitude values from the NetCDF file.\n",
    "- `longitudes`: A `numpy` array of longitude values from the NetCDF file.\n",
    "- `day`: A `numpy` array representing the days of the year.\n",
    "- `long_var_name`: The full name of the variable being extracted, as mapped from `var_name`.\n",
    "- `var_col`: The data array for the selected variable from the NetCDF file.\n",
    "- `day_of_year`: An integer representing the day of the year corresponding to the `target_date`.\n",
    "- `day_index`: The index in the `day` array corresponding to the `day_of_year`.\n",
    "- `get_gridmet_var_value(row)`: A function to extract the variable value for each coordinate from the NetCDF data.\n",
    "- `Latitude`, `Longitude`: The final latitude and longitude columns in the returned DataFrame.\n",
    "\n",
    "\n",
    "- **Geospatial Data Extraction:** This function automates the process of extracting specific meteorological data from a NetCDF file based on geospatial coordinates, allowing for detailed analysis of climate variables at specific locations.\n",
    "- **Efficiency:** By leveraging a pre-generated mapping (from DEM to GridMET coordinates), the function efficiently retrieves the data for the exact locations of interest, making it highly useful in spatial analysis and modeling tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c09fdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nc_csv_by_coords_and_variable(nc_file, var_name, target_date=test_start_date):\n",
    "    \n",
    "    create_gridmet_to_dem_mapper(nc_file)\n",
    "  \t\n",
    "    mapper_df = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n",
    "    \n",
    "    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n",
    "    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    # Read the NetCDF file\n",
    "    with nc.Dataset(nc_file) as nc_file:\n",
    "      # Get a list of all variables in the NetCDF file\n",
    "      variables = nc_file.variables.keys()\n",
    "      \n",
    "      # Get the values at each coordinate using rasterio's sample function\n",
    "      latitudes = nc_file.variables['lat'][:]\n",
    "      longitudes = nc_file.variables['lon'][:]\n",
    "      day = nc_file.variables['day'][:]\n",
    "      long_var_name = gridmet_var_mapping[var_name]\n",
    "      var_col = nc_file.variables[long_var_name][:]\n",
    "\n",
    "      \n",
    "      # Calculate the day of the year\n",
    "      day_of_year = selected_date.timetuple().tm_yday\n",
    "      day_index = day_of_year - 1\n",
    "      \n",
    "      def get_gridmet_var_value(row):\n",
    "        # Perform your custom calculation here\n",
    "        lat_index = int(row[\"gridmet_lat_idx\"])\n",
    "        lon_index = int(row[\"gridmet_lon_idx\"])\n",
    "        var_value = var_col[day_index, lat_index, lon_index]\n",
    "        \n",
    "        return var_value\n",
    "    \n",
    "      # Use the apply function to apply the custom function to each row\n",
    "      mapper_df[var_name] = mapper_df.apply(get_gridmet_var_value, axis=1)\n",
    "      \n",
    "      # drop useless columns\n",
    "      mapper_df = mapper_df[[\"dem_lat\", \"dem_lon\", var_name]]\n",
    "      mapper_df.rename(columns={\"dem_lat\": \"Latitude\",\n",
    "                               \"dem_lon\": \"Longitude\"}, inplace=True)\n",
    "    return mapper_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c685758",
   "metadata": {},
   "source": [
    "## 3.1.13 Converts GridMET NetCDF Files to CSV\n",
    "\n",
    "Here we converts NetCDF files containing GridMET meteorological data into CSV files for a specific date. It processes each NetCDF file in a specified directory and extracts relevant data based on the date provided.\n",
    "\n",
    "- `target_date`: The date for which the data is extracted, formatted as \"YYYY-MM-DD\".\n",
    "- `selected_date`: A `datetime` object representing the `target_date`.\n",
    "- `generated_csvs`: A list that stores the paths to the CSV files that are generated during the function's execution.\n",
    "- `gridmet_folder_name`: The root directory containing the NetCDF files to be processed.\n",
    "- `file_name`: The name of the NetCDF file currently being processed.\n",
    "- `var_name`: The meteorological variable extracted from the file name.\n",
    "- `res_csv`: The path where the resulting CSV file will be saved.\n",
    "- `netcdf_file_path`: The full path to the NetCDF file being processed.\n",
    "- `df`: A pandas DataFrame containing the extracted data for the specified coordinates and variable.\n",
    "\n",
    "- **Automated Data Conversion:** This function automates the process of converting multiple NetCDF files into CSV format, making it easier to handle and analyze the data outside of specialized NetCDF tools.\n",
    "- **Efficiency:** By checking for existing CSVs and skipping files that have already been processed, the function avoids redundant work, saving time and computational resources.\n",
    "\n",
    "We extract data for a specific variable from a NetCDF file by matching coordinates from a DEM template CSV file. This enables us to create a DataFrame containing the variable values alongside the corresponding coordinates. By doing so, we can effectively extract and analyze meteorological data for specific geographical locations, aiding in various environmental and geographical studies, as well as modeling endeavors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "46b63faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_gridmet_nc_to_csv(target_date=test_start_date):\n",
    "    \n",
    "    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    generated_csvs = []\n",
    "    for root, dirs, files in os.walk(gridmet_folder_name):\n",
    "        for file_name in files:\n",
    "            \n",
    "            if str(selected_date.year) in file_name and file_name.endswith(\".nc\"):\n",
    "                print(f\"Checking file: {file_name}\")\n",
    "                var_name = get_var_from_file_name(file_name)\n",
    "                res_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n",
    "\n",
    "                if os.path.exists(res_csv):\n",
    "                    #os.remove(res_csv)\n",
    "                    print(f\"{res_csv} already exists. Skipping..\")\n",
    "                    generated_csvs.append(res_csv)\n",
    "                    continue\n",
    "\n",
    "                # Perform operations on each file here\n",
    "                netcdf_file_path = os.path.join(root, file_name)\n",
    "                print(\"Processing file:\", netcdf_file_path)\n",
    "                file_name = get_file_name_from_path(netcdf_file_path)\n",
    "\n",
    "                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, \n",
    "                                                       var_name, target_date)\n",
    "                df.replace('--', pd.NA, inplace=True)\n",
    "                df.to_csv(res_csv, index=False)\n",
    "                print(\"gridmet var saved: \", res_csv)\n",
    "                generated_csvs.append(res_csv)\n",
    "                \n",
    "    return generated_csvs   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77512189",
   "metadata": {},
   "source": [
    "## 3.1.14 Plot GridMET Data\n",
    "\n",
    "The following code snippet generates a scatter plot of GridMET data for a specified date and saves the resulting image to a file. It processes data from a CSV file and creates a visual representation of the variable \"pr\" (precipitation) on a geographical grid.\n",
    "\n",
    "- `target_date`: The date for which the data is plotted, formatted as \"YYYY-MM-DD\".\n",
    "- `selected_date`: A `datetime` object representing the `target_date`.\n",
    "- `var_name`: A string representing the name of the variable to be plotted, set to \"pr\" (precipitation).\n",
    "- `test_csv`: The file path to the CSV file containing the data to be plotted.\n",
    "- `gridmet_var_df`: A DataFrame containing the loaded data from the CSV file.\n",
    "- `colormaplist`: A list of colors corresponding to the value ranges in the data.\n",
    "- `value_ranges`: The value ranges used to map the colors in the plot.\n",
    "- `res_png_path`: The file path where the resulting plot image will be saved.\n",
    "\n",
    "- **Visualization:** This function provides a visual representation of the GridMET data, specifically focusing on precipitation (\"pr\") values. Visualization helps in understanding spatial patterns and distributions in the data, making it easier to interpret and analyze.\n",
    "- **Data Communication:** By saving the plot as an image, the function allows the results to be easily shared, included in reports, or further analyzed.\n",
    "\n",
    "We convert GridMET NetCDF files to CSV format for a specified date. We iterate through files in the GridMET folder, checking for files corresponding to the selected date. For each matching file, we extract the variable name and generate a CSV file containing the data. If the CSV file already exists, we skip the process. This process facilitates easy access and analysis of meteorological data for a specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "62f7b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gridmet(target_date=test_start_date):\n",
    "  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "  var_name = \"pr\"\n",
    "  test_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n",
    "  gridmet_var_df = pd.read_csv(test_csv)\n",
    "  gridmet_var_df.replace('--', pd.NA, inplace=True)\n",
    "  gridmet_var_df.dropna(inplace=True)\n",
    "  gridmet_var_df['pr'] = pd.to_numeric(gridmet_var_df['pr'], errors='coerce')\n",
    "  \n",
    "  colormaplist, value_ranges = create_color_maps_with_value_range(gridmet_var_df[var_name])\n",
    "  \n",
    "  # Create a scatter plot\n",
    "  plt.scatter(gridmet_var_df[\"Longitude\"].values, \n",
    "              gridmet_var_df[\"Latitude\"].values, \n",
    "              label='Pressure', \n",
    "              color=colormaplist, \n",
    "              marker='o')\n",
    "\n",
    "  # Add labels and a legend\n",
    "  plt.xlabel('X-axis')\n",
    "  plt.ylabel('Y-axis')\n",
    "  plt.title('Scatter Plot Example')\n",
    "  plt.legend()\n",
    "  \n",
    "  res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.png\"\n",
    "  plt.savefig(res_png_path)\n",
    "  print(f\"test image is saved at {res_png_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27997ec",
   "metadata": {},
   "source": [
    "## 3.1.15 Prepare Folder and Generating Year List\n",
    "The code snippet prepares a directory for storing GridMET NetCDF files and determines the relevant years based on a specified target date. It also checks if existing files cover the selected date and removes them if necessary to ensure up-to-date data.\n",
    "\n",
    "- `target_date`: The date for which the data is being prepared, formatted as \"YYYY-MM-DD\".\n",
    "- `selected_date`: A `datetime` object representing the `target_date`.\n",
    "- `past_october_1`: A `datetime` object representing October 1st of either the current year or the previous year, depending on the `selected_date`.\n",
    "- `year_list`: A list of years that are relevant to the `selected_date`, used for data processing.\n",
    "- `gridmet_folder_name`: The directory where the GridMET NetCDF files are stored.\n",
    "- `nc_file`: The file path to the NetCDF file for the `tmmx` variable of the current year.\n",
    "- `ifremove`: A boolean flag indicating whether the existing files should be removed based on the date coverage.\n",
    "\n",
    "- **Folder Preparation:** Ensuring that the necessary directory exists before proceeding with file operations is crucial for organizing and managing data effectively.\n",
    "- **Data Integrity:** By checking whether existing files cover the required date range and removing them if they do not, the function ensures that the data used in the project is up-to-date and accurate.\n",
    "- **Year Selection:** The `year_list` is essential for determining which years' data should be processed, ensuring that the analysis covers the appropriate time span.\n",
    "\n",
    "We plot GridMET meteorological data for a specific variable and date. We read the data from a corresponding CSV file and preprocess it, ensuring valid numerical values. Then, we create a scatter plot, mapping the variable values to geographic coordinates. The color of each point on the plot represents the magnitude of the variable value. Finally, we save the plot as a PNG image for further analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c643f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_folder_and_get_year_list(target_date=test_start_date):\n",
    "  # Check if the folder exists, if not, create it\n",
    "  if not os.path.exists(gridmet_folder_name):\n",
    "      os.makedirs(gridmet_folder_name)\n",
    "\n",
    "  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "  if selected_date.month < 10:\n",
    "    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n",
    "  else:\n",
    "    past_october_1 = datetime(selected_date.year, 10, 1)\n",
    "  year_list = [selected_date.year, past_october_1.year]\n",
    "\n",
    "  # Remove any existing files in the folder\n",
    "  if selected_date.year == datetime.now().year:\n",
    "    # check if the current year's netcdf contains the selected date\n",
    "    # get etr netcdf and read\n",
    "    nc_file = f\"{gridmet_folder_name}/tmmx_{selected_date.year}.nc\"\n",
    "    ifremove = False\n",
    "    if os.path.exists(nc_file):\n",
    "      with nc.Dataset(nc_file) as ncd:\n",
    "        day = ncd.variables['day'][:]\n",
    "        # Calculate the day of the year\n",
    "        day_of_year = selected_date.timetuple().tm_yday\n",
    "        day_index = day_of_year - 1\n",
    "        if len(day) <= day_index:\n",
    "          ifremove = True\n",
    "    \n",
    "    if ifremove:\n",
    "      print(\"The current year netcdf has new data. Redownloading..\")\n",
    "      remove_files_in_folder(gridmet_folder_name, selected_date.year)  # only redownload when the year is the current year\n",
    "    else:\n",
    "      print(\"The existing netcdf already covers the selected date. Avoid downloading..\")\n",
    "  return year_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe1332",
   "metadata": {},
   "source": [
    "## 3.1.16 Adds a cummulative column\n",
    "\n",
    "Herw we add a new column to a DataFrame that contains the cumulative sum of the values in an existing column.\n",
    "\n",
    "- `df`: The pandas DataFrame that contains the data to which the cumulative sum column will be added.\n",
    "- `column_name`: The name of the column in the DataFrame for which the cumulative sum is to be calculated.\n",
    "\n",
    "- **Data Enrichment:** Adding a cumulative sum column is useful for understanding trends over time, such as total precipitation over a period, cumulative sales, or accumulated values in any time series data.\n",
    "- **Ease of Analysis:** By including the cumulative sum directly in the DataFrame, the function simplifies further analysis and visualization tasks that might require cumulative data.\n",
    "\n",
    "\n",
    "We prepare the folder structure for storing GridMET data and obtain a list of relevant years based on the target date. This process ensures that the necessary directory exists for data storage and determines the appropriate years for data retrieval without delving into technical details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1621c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cumulative_column(df, column_name):\n",
    "  df[f'cumulative_{column_name}'] = df[column_name].sum()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e461b",
   "metadata": {},
   "source": [
    "## 3.1.17 Prepare Cumulative History CSVs\n",
    "\n",
    "generates cumulative history CSVs for a specified target date. It processes GridMET data by traversing the date range from the previous October 1st to the target date, downloading the necessary data, converting it to CSV, and calculating cumulative values for specific meteorological variables. The results are saved in new CSV files.\n",
    "\n",
    "- `target_date`: The target date for generating cumulative history CSVs.\n",
    "- `selected_date`: The `datetime` object representing the `target_date`.\n",
    "- `past_october_1`: A `datetime` object representing October 1st of the current or previous year.\n",
    "- `generated_csvs`: A list of paths to the CSV files generated for the specific date.\n",
    "- `cumulative_target_path`: The file path where the cumulative CSV will be saved.\n",
    "- `date_keyed_objects`: A dictionary holding CSV file paths keyed by date.\n",
    "- `force`: A boolean flag indicating whether to force regeneration of cumulative CSVs.\n",
    "\n",
    "- **Cumulative Data Analysis:** The function enables the analysis of cumulative meteorological data, such as total precipitation over a period, which is crucial for understanding long-term trends and impacts.\n",
    "- **Automated Data Processing:** By automating the download, processing, and cumulative calculation steps, the function ensures that the data is prepared consistently and efficiently, reducing manual workload.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ef804f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cumulative_history_csvs(target_date=test_start_date, force=False):\n",
    "  \"\"\"\n",
    "    Prepare cumulative history CSVs for a specified target date.\n",
    "\n",
    "    Parameters:\n",
    "    - target_date (str, optional): The target date in the format 'YYYY-MM-DD'. Default is 'test_start_date'.\n",
    "    - force (bool, optional): If True, forcefully regenerate cumulative CSVs even if they already exist. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function generates cumulative history CSVs for a specified target date. It traverses the date range from the past\n",
    "    October 1 to the target date, downloads gridmet data, converts it to CSV, and merges it into a big DataFrame.\n",
    "    The cumulative values are calculated and saved in new CSV files.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    prepare_cumulative_history_csvs(target_date='2023-01-01', force=True)\n",
    "    ```\n",
    "\n",
    "    Note: This function assumes the existence of the following helper functions:\n",
    "    - download_gridmet_of_specific_variables\n",
    "    - prepare_folder_and_get_year_list\n",
    "    - turn_gridmet_nc_to_csv\n",
    "    - add_cumulative_column\n",
    "    - process_group_value_filling\n",
    "    ```\n",
    "\n",
    "    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    print(selected_date)\n",
    "    if selected_date.month < 10:\n",
    "        past_october_1 = datetime(selected_date.year - 1, 10, 1)\n",
    "    else:\n",
    "        past_october_1 = datetime(selected_date.year, 10, 1)\n",
    "\n",
    "    # Rest of the function logic...\n",
    "\n",
    "    filled_data = filled_data.loc[:, ['Latitude', 'Longitude', var_name, f'cumulative_{var_name}']]\n",
    "    print(\"new_df final shape: \", filled_data.head())\n",
    "    filled_data.to_csv(cumulative_target_path, index=False)\n",
    "    print(f\"new df is saved to {cumulative_target_path}\")\n",
    "    print(filled_data.describe())\n",
    "    ```\n",
    "Note: This docstring includes placeholders such as \"download_gridmet_of_specific_variables\" and \"prepare_folder_and_get_year_list\" for the assumed existence of related helper functions. You should replace these placeholders with actual documentation for those functions.\n",
    "  \"\"\"\n",
    "  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "  print(selected_date)\n",
    "  if selected_date.month < 10:\n",
    "    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n",
    "  else:\n",
    "    past_october_1 = datetime(selected_date.year, 10, 1)\n",
    "\n",
    "  # Traverse and print every day from past October 1 to the specific date\n",
    "  current_date = past_october_1\n",
    "  \n",
    "  date_keyed_objects = {}\n",
    "  \n",
    "  download_gridmet_of_specific_variables(\n",
    "    prepare_folder_and_get_year_list(target_date=target_date)\n",
    "  )\n",
    "  # Set the specific date you want to process\n",
    "  # current_date = datetime.strptime(\"2024-08-11\", '%Y-%m-%d')\n",
    "  current_date = datetime.strptime(\"2024-08-11\", '%Y-%m-%d')\n",
    "\n",
    "  # Initialize a dictionary to hold the date-keyed objects\n",
    "  date_keyed_objects = {}\n",
    "\n",
    "  # Process the specific date\n",
    "  print('Processing date:', current_date.strftime('%Y-%m-%d'))\n",
    "  current_date_str = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "  # Call the function to generate CSVs for the specific date\n",
    "  generated_csvs = turn_gridmet_nc_to_csv(target_date=current_date_str)\n",
    "\n",
    "  # Read the CSV into a dataframe and store it in the dictionary\n",
    "  date_keyed_objects[current_date_str] = generated_csvs\n",
    "\n",
    "  print(\"Processing complete for the date:\", current_date_str)\n",
    "  print(\"date_keyed_objects: \", date_keyed_objects)\n",
    "  print('target date is ',target_date)\n",
    "  target_date = '2023-08-07'\n",
    "  target_date = '2024-08-11'\n",
    "  target_generated_csvs = date_keyed_objects[target_date]\n",
    "  for index, single_csv in enumerate(target_generated_csvs):\n",
    "    print('in for')\n",
    "    # traverse the variables of gridmet here\n",
    "    # each variable is a loop\n",
    "    print(f\"creating cumulative for {single_csv}\")\n",
    "    \n",
    "    cumulative_target_path = f\"{single_csv}_cumulative.csv\"\n",
    "    print(\"cumulative_target_path = \", cumulative_target_path)\n",
    "    \n",
    "    if os.path.exists(cumulative_target_path) and not force:\n",
    "      print(f\"{cumulative_target_path} already exists, skipping..\")\n",
    "      continue\n",
    "    \n",
    "    # Extract the file name without extension\n",
    "    file_name = os.path.splitext(os.path.basename(single_csv))[0]\n",
    "    gap_filled_csv = f\"{cumulative_target_path}_gap_filled.csv\"\n",
    "\n",
    "\t# Split the file name using underscores\n",
    "    var_name = file_name.split('_')[1]\n",
    "    print(f\"Found variable name {var_name}\")\n",
    "    current_date = past_october_1\n",
    "    new_df = pd.read_csv(single_csv)\n",
    "    print(new_df.head())\n",
    "    \n",
    "    all_df = pd.read_csv(f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\")\n",
    "    all_df[\"date\"] = target_date\n",
    "    all_df[var_name] = pd.to_numeric(all_df[var_name], errors='coerce')\n",
    "    filled_data = all_df\n",
    "    filled_data = filled_data[(filled_data['date'] == target_date)]\n",
    "    filled_data.fillna(0, inplace=True)\n",
    "    print(\"Finished correctly \", filled_data.head())\n",
    "    filled_data = filled_data[['Latitude', 'Longitude', \n",
    "                               var_name, \n",
    "#                                f'cumulative_{var_name}'\n",
    "                              ]]\n",
    "    print(filled_data.shape)\n",
    "    filled_data.to_csv(cumulative_target_path, index=False)\n",
    "    print(f\"new df is saved to {cumulative_target_path}\")\n",
    "    print(filled_data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa482514",
   "metadata": {},
   "source": [
    "## 3.1.18 Running the Workflow for GridMET Data Processing\n",
    "\n",
    "This script sets up the necessary directories and file paths for processing GridMET data, then runs the cumulative history CSV preparation function to generate cumulative data for meteorological variables.\n",
    "\n",
    "- `homedir = os.path.expanduser('~')`: Expands the tilde (`~`) to the full path of the user's home directory.\n",
    "- `work_dir`: Defines the main working directory for the project, where all data processing will take place.\n",
    "- `gridmet_folder_name`: Specifies the folder within the working directory where the GridMET climatology data will be stored.\n",
    "- `western_us_coords`: Points to a CSV file containing the coordinates for the western U.S., derived from a DEM file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d202cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-11 00:00:00\n",
      "The existing netcdf already covers the selected date. Avoid downloading..\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/tmmn_2024.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/tmmn_2023.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/tmmx_2024.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/tmmx_2023.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/pr_2024.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/pr_2023.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/vpd_2024.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/vpd_2023.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/etr_2024.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/etr_2023.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/rmax_2024.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/rmax_2023.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/rmin_2024.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/rmin_2023.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/vs_2024.nc exists\n",
      "File /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/gridmet_climatology/vs_2023.nc exists\n",
      "Processing date: 2024-08-11\n",
      "Checking file: rmin_2024.nc\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_rmin_2024-08-11.csv already exists. Skipping..\n",
      "Checking file: tmmn_2024.nc\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_tmmn_2024-08-11.csv already exists. Skipping..\n",
      "Checking file: etr_2024.nc\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_etr_2024-08-11.csv already exists. Skipping..\n",
      "Checking file: tmmx_2024.nc\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_tmmx_2024-08-11.csv already exists. Skipping..\n",
      "Checking file: vpd_2024.nc\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_vpd_2024-08-11.csv already exists. Skipping..\n",
      "Checking file: pr_2024.nc\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_pr_2024-08-11.csv already exists. Skipping..\n",
      "Checking file: rmax_2024.nc\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_rmax_2024-08-11.csv already exists. Skipping..\n",
      "Checking file: vs_2024.nc\n",
      "/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_vs_2024-08-11.csv already exists. Skipping..\n",
      "Processing complete for the date: 2024-08-11\n",
      "date_keyed_objects:  {'2024-08-11': ['/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_rmin_2024-08-11.csv', '/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_tmmn_2024-08-11.csv', '/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_etr_2024-08-11.csv', '/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_tmmx_2024-08-11.csv', '/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_vpd_2024-08-11.csv', '/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_pr_2024-08-11.csv', '/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_rmax_2024-08-11.csv', '/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_vs_2024-08-11.csv']}\n",
      "target date is  2024-08-11\n",
      "in for\n",
      "creating cumulative for /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_rmin_2024-08-11.csv\n",
      "cumulative_target_path =  /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_rmin_2024-08-11.csv_cumulative.csv\n",
      "Found variable name rmin\n",
      "   Latitude  Longitude rmin\n",
      "0      49.0   -125.000   --\n",
      "1      49.0   -124.964   --\n",
      "2      49.0   -124.928   --\n",
      "3      49.0   -124.892   --\n",
      "4      49.0   -124.856   --\n",
      "Finished correctly     Latitude  Longitude  rmin        date\n",
      "0      49.0   -125.000   0.0  2024-08-11\n",
      "1      49.0   -124.964   0.0  2024-08-11\n",
      "2      49.0   -124.928   0.0  2024-08-11\n",
      "3      49.0   -124.892   0.0  2024-08-11\n",
      "4      49.0   -124.856   0.0  2024-08-11\n",
      "(462204, 3)\n",
      "new df is saved to /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_rmin_2024-08-11.csv_cumulative.csv\n",
      "            Latitude     Longitude           rmin\n",
      "count  462204.000000  462204.00000  462204.000000\n",
      "mean       37.030000    -112.52600      17.643610\n",
      "std         6.921275       7.21226      15.937487\n",
      "min        25.060000    -125.00000       0.000000\n",
      "25%        31.036000    -118.77200       0.000000\n",
      "50%        37.030000    -112.52600      17.500000\n",
      "75%        43.024000    -106.28000      28.000000\n",
      "max        49.000000    -100.05200     100.000000\n",
      "in for\n",
      "creating cumulative for /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_tmmn_2024-08-11.csv\n",
      "cumulative_target_path =  /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_tmmn_2024-08-11.csv_cumulative.csv\n",
      "Found variable name tmmn\n",
      "   Latitude  Longitude tmmn\n",
      "0      49.0   -125.000   --\n",
      "1      49.0   -124.964   --\n",
      "2      49.0   -124.928   --\n",
      "3      49.0   -124.892   --\n",
      "4      49.0   -124.856   --\n",
      "Finished correctly     Latitude  Longitude  tmmn        date\n",
      "0      49.0   -125.000   0.0  2024-08-11\n",
      "1      49.0   -124.964   0.0  2024-08-11\n",
      "2      49.0   -124.928   0.0  2024-08-11\n",
      "3      49.0   -124.892   0.0  2024-08-11\n",
      "4      49.0   -124.856   0.0  2024-08-11\n",
      "(462204, 3)\n",
      "new df is saved to /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_tmmn_2024-08-11.csv_cumulative.csv\n",
      "            Latitude     Longitude           tmmn\n",
      "count  462204.000000  462204.00000  462204.000000\n",
      "mean       37.030000    -112.52600     193.127659\n",
      "std         6.921275       7.21226     135.027899\n",
      "min        25.060000    -125.00000       0.000000\n",
      "25%        31.036000    -118.77200       0.000000\n",
      "50%        37.030000    -112.52600     283.600000\n",
      "75%        43.024000    -106.28000     288.100000\n",
      "max        49.000000    -100.05200     310.900000\n",
      "in for\n",
      "creating cumulative for /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_etr_2024-08-11.csv\n",
      "cumulative_target_path =  /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_etr_2024-08-11.csv_cumulative.csv\n",
      "Found variable name etr\n",
      "   Latitude  Longitude etr\n",
      "0      49.0   -125.000  --\n",
      "1      49.0   -124.964  --\n",
      "2      49.0   -124.928  --\n",
      "3      49.0   -124.892  --\n",
      "4      49.0   -124.856  --\n",
      "Finished correctly     Latitude  Longitude  etr        date\n",
      "0      49.0   -125.000  0.0  2024-08-11\n",
      "1      49.0   -124.964  0.0  2024-08-11\n",
      "2      49.0   -124.928  0.0  2024-08-11\n",
      "3      49.0   -124.892  0.0  2024-08-11\n",
      "4      49.0   -124.856  0.0  2024-08-11\n",
      "(462204, 3)\n",
      "new df is saved to /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_etr_2024-08-11.csv_cumulative.csv\n",
      "            Latitude     Longitude            etr\n",
      "count  462204.000000  462204.00000  462204.000000\n",
      "mean       37.030000    -112.52600       5.175161\n",
      "std         6.921275       7.21226       4.229343\n",
      "min        25.060000    -125.00000       0.000000\n",
      "25%        31.036000    -118.77200       0.000000\n",
      "50%        37.030000    -112.52600       5.800000\n",
      "75%        43.024000    -106.28000       8.000000\n",
      "max        49.000000    -100.05200      20.000000\n",
      "in for\n",
      "creating cumulative for /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_tmmx_2024-08-11.csv\n",
      "cumulative_target_path =  /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_tmmx_2024-08-11.csv_cumulative.csv\n",
      "Found variable name tmmx\n",
      "   Latitude  Longitude tmmx\n",
      "0      49.0   -125.000   --\n",
      "1      49.0   -124.964   --\n",
      "2      49.0   -124.928   --\n",
      "3      49.0   -124.892   --\n",
      "4      49.0   -124.856   --\n",
      "Finished correctly     Latitude  Longitude  tmmx        date\n",
      "0      49.0   -125.000   0.0  2024-08-11\n",
      "1      49.0   -124.964   0.0  2024-08-11\n",
      "2      49.0   -124.928   0.0  2024-08-11\n",
      "3      49.0   -124.892   0.0  2024-08-11\n",
      "4      49.0   -124.856   0.0  2024-08-11\n",
      "(462204, 3)\n",
      "new df is saved to /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_tmmx_2024-08-11.csv_cumulative.csv\n",
      "            Latitude     Longitude           tmmx\n",
      "count  462204.000000  462204.00000  462204.000000\n",
      "mean       37.030000    -112.52600     203.360382\n",
      "std         6.921275       7.21226     142.170743\n",
      "min        25.060000    -125.00000       0.000000\n",
      "25%        31.036000    -118.77200       0.000000\n",
      "50%        37.030000    -112.52600     298.800000\n",
      "75%        43.024000    -106.28000     304.500000\n",
      "max        49.000000    -100.05200     322.200000\n",
      "in for\n",
      "creating cumulative for /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_vpd_2024-08-11.csv\n",
      "cumulative_target_path =  /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_vpd_2024-08-11.csv_cumulative.csv\n",
      "Found variable name vpd\n",
      "   Latitude  Longitude vpd\n",
      "0      49.0   -125.000  --\n",
      "1      49.0   -124.964  --\n",
      "2      49.0   -124.928  --\n",
      "3      49.0   -124.892  --\n",
      "4      49.0   -124.856  --\n",
      "Finished correctly     Latitude  Longitude  vpd        date\n",
      "0      49.0   -125.000  0.0  2024-08-11\n",
      "1      49.0   -124.964  0.0  2024-08-11\n",
      "2      49.0   -124.928  0.0  2024-08-11\n",
      "3      49.0   -124.892  0.0  2024-08-11\n",
      "4      49.0   -124.856  0.0  2024-08-11\n",
      "(462204, 3)\n",
      "new df is saved to /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_vpd_2024-08-11.csv_cumulative.csv\n",
      "            Latitude     Longitude            vpd\n",
      "count  462204.000000  462204.00000  462204.000000\n",
      "mean       37.030000    -112.52600       1.210143\n",
      "std         6.921275       7.21226       1.092028\n",
      "min        25.060000    -125.00000       0.000000\n",
      "25%        31.036000    -118.77200       0.000000\n",
      "50%        37.030000    -112.52600       1.190000\n",
      "75%        43.024000    -106.28000       1.980000\n",
      "max        49.000000    -100.05200       7.460000\n",
      "in for\n",
      "creating cumulative for /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_pr_2024-08-11.csv\n",
      "cumulative_target_path =  /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_pr_2024-08-11.csv_cumulative.csv\n",
      "Found variable name pr\n",
      "   Latitude  Longitude  pr\n",
      "0      49.0   -125.000  --\n",
      "1      49.0   -124.964  --\n",
      "2      49.0   -124.928  --\n",
      "3      49.0   -124.892  --\n",
      "4      49.0   -124.856  --\n",
      "Finished correctly     Latitude  Longitude   pr        date\n",
      "0      49.0   -125.000  0.0  2024-08-11\n",
      "1      49.0   -124.964  0.0  2024-08-11\n",
      "2      49.0   -124.928  0.0  2024-08-11\n",
      "3      49.0   -124.892  0.0  2024-08-11\n",
      "4      49.0   -124.856  0.0  2024-08-11\n",
      "(462204, 3)\n",
      "new df is saved to /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_pr_2024-08-11.csv_cumulative.csv\n",
      "            Latitude     Longitude             pr\n",
      "count  462204.000000  462204.00000  462204.000000\n",
      "mean       37.030000    -112.52600       0.883679\n",
      "std         6.921275       7.21226       2.777191\n",
      "min        25.060000    -125.00000       0.000000\n",
      "25%        31.036000    -118.77200       0.000000\n",
      "50%        37.030000    -112.52600       0.000000\n",
      "75%        43.024000    -106.28000       0.000000\n",
      "max        49.000000    -100.05200      45.400000\n",
      "in for\n",
      "creating cumulative for /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_rmax_2024-08-11.csv\n",
      "cumulative_target_path =  /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_rmax_2024-08-11.csv_cumulative.csv\n",
      "Found variable name rmax\n",
      "   Latitude  Longitude rmax\n",
      "0      49.0   -125.000   --\n",
      "1      49.0   -124.964   --\n",
      "2      49.0   -124.928   --\n",
      "3      49.0   -124.892   --\n",
      "4      49.0   -124.856   --\n",
      "Finished correctly     Latitude  Longitude  rmax        date\n",
      "0      49.0   -125.000   0.0  2024-08-11\n",
      "1      49.0   -124.964   0.0  2024-08-11\n",
      "2      49.0   -124.928   0.0  2024-08-11\n",
      "3      49.0   -124.892   0.0  2024-08-11\n",
      "4      49.0   -124.856   0.0  2024-08-11\n",
      "(462204, 3)\n",
      "new df is saved to /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_rmax_2024-08-11.csv_cumulative.csv\n",
      "            Latitude     Longitude           rmax\n",
      "count  462204.000000  462204.00000  462204.000000\n",
      "mean       37.030000    -112.52600      48.367108\n",
      "std         6.921275       7.21226      36.681029\n",
      "min        25.060000    -125.00000       0.000000\n",
      "25%        31.036000    -118.77200       0.000000\n",
      "50%        37.030000    -112.52600      59.300000\n",
      "75%        43.024000    -106.28000      78.000000\n",
      "max        49.000000    -100.05200     100.000000\n",
      "in for\n",
      "creating cumulative for /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_vs_2024-08-11.csv\n",
      "cumulative_target_path =  /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_vs_2024-08-11.csv_cumulative.csv\n",
      "Found variable name vs\n",
      "   Latitude  Longitude  vs\n",
      "0      49.0   -125.000  --\n",
      "1      49.0   -124.964  --\n",
      "2      49.0   -124.928  --\n",
      "3      49.0   -124.892  --\n",
      "4      49.0   -124.856  --\n",
      "Finished correctly     Latitude  Longitude   vs        date\n",
      "0      49.0   -125.000  0.0  2024-08-11\n",
      "1      49.0   -124.964  0.0  2024-08-11\n",
      "2      49.0   -124.928  0.0  2024-08-11\n",
      "3      49.0   -124.892  0.0  2024-08-11\n",
      "4      49.0   -124.856  0.0  2024-08-11\n",
      "(462204, 3)\n",
      "new df is saved to /Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run/testing_output/2024_vs_2024-08-11.csv_cumulative.csv\n",
      "            Latitude     Longitude             vs\n",
      "count  462204.000000  462204.00000  462204.000000\n",
      "mean       37.030000    -112.52600       2.173956\n",
      "std         6.921275       7.21226       1.854614\n",
      "min        25.060000    -125.00000       0.000000\n",
      "25%        31.036000    -118.77200       0.000000\n",
      "50%        37.030000    -112.52600       2.300000\n",
      "75%        43.024000    -106.28000       3.400000\n",
      "max        49.000000    -100.05200       8.500000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "work_dir = \"/Users/meghana/Documents/projects/swe-workflow-book/gridmet_test_run\"\n",
    "gridmet_folder_name = f'{work_dir}/gridmet_climatology'\n",
    "western_us_coords = f'{work_dir}/dem_file.tif.csv'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # Run the download function\n",
    "  #   download_gridmet_of_specific_variables(prepare_folder_and_get_year_list())\n",
    "  #   turn_gridmet_nc_to_csv()\n",
    "  #   plot_gridmet()\n",
    "\n",
    "  # prepare testing data with cumulative variables\n",
    "  prepare_cumulative_history_csvs(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e148e",
   "metadata": {},
   "source": [
    "When we run the script directly (`__name__ == \"__main__\"`), our main focus is to ensure the preparation of cumulative history CSVs for testing data. By triggering the `prepare_cumulative_history_csvs` function with the `force` parameter set to True, we ensure that all necessary steps are taken to generate accurate cumulative values. Throughout this process, we manage tasks such as downloading GridMET data for specific variables, converting it into CSV format, and calculating cumulative values for each variable over the specified date range. The use of `force=True` ensures that the cumulative CSVs are regenerated if they already exist, mai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2bea0",
   "metadata": {},
   "source": [
    "# GridMET Climatology Data Retrieval and Analysis\n",
    "\n",
    "In this chapter, we do\n",
    "- **Data Collection:** : The script fetches gridMET climatology data from a specified source for various meteorological variables (e.g., temperature, precipitation) and multiple years.It ensures that the data is downloaded for each variable and year required for analysis.\n",
    "\n",
    "- **Data Processing:** After downloading, the script extracts relevant data for specific geographical locations corresponding to weather stations. It organizes the data into structured formats (CSV files) for easier handling and analysis.\n",
    "\n",
    "- **Data Integration:** We merge similar variables obtained from different years into separate CSV files. We then combine all variables together into a single comprehensive dataset for further analysis and modeling tasks. By integrating data from various sources and time periods, it creates a unified dataset that can provide insights into long-term weather patterns and trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1a5713a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import urllib.request\n",
    "from datetime import date, datetime\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "\n",
    "# Changing the timeframe from 10 years to 1 year to demonstrate\n",
    "train_start_date = \"2020-01-03\"\n",
    "train_end_date = \"2021-12-31\"\n",
    "\n",
    "work_dir = f\"{homedir}/gridmet_test_run\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb3626",
   "metadata": {},
   "source": [
    "We import necessary modules and libraries for its operation. These imports bring in functionalities like interacting with the operating system, handling file paths, making URL requests, managing dates and times, manipulating data, working with multi-dimensional arrays, and handling file system paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "59c9d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppress FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "start_date = datetime.strptime(train_start_date, \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(train_end_date, \"%Y-%m-%d\")\n",
    "\n",
    "year_list = [start_date.year + i for i in range(end_date.year - start_date.year + 1)]\n",
    "\n",
    "working_dir = work_dir\n",
    "#stations = pd.read_csv(f'{working_dir}/station_cell_mapping.csv')\n",
    "stations = pd.read_csv(f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\")\n",
    "gridmet_save_location = f'{working_dir}/gridmet_climatology'\n",
    "final_merged_csv = f\"{work_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72aace6",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "In this code snippet, FutureWarnings are suppressed using `warnings.filterwarnings(\"ignore\", category=FutureWarning)`. Following that, the start and end dates for data processing are defined by converting the `train_start_date` and `train_end_date` strings into datetime objects. Subsequently, a list of years between the start and end dates is generated. The working directory path and file paths for SNOTEL stations data, GridMET climatology data storage, and the final merged CSV file are then set up accordingly. Overall, this snippet prepares the necessary parameters and file paths for subsequent data processing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eb0bc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_directory():\n",
    "    f = list()\n",
    "    for files in glob.glob(gridmet_save_location + \"/*.nc\"):\n",
    "        f.append(files)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670deb8",
   "metadata": {},
   "source": [
    "We collect the names of files with the extension \".nc\" within a specified directory by iterating through all files, appending their names to a list, and returning the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e1ad571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, save_location):\n",
    "    try:\n",
    "        print(\"download_file\")\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            file_content = response.read()\n",
    "        file_name = os.path.basename(url)\n",
    "        save_path = os.path.join(save_location, file_name)\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(file_content)\n",
    "        print(f\"File downloaded successfully and saved as: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87896c19",
   "metadata": {},
   "source": [
    "We attempt to download a file from a specified URL. We then save the downloaded file to a specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "45bab2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gridmet_climatology():\n",
    "    folder_name = gridmet_save_location\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n",
    "    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n",
    "\n",
    "    for var in variables_list:\n",
    "        for y in year_list:\n",
    "            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n",
    "            print(\"downloading\", download_link)\n",
    "            if not os.path.exists(os.path.join(folder_name, var + '_' + '%s' % y + '.nc')):\n",
    "                download_file(download_link, folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697fb51",
   "metadata": {},
   "source": [
    "We set up a folder to store data. Then, we gather GridMET climatology data for a range of variables over multiple years. This involves collecting information on temperature, precipitation, vapor pressure deficit, reference evapotranspiration, maximum and minimum radiation, and wind speed. We ensure that the data for each variable and year combination is acquired and stored for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3b4e75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gridmet_variable(file_name):\n",
    "    print(f\"reading values from {file_name}\")\n",
    "    result_data = []\n",
    "    ds = xr.open_dataset(file_name)\n",
    "    var_to_extract = list(ds.keys())\n",
    "    print(var_to_extract)\n",
    "    var_name = var_to_extract[0]\n",
    "    \n",
    "    df = pd.DataFrame(columns=['day', 'lat', 'lon', var_name])\n",
    "    \n",
    "    csv_file = f'{gridmet_save_location}/{Path(file_name).stem}.csv'\n",
    "    if os.path.exists(csv_file):\n",
    "    \tprint(f\"The file '{csv_file}' exists.\")\n",
    "    \treturn\n",
    "\n",
    "    for idx, row in stations.iterrows():\n",
    "        lat = row['latitude']\n",
    "        lon = row['longitude']\n",
    "\t\t\n",
    "        subset_data = ds.sel(lat=lat, lon=lon, method='nearest')\n",
    "        subset_data['lat'] = lat\n",
    "        subset_data['lon'] = lon\n",
    "        converted_df = subset_data.to_dataframe()\n",
    "        converted_df = converted_df.reset_index(drop=False)\n",
    "        converted_df = converted_df.drop('crs', axis=1)\n",
    "        df = pd.concat([df, converted_df], ignore_index=True)\n",
    "        \n",
    "    result_df = df\n",
    "    print(\"got result_df : \", result_df.head())\n",
    "    result_df.to_csv(csv_file, index=False)\n",
    "    print(f'completed extracting data for {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20501ee",
   "metadata": {},
   "source": [
    "We retrieve data from a specific GridMET file, capturing information relevant to latitude and longitude coordinates provided in a dataset. Subsequently, we organize this data into a structured format, ensuring each entry corresponds to a specific day, latitude, and longitude, along with the associated variable values. Following this organization, we save this processed data as a CSV file, providing a convenient and accessible format for further analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5798ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_similar_variables_from_different_years():\n",
    "    files = os.listdir(gridmet_save_location)\n",
    "    file_groups = {}\n",
    "\n",
    "    for filename in files:\n",
    "        base_name, year_ext = os.path.splitext(filename)\n",
    "        parts = base_name.split('_')\n",
    "        if len(parts) == 2 and year_ext == '.csv':\n",
    "            file_groups.setdefault(parts[0], []).append(filename)\n",
    "\n",
    "    for base_name, file_list in file_groups.items():\n",
    "        if len(file_list) > 1:\n",
    "            dfs = []\n",
    "            for filename in file_list[:5]:\n",
    "                df = pd.read_csv(os.path.join(gridmet_save_location, filename))\n",
    "                dfs.append(df)\n",
    "            merged_df = pd.concat(dfs, ignore_index=True)\n",
    "            merged_filename = f\"{base_name}_merged.csv\"\n",
    "            merged_df.to_csv(os.path.join(gridmet_save_location, merged_filename), index=False)\n",
    "            print(f\"Merged {file_list} into {merged_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f03a0",
   "metadata": {},
   "source": [
    "We collect list of files from a designated location. Then, we organize these files into groups based on similarities in their names. For each group of similar files, if there are multiple files present, we proceed to read each file as a DataFrame. Subsequently, we merge these DataFrames into a single comprehensive DataFrame. Following this merging process, we save the resulting merged DataFrame into a new CSV file. Finally, we print a notification message to indicate which files have been successfully merged. Through these steps, the function facilitates the consolidation of related data from different files into cohesive datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "35fff41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_variables_together():\n",
    "    merged_df = None\n",
    "    file_paths = []\n",
    "\n",
    "    for filename in os.listdir(gridmet_save_location):\n",
    "        if filename.endswith(\"_merged.csv\"):\n",
    "            file_paths.append(os.path.join(gridmet_save_location, filename))\n",
    "\t\n",
    "    rmin_merged_path = os.path.join(gridmet_save_location, 'rmin_merged.csv')\n",
    "    rmax_merged_path = os.path.join(gridmet_save_location, 'rmax_merged.csv')\n",
    "    tmmn_merged_path = os.path.join(gridmet_save_location, 'tmmn_merged.csv')\n",
    "    tmmx_merged_path = os.path.join(gridmet_save_location, 'tmmx_merged.csv')\n",
    "    \n",
    "    df_rmin = pd.read_csv(rmin_merged_path)\n",
    "    df_rmax = pd.read_csv(rmax_merged_path)\n",
    "    df_tmmn = pd.read_csv(tmmn_merged_path)\n",
    "    df_tmmx = pd.read_csv(tmmx_merged_path)\n",
    "    \n",
    "    df_rmin.rename(columns={'relative_humidity': 'relative_humidity_rmin'}, inplace=True)\n",
    "    df_rmax.rename(columns={'relative_humidity': 'relative_humidity_rmax'}, inplace=True)\n",
    "    df_tmmn.rename(columns={'air_temperature': 'air_temperature_tmmn'}, inplace=True)\n",
    "    df_tmmx.rename(columns={'air_temperature': 'air_temperature_tmmx'}, inplace=True)\n",
    "    \n",
    "    df_rmin.to_csv(os.path.join(gridmet_save_location, 'rmin_merged.csv'))\n",
    "    df_rmax.to_csv(os.path.join(gridmet_save_location, 'rmax_merged.csv'))\n",
    "    df_tmmn.to_csv(os.path.join(gridmet_save_location, 'tmmn_merged.csv'))\n",
    "    df_tmmx.to_csv(os.path.join(gridmet_save_location, 'tmmx_merged.csv'))\n",
    "    \n",
    "    if file_paths:\n",
    "        merged_df = pd.read_csv(file_paths[0])\n",
    "        for file_path in file_paths[1:10]:\n",
    "            df = pd.read_csv(file_path)\n",
    "            merged_df = pd.concat([merged_df, df], axis=1)\n",
    "        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "        merged_df.to_csv(final_merged_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d5177",
   "metadata": {},
   "source": [
    "We identify and collect CSV files with specific names from a designated location. Then, we proceed to read each of these CSV files, containing data for different variables, into separate DataFrames. Subsequently, we rename specific columns within each DataFrame to ensure clarity and consistency across variables. After updating column names, we overwrite the original CSV files with the renamed versions for consistency. Next, if there are multiple CSV files available, we merge them together into a single comprehensive DataFrame. To avoid redundancy and ensure data integrity, we remove any duplicated columns in the merged DataFrame. Finally, we save the merged DataFrame as a new CSV file, representing the combined dataset encompassing all variables. Through these steps, the function facilitates the integration of data from various sources into a unified dataset for comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "803121ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The python kernel does not appear to be a conda environment.  Please use ``%pip install`` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstall -c conda-forge netcdf4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/core/magics/packaging.py:26\u001b[0m, in \u001b[0;36mis_conda_environment.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# TODO: does this need to change on windows?\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(sys\u001b[38;5;241m.\u001b[39mprefix, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconda-meta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe python kernel does not appear to be a conda environment.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use ``\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mpip install`` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: The python kernel does not appear to be a conda environment.  Please use ``%pip install`` instead."
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bfa36fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5netcdf\n",
      "  Downloading h5netcdf-1.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: h5py in /opt/homebrew/lib/python3.10/site-packages (from h5netcdf) (3.11.0)\n",
      "Requirement already satisfied: packaging in /Users/meghana/Library/Python/3.10/lib/python/site-packages (from h5netcdf) (24.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/homebrew/lib/python3.10/site-packages (from h5py->h5netcdf) (2.0.1)\n",
      "Downloading h5netcdf-1.3.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h5netcdf\n",
      "Successfully installed h5netcdf-1.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e4be46ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The python kernel does not appear to be a conda environment.  Please use ``%pip install`` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstall --force-reinstall -c conda-forge h5netcdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/core/magics/packaging.py:26\u001b[0m, in \u001b[0;36mis_conda_environment.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# TODO: does this need to change on windows?\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(sys\u001b[38;5;241m.\u001b[39mprefix, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconda-meta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe python kernel does not appear to be a conda environment.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use ``\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mpip install`` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: The python kernel does not appear to be a conda environment.  Please use ``%pip install`` instead."
     ]
    }
   ],
   "source": [
    "conda install --force-reinstall -c conda-forge h5netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aca8aa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netCDF4 in /opt/homebrew/lib/python3.10/site-packages (1.7.1.post2)\n",
      "Requirement already satisfied: cftime in /opt/homebrew/lib/python3.10/site-packages (from netCDF4) (1.6.4)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from netCDF4) (2024.7.4)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from netCDF4) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install netCDF4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7dc9d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5netcdf in /opt/homebrew/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: h5py in /opt/homebrew/lib/python3.10/site-packages (from h5netcdf) (3.11.0)\n",
      "Requirement already satisfied: packaging in /Users/meghana/Library/Python/3.10/lib/python/site-packages (from h5netcdf) (24.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/homebrew/lib/python3.10/site-packages (from h5py->h5netcdf) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5netcdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eca71d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cfgrib\n",
      "  Downloading cfgrib-0.9.14.0-py3-none-any.whl.metadata (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting attrs>=19.2 (from cfgrib)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.10/site-packages (from cfgrib) (8.1.7)\n",
      "Collecting eccodes>=0.9.8 (from cfgrib)\n",
      "  Downloading eccodes-1.7.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from cfgrib) (2.0.1)\n",
      "Collecting cffi (from eccodes>=0.9.8->cfgrib)\n",
      "  Downloading cffi-1.17.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting findlibs (from eccodes>=0.9.8->cfgrib)\n",
      "  Downloading findlibs-0.0.5.tar.gz (6.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pycparser (from cffi->eccodes>=0.9.8->cfgrib)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading cfgrib-0.9.14.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading eccodes-1.7.1-py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.17.0-cp310-cp310-macosx_11_0_arm64.whl (178 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: findlibs\n",
      "  Building wheel for findlibs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for findlibs: filename=findlibs-0.0.5-py3-none-any.whl size=6941 sha256=1fc97ca77ecb4cc29e2a26c854fb29bc8da3298472606151b18eda9e5113eb68\n",
      "  Stored in directory: /Users/meghana/Library/Caches/pip/wheels/2e/67/14/22fa5b9fd9c41be520b37e908597d6a262803c0fcf2ba7c2c3\n",
      "Successfully built findlibs\n",
      "Installing collected packages: findlibs, pycparser, attrs, cffi, eccodes, cfgrib\n",
      "Successfully installed attrs-24.2.0 cffi-1.17.0 cfgrib-0.9.14.0 eccodes-1.7.1 findlibs-0.0.5 pycparser-2.22\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cfgrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6a49e156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading http://www.northwestknowledge.net/metdata/data/tmmn_2020.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/tmmn_2021.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/tmmx_2020.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/tmmx_2021.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/pr_2020.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/pr_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/pr_2021.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/pr_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/vpd_2020.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/vpd_2021.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/etr_2020.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/etr_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/etr_2021.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/etr_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/rmax_2020.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/rmax_2021.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/rmin_2020.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/rmin_2021.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/vs_2020.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/vs_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/vs_2021.nc\n",
      "download_file\n",
      "File downloaded successfully and saved as: /Users/meghana/gridmet_test_run/gridmet_climatology/vs_2021.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\n",
      "['relative_humidity']\n",
      "got result_df :           day     lat      lon  relative_humidity\n",
      "0 2021-01-01  39.955 -120.538               43.5\n",
      "1 2021-01-02  39.955 -120.538               62.4\n",
      "2 2021-01-03  39.955 -120.538               60.1\n",
      "3 2021-01-04  39.955 -120.538               68.1\n",
      "4 2021-01-05  39.955 -120.538               39.9\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2021.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2021.nc\n",
      "['air_temperature']\n",
      "got result_df :           day     lat      lon  air_temperature\n",
      "0 2021-01-01  39.955 -120.538            263.9\n",
      "1 2021-01-02  39.955 -120.538            272.2\n",
      "2 2021-01-03  39.955 -120.538            272.2\n",
      "3 2021-01-04  39.955 -120.538            270.5\n",
      "4 2021-01-05  39.955 -120.538            267.7\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2021.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/etr_2021.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/etr_2021.nc\n",
      "['potential_evapotranspiration']\n",
      "got result_df :           day     lat      lon  potential_evapotranspiration\n",
      "0 2021-01-01  39.955 -120.538                           1.4\n",
      "1 2021-01-02  39.955 -120.538                           1.4\n",
      "2 2021-01-03  39.955 -120.538                           1.5\n",
      "3 2021-01-04  39.955 -120.538                           1.4\n",
      "4 2021-01-05  39.955 -120.538                           1.5\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/etr_2021.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/etr_2020.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/etr_2020.nc\n",
      "['potential_evapotranspiration']\n",
      "got result_df :           day     lat      lon  potential_evapotranspiration\n",
      "0 2020-01-01  39.955 -120.538                           1.8\n",
      "1 2020-01-02  39.955 -120.538                           1.2\n",
      "2 2020-01-03  39.955 -120.538                           2.4\n",
      "3 2020-01-04  39.955 -120.538                           1.7\n",
      "4 2020-01-05  39.955 -120.538                           1.5\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/etr_2020.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\n",
      "['relative_humidity']\n",
      "got result_df :           day     lat      lon  relative_humidity\n",
      "0 2020-01-01  39.955 -120.538               50.9\n",
      "1 2020-01-02  39.955 -120.538               36.9\n",
      "2 2020-01-03  39.955 -120.538               24.1\n",
      "3 2020-01-04  39.955 -120.538               51.1\n",
      "4 2020-01-05  39.955 -120.538               48.4\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2020.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2020.nc\n",
      "['air_temperature']\n",
      "got result_df :           day     lat      lon  air_temperature\n",
      "0 2020-01-01  39.955 -120.538            269.7\n",
      "1 2020-01-02  39.955 -120.538            269.2\n",
      "2 2020-01-03  39.955 -120.538            270.5\n",
      "3 2020-01-04  39.955 -120.538            270.9\n",
      "4 2020-01-05  39.955 -120.538            266.7\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2020.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\n",
      "['air_temperature']\n",
      "got result_df :           day     lat      lon  air_temperature\n",
      "0 2021-01-01  39.955 -120.538            278.1\n",
      "1 2021-01-02  39.955 -120.538            276.6\n",
      "2 2021-01-03  39.955 -120.538            277.5\n",
      "3 2021-01-04  39.955 -120.538            276.3\n",
      "4 2021-01-05  39.955 -120.538            279.1\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2020.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2020.nc\n",
      "['air_temperature']\n",
      "got result_df :           day     lat      lon  air_temperature\n",
      "0 2020-01-01  39.955 -120.538            280.2\n",
      "1 2020-01-02  39.955 -120.538            281.0\n",
      "2 2020-01-03  39.955 -120.538            285.5\n",
      "3 2020-01-04  39.955 -120.538            278.2\n",
      "4 2020-01-05  39.955 -120.538            276.4\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2020.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2020.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2020.nc\n",
      "['mean_vapor_pressure_deficit']\n",
      "got result_df :           day     lat      lon  mean_vapor_pressure_deficit\n",
      "0 2020-01-01  39.955 -120.538                         0.26\n",
      "1 2020-01-02  39.955 -120.538                         0.39\n",
      "2 2020-01-03  39.955 -120.538                         0.65\n",
      "3 2020-01-04  39.955 -120.538                         0.28\n",
      "4 2020-01-05  39.955 -120.538                         0.23\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2020.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/pr_2021.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/pr_2021.nc\n",
      "['precipitation_amount']\n",
      "got result_df :           day     lat      lon  precipitation_amount\n",
      "0 2021-01-01  39.955 -120.538                   0.0\n",
      "1 2021-01-02  39.955 -120.538                   0.5\n",
      "2 2021-01-03  39.955 -120.538                   1.3\n",
      "3 2021-01-04  39.955 -120.538                  20.4\n",
      "4 2021-01-05  39.955 -120.538                   0.0\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/pr_2021.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/pr_2020.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/pr_2020.nc\n",
      "['precipitation_amount']\n",
      "got result_df :           day     lat      lon  precipitation_amount\n",
      "0 2020-01-01  39.955 -120.538                   0.0\n",
      "1 2020-01-02  39.955 -120.538                   0.0\n",
      "2 2020-01-03  39.955 -120.538                   0.0\n",
      "3 2020-01-04  39.955 -120.538                   0.8\n",
      "4 2020-01-05  39.955 -120.538                   0.0\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/pr_2020.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\n",
      "['mean_vapor_pressure_deficit']\n",
      "got result_df :           day     lat      lon  mean_vapor_pressure_deficit\n",
      "0 2021-01-01  39.955 -120.538                         0.23\n",
      "1 2021-01-02  39.955 -120.538                         0.22\n",
      "2 2021-01-03  39.955 -120.538                         0.23\n",
      "3 2021-01-04  39.955 -120.538                         0.14\n",
      "4 2021-01-05  39.955 -120.538                         0.32\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/vs_2021.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/vs_2021.nc\n",
      "['wind_speed']\n",
      "got result_df :           day     lat      lon  wind_speed\n",
      "0 2021-01-01  39.955 -120.538         3.0\n",
      "1 2021-01-02  39.955 -120.538         3.5\n",
      "2 2021-01-03  39.955 -120.538         3.4\n",
      "3 2021-01-04  39.955 -120.538         6.5\n",
      "4 2021-01-05  39.955 -120.538         2.3\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/vs_2021.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2020.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2020.nc\n",
      "['relative_humidity']\n",
      "got result_df :           day     lat      lon  relative_humidity\n",
      "0 2020-01-01  39.955 -120.538               93.0\n",
      "1 2020-01-02  39.955 -120.538               73.6\n",
      "2 2020-01-03  39.955 -120.538               58.6\n",
      "3 2020-01-04  39.955 -120.538               74.6\n",
      "4 2020-01-05  39.955 -120.538               84.8\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2020.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/vs_2020.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/vs_2020.nc\n",
      "['wind_speed']\n",
      "got result_df :           day     lat      lon  wind_speed\n",
      "0 2020-01-01  39.955 -120.538         4.5\n",
      "1 2020-01-02  39.955 -120.538         1.2\n",
      "2 2020-01-03  39.955 -120.538         2.2\n",
      "3 2020-01-04  39.955 -120.538         3.6\n",
      "4 2020-01-05  39.955 -120.538         3.6\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/vs_2020.nc\n",
      "/Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2021.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2021.nc\n",
      "['relative_humidity']\n",
      "got result_df :           day     lat      lon  relative_humidity\n",
      "0 2021-01-01  39.955 -120.538              100.0\n",
      "1 2021-01-02  39.955 -120.538               74.3\n",
      "2 2021-01-03  39.955 -120.538               76.2\n",
      "3 2021-01-04  39.955 -120.538               89.7\n",
      "4 2021-01-05  39.955 -120.538               78.1\n",
      "completed extracting data for /Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2021.nc\n",
      "Merged ['tmmn_2020.csv', 'tmmn_2021.csv'] into tmmn_merged.csv\n",
      "Merged ['vs_2020.csv', 'vs_2021.csv'] into vs_merged.csv\n",
      "Merged ['tmmx_2021.csv', 'tmmx_2020.csv'] into tmmx_merged.csv\n",
      "Merged ['pr_2021.csv', 'pr_2020.csv'] into pr_merged.csv\n",
      "Merged ['rmax_2020.csv', 'rmax_2021.csv'] into rmax_merged.csv\n",
      "Merged ['vpd_2020.csv', 'vpd_2021.csv'] into vpd_merged.csv\n",
      "Merged ['rmin_2021.csv', 'rmin_2020.csv'] into rmin_merged.csv\n",
      "Merged ['etr_2020.csv', 'etr_2021.csv'] into etr_merged.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    download_gridmet_climatology()\n",
    "    \n",
    "    nc_files = get_files_in_directory()\n",
    "    count = 0\n",
    "    for nc in nc_files:\n",
    "        print(nc)\n",
    "        count += 1\n",
    "        if count > 5:\n",
    "            break\n",
    "        get_gridmet_variable(nc)\n",
    "    \n",
    "    merge_similar_variables_from_different_years()\n",
    "    merge_all_variables_together()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71256ac1",
   "metadata": {},
   "source": [
    "We start by checking if our script is being executed directly. If it is, we proceed to download data related to GridMET climatology. After that, we retrieve a list of files from a directory, likely containing data files. Next, we iterate through each file, processing them to extract GridMET variables. Following that, we merge similar variables obtained from different years, presumably to create a comprehensive dataset. Lastly, we merge all the variables together, possibly creating a unified dataset. These actions collectively aim to handle and organize GridMET climatology data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
