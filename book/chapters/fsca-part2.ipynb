{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating MODIS Snow Cover Data with Station Measurements for Training and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will explore the process of mapping snow cover data from MODIS GeoTIFF files with station locations, and merge the results into a comprehensive CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from pyproj import Transformer\n",
    "from rasterio.enums import Resampling\n",
    "import concurrent.futures\n",
    "from datetime import datetime, timedelta\n",
    "import dask.dataframe as dd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = '../data'\n",
    "train_start_date = \"2022-01-03\"\n",
    "train_end_date = \"2022-01-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = f\"../data/fsca\"\n",
    "folder_path = f\"../data/fsca/final_output/\"\n",
    "new_base_station_list_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\n",
    "cell_to_modis_mapping = f\"{working_dir}/training_cell_to_modis_mapper_original_snotel_stations.csv\"\n",
    "non_station_random_points_file = f\"{work_dir}/non_station_random_points_in_westus.csv\"\n",
    "only_active_ghcd_station_in_west_conus_file = f\"{work_dir}/active_ghcnd_station_only_list.csv\"\n",
    "ghcd_station_to_modis_mapper_file = f\"{working_dir}/active_ghcnd_mapper_modis.csv\"\n",
    "all_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\n",
    "modis_day_wise = f\"{working_dir}/final_output/\"\n",
    "os.makedirs(modis_day_wise, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_modis_to_station(row, src):\n",
    "    drow, dcol = src.index(row[\"lon\"], row[\"lat\"])\n",
    "    return drow, dcol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maps MODIS pixel coordinates to station coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Points in the MODIS Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_non_station_points():\n",
    "    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n",
    "    print(f\"loading geotiff {sample_modis_tif}\")\n",
    "    with rasterio.open(sample_modis_tif) as src:\n",
    "        bounds = src.bounds\n",
    "        transform = src.transform\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "        raster_array = src.read(1)\n",
    "\n",
    "        random_points = []\n",
    "        while len(random_points) < 4000:\n",
    "            random_x = np.random.uniform(bounds.left, bounds.right)\n",
    "            random_y = np.random.uniform(bounds.bottom, bounds.top)\n",
    "            col, row = ~transform * (random_x, random_y)\n",
    "\n",
    "            if 0 <= row < height and 0 <= col < width:\n",
    "                value = raster_array[int(row), int(col)]\n",
    "                if value != 239 and value != 255:\n",
    "                    random_points.append((random_x, random_y, col, row))\n",
    "\n",
    "        random_points = [(lat, lon, col, row) for lon, lat, col, row in random_points]\n",
    "        random_points_df = pd.DataFrame(random_points, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n",
    "        random_points_df.to_csv(non_station_random_points_file, index=False)\n",
    "        print(f\"random points are saved to {non_station_random_points_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loads a sample GeoTIFF file.\n",
    "- Generates random points within the bounds of the raster, ensuring they are valid points.\n",
    "- Saves the generated points to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MODIS Grid Mapper for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_modis_grid_mapper_training():\n",
    "    if os.path.exists(cell_to_modis_mapping):\n",
    "        print(f\"The file {cell_to_modis_mapping} exists. skip.\")\n",
    "    else:\n",
    "        print(f\"start to generate {cell_to_modis_mapping}\")\n",
    "        station_df = pd.read_csv(new_base_station_list_file)\n",
    "        print(\"original station_df describe() = \", station_df.describe())\n",
    "\n",
    "        sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n",
    "\n",
    "        with rasterio.open(sample_modis_tif) as src:\n",
    "            transform = src.transform\n",
    "            station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n",
    "                src.transform, \n",
    "                station_df[\"longitude\"], \n",
    "                station_df[\"latitude\"])\n",
    "            station_df.to_csv(cell_to_modis_mapping, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n",
    "            print(\"after mapped modis station_df.describe() = \", station_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loads station data and a sample MODIS GeoTIFF file.\n",
    "- Maps station coordinates to MODIS grid coordinates.\n",
    "- Saves the mapping to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Station and Non-Station Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_station_and_non_station_to_one_csv():\n",
    "    df1 = pd.read_csv(cell_to_modis_mapping)\n",
    "    df2 = pd.read_csv(non_station_random_points_file)\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    combined_df.to_csv(all_training_points_with_station_and_non_station_file, index=False)\n",
    "    print(f\"Combined CSV saved to {all_training_points_with_station_and_non_station_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges station data and non-station random points into one CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge SNOTEL and GHCND Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_snotel_ghcnd_station_to_one_csv():\n",
    "    df1 = pd.read_csv(cell_to_modis_mapping)\n",
    "    df2 = pd.read_csv(ghcd_station_to_modis_mapper_file)\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    combined_df.to_csv(all_training_points_with_snotel_ghcnd_file, index=False)\n",
    "    print(f\"Combined CSV saved to {all_training_points_with_snotel_ghcnd_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GHCND Station Mapping for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ghcnd_station_mapping_training():\n",
    "    if os.path.exists(ghcd_station_to_modis_mapper_file):\n",
    "        print(f\"The file {ghcd_station_to_modis_mapper_file} exists. skip.\")\n",
    "    else:\n",
    "        print(f\"start to generate {ghcd_station_to_modis_mapper_file}\")\n",
    "        station_df = pd.read_csv(only_active_ghcd_station_in_west_conus_file)\n",
    "        station_df = station_df.rename(columns={'Latitude': 'latitude', 'Longitude': 'longitude'})\n",
    "        print(\"original station_df describe() = \", station_df.describe())\n",
    "\n",
    "        sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n",
    "\n",
    "        with rasterio.open(sample_modis_tif) as src:\n",
    "            transform = src.transform\n",
    "            station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n",
    "                src.transform, \n",
    "                station_df[\"longitude\"],\n",
    "                station_df[\"latitude\"])\n",
    "            station_df.to_csv(ghcd_station_to_modis_mapper_file, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n",
    "            print(f\"the new mapper to the ghcnd is saved to {ghcd_station_to_modis_mapper_file}\")\n",
    "            print(\"after mapped modis station_df.describe() = \", station_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loads GHCND station data and a sample MODIS GeoTIFF file.\n",
    "- Maps GHCND station coordinates to MODIS grid coordinates.\n",
    "- Saves the mapping to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Value at Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_at_coords(src, lat, lon, band_number=1):\n",
    "    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n",
    "        return None\n",
    "    row, col = src.index(lat, lon)\n",
    "    if (0 <= row < src.height) and (0 <= col < src.width):\n",
    "        return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Band Value for a Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_band_value(row, src):\n",
    "    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n",
    "        valid_value =  src.read(1, window=((int(row[\"modis_y\"]), int(row[\"modis_y\"])+1), (int(row[\"modis_x\"]), int(row[\"modis_x\"])+1)))\n",
    "        return valid_value[0,0]\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieves the value from the MODIS raster at the coordinates specified in the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, current_date_str, outfile):\n",
    "  print(f\"processing {file_path}\")\n",
    "  station_df = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\n",
    "  # print(\"station_df.head() = \", station_df.head())\n",
    "\n",
    "  # Apply get_band_value for each row in the DataFrame\n",
    "  with rasterio.open(file_path) as src:\n",
    "    # Apply get_band_value for each row in the DataFrame\n",
    "    # Get the affine transformation matrix\n",
    "    transform = src.transform\n",
    "\n",
    "    # Extract the spatial extent using the affine transformation\n",
    "    left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n",
    "\n",
    "    # Print the spatial extent\n",
    "    # print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n",
    "\n",
    "    station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n",
    "\n",
    "    \n",
    "  # Prepare final data\n",
    "  station_df['date'] = current_date_str\n",
    "  station_df.to_csv(outfile, index=False, \n",
    "                    columns=['date', 'latitude', 'longitude', 'fsca'])\n",
    "  print(f\"Saved to csv: {outfile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(start_date, end_date):\n",
    "  import glob\n",
    "  # Find CSV files within the specified date range\n",
    "  csv_files = glob.glob(folder_path + '*_training_output_station_corrected.csv')\n",
    "  relevant_csv_files = []\n",
    "\n",
    "  for c in csv_files:\n",
    "    # Extract the date from the file name\n",
    "    # print(\"c = \", c)\n",
    "    file_name = os.path.basename(c)\n",
    "    date_str = file_name.split('_')[0]  # Assuming the date is part of the file name\n",
    "    # print(\"date_str = \", date_str)\n",
    "    file_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "    # Check if the file date is within the specified range\n",
    "    if start_date <= file_date <= end_date:\n",
    "      relevant_csv_files.append(c)\n",
    "#       # Read and concatenate only relevant CSV files\n",
    "#       df = []\n",
    "#       for c in relevant_csv_files:\n",
    "#         tmp = pd.read_csv(c, low_memory=False, usecols=['date', 'latitude', 'longitude', 'fsca'])\n",
    "#         df.append(tmp)\n",
    "\n",
    "#         combined_df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "  # Initialize a Dask DataFrame\n",
    "  print(\"start to use dask to read all csv files\")\n",
    "  dask_df = dd.read_csv(relevant_csv_files)\n",
    "\n",
    "  # Save the merged DataFrame to a CSV file\n",
    "  output_file = f'{working_dir}/fsca_final_training_all.csv'\n",
    "  # Write the Dask DataFrame to a single CSV file\n",
    "  print(f\"saving all csvs into one file: {output_file}\")\n",
    "  dask_df.to_csv(output_file, index=False, single_file=True)\n",
    "  #combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "  #print(combined_df.describe())\n",
    "  print(f\"Merged data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  \n",
    "  start_date = datetime.strptime(train_start_date, \"%Y-%m-%d\")\n",
    "  \n",
    "  end_date = datetime.strptime(train_end_date, \"%Y-%m-%d\")\n",
    "  \n",
    "  prepare_modis_grid_mapper_training()\n",
    "  prepare_ghcnd_station_mapping_training()\n",
    "  # running this function will generate a new set of random points\n",
    "  # generate_random_non_station_points()\n",
    "  #merge_station_and_non_station_to_one_csv()\n",
    "  merge_snotel_ghcnd_station_to_one_csv()\n",
    "  \n",
    "  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n",
    "  for i in date_list:\n",
    "    current_date = i.strftime(\"%Y-%m-%d\")\n",
    "    #print(f\"extracting data for {current_date}\")\n",
    "    outfile = os.path.join(modis_day_wise, f'{current_date}_training_output_station_with_ghcnd.csv')\n",
    "    if os.path.exists(outfile):\n",
    "      print(f\"The file {outfile} exists. skip.\")\n",
    "      pass\n",
    "    else:\n",
    "      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date, outfile)\n",
    "  \n",
    "  merge_csv(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()\n",
    "# print(\"fsca Data extraction complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
